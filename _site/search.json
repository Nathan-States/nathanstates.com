[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nathan States",
    "section": "",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWhat the Inside of a Flooded House Looks Like\n\n\n\n\n\n\n\nsite_updates\n\n\npersonal\n\n\n\n\nGetting rid of some old photos on my computer from Hurricane Ian.\n\n\n\n\n\n\nApr 12, 2023\n\n\nNathan Statses\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a COVID-19 Economic Relief Database: Part 1 (usaspending.gov)\n\n\n\n\n\nLaying the groundwork for the database and talking about the problem with public data on subsidies as a whole.\n\n\n\n\n\n\nFeb 1, 2023\n\n\nNathan States\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing 20 Years of PCAOB Data: Part 1 (Inspections)\n\n\n\n\n\n\n\npcaob\n\n\ngovernment\n\n\n\n\nBreaking down inspection reports from eight of the largest audit firms, including international affiliates.\n\n\n\n\n\n\nJan 27, 2023\n\n\nNathan States\n\n\n\n\n\n\n  \n\n\n\n\nBengaluru Text Mining\n\n\n\n\n\n\n\ntext_mining\n\n\nmachine_learning\n\n\n\n\nAnalyzing over 100,000 complaints from the Silicon Valley of India.\n\n\n\n\n\n\nJan 22, 2023\n\n\nNathan States\n\n\n\n\n\n\n  \n\n\n\n\nSIPRI Dashboard Announcement\n\n\n\n\n\n\n\nmilitary\n\n\ndashboards\n\n\nproject_announcements\n\n\n\n\nAnnouncing my new RShiny Dashboard that allows you to view military spending around the world.\n\n\n\n\n\n\nJan 15, 2023\n\n\nNathan States\n\n\n\n\n\n\n  \n\n\n\n\nSwitching Over to Quarto (And Why You Should Consider Too)\n\n\n\n\n\n\n\nsite_updates\n\n\n\n\nExplaining the new look.\n\n\n\n\n\n\nJan 10, 2023\n\n\nNathan States\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nathan States",
    "section": "",
    "text": "I‚Äôm a data scientist who is passionate about statistics, government, and socioeconomic issues, focusing primarily on the United States. This blog hosts all my work I‚Äôve done related to those topics. While this site is geared towards other data people, it‚Äôs also meant to be accessible to anyone who also shares my passions. No matter your background, I hope you find some use out of this site.\nCheck out my recent posts, new projects, and updates to the archives. I also have a notes tab that has info on various data tools I use in real life.\n\n\nIf you‚Äôre interested in collaborating together, or have other business inquiries, head over to my contact page and hit me up.\n\n\n\nGet a PDF of my resume here."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "",
    "text": "To all my eight viewers who happened to stumble upon the initial release of my blog - you might of noticed it looks drastically different now. When I first built this site, I was using blogdown along with Hugo Apero, but I‚Äôm making the switch over to Quarto from now on.\nHere are some reasons why you should consider migrating as well."
  },
  {
    "objectID": "posts/welcome/index.html#quarto-is-easy.",
    "href": "posts/welcome/index.html#quarto-is-easy.",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "1. Quarto is easy.",
    "text": "1. Quarto is easy.\nblogdown is essentially the same as Hugo, but with the ability to compile RMarkdown files. Some themes are very easy to work with, while others can be a bit of a pain. Quarto is extremely simple and comes with all the tools anyone would need for blogging.\nHugo Apero, the theme I used previously, is actually designed really well in this respect, so this may or may not be a problem for you - it‚Äôs very theme dependent."
  },
  {
    "objectID": "posts/welcome/index.html#not-just-for-blogging.",
    "href": "posts/welcome/index.html#not-just-for-blogging.",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "2. Not just for blogging.",
    "text": "2. Not just for blogging.\nQuarto is capable of creating much more than blogs; presentations, PDF reports, interactive documents, and easy documentation pages are all supported. You can view some examples in their gallery.\nThere‚Äôs obviously a good amount of carryover between these formats and this blog, so it‚Äôs good practice for the real world down the line."
  },
  {
    "objectID": "posts/welcome/index.html#it-supports-multiple-languages",
    "href": "posts/welcome/index.html#it-supports-multiple-languages",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "3. It supports multiple languages",
    "text": "3. It supports multiple languages\nTechnically, blogdown does as well; you can use the reticulate package and hard-code HTML into RMarkdown to create D3 objects, for example. And by that nature, you can always just stick with RMarkdown. However, Quarto uses a custom .qmd format, which is specifically designed for cross-language support. That‚Äôs more or less the selling point of using Quarto."
  },
  {
    "objectID": "posts/welcome/index.html#awesome-visual-bar",
    "href": "posts/welcome/index.html#awesome-visual-bar",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "4. Awesome Visual Bar",
    "text": "4. Awesome Visual Bar\nThis probably falls under point #1, but it‚Äôs so neat that it deserves it‚Äôs own section. The visual tab makes creating documents incredibly intuitive and easy, and includes almost everything you‚Äôd want out of a modern text editor.\nIt‚Äôs pretty neat."
  },
  {
    "objectID": "posts/welcome/index.html#its-compatible-with-hugo.",
    "href": "posts/welcome/index.html#its-compatible-with-hugo.",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "5. It‚Äôs compatible with Hugo.",
    "text": "5. It‚Äôs compatible with Hugo.\nMy old posts were written in RMarkdown, but Quarto only supports qmd. Hugo understands this format perfectly fine, though, so if I were to ever go back to blogdown, it would be incredibly easy to migrate."
  },
  {
    "objectID": "posts/welcome/index.html#superior-to-distill",
    "href": "posts/welcome/index.html#superior-to-distill",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "6. Superior to Distill?",
    "text": "6. Superior to Distill?\nI don‚Äôt think the advantages of Quarto are so significant that it mandates switching over to Distill. With that said, while both offer blogs that are extremely similar in style, Quarto has more features and support. If you are creating your first blog, there isn‚Äôt much of a reason (IMO) to choose Distill over Quarto."
  },
  {
    "objectID": "posts/welcome/index.html#posts-to-read-about-quarto",
    "href": "posts/welcome/index.html#posts-to-read-about-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "Posts to Read About Quarto",
    "text": "Posts to Read About Quarto\nHere is a github that has a ton of links related to making awesome things in Quarto. Here are some posts I‚Äôd recommend reading:\n\nAlison Hill: We don‚Äôt talk about Quarto..\nDanielle Navarro: Posting a Distill blog to Quarto.\nYihui Xie: With Quarto Coming, is RMarkdown Going Away? No.\n\nOn that last point, I‚Äôll briefly discuss the‚Ä¶"
  },
  {
    "objectID": "posts/welcome/index.html#disadvantages-of-quarto",
    "href": "posts/welcome/index.html#disadvantages-of-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "Disadvantages of Quarto",
    "text": "Disadvantages of Quarto\nReally, the main one is lack of customization. Hugo is far more complex, and can create some sophisticated websites to suit any need. Quarto, not so much. This does make blogging on this platform feel slightly generic in comparison, unfortunately. Optimized Hugo sites might also be faster, though admittedly, I have no idea if this is likely / true or not.\nWith that said, the pros outweigh the cons in my case, which is why I decided to make the jump. I hope you enjoy the new look."
  },
  {
    "objectID": "posts/pcaob-part-1/index.html",
    "href": "posts/pcaob-part-1/index.html",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "",
    "text": "A couple years back, I remember reading a story published by the Project on Government Oversight titled, ‚ÄúHow an Agency You‚Äôve Never Heard of Is Leaving the Economy at Risk.‚Äù The agency in question is the Public Company Accounting Oversight Board (PCAOB), who is responsible for regulating the audit industry.\nThe article breaks down how the board has been influenced by politics, limited by budget restrictions, and directly manipulated by the accounting industry itself. Their conclusion - which they support by collecting data from two PCAOB databases - is that the agency isn‚Äôt effective, is lenient on enforcement, and likely not looking in the interests of investors.\nWhen the SEC was created in 1934, all publicly traded companies were required to be audited, but the audit industry itself remained almost entirely self-regulated. There were several noticeable incidents that cast doubt on such an arrangement, but none were greater than the collapse of Enron in 2002. At one point the 6th largest corporation in the United States, the energy giant would file for bankruptcy less than a year later once it was discovered that the vast majority of their profits were the result of fraud. Arthur Andersen, the firm who was receiving $52 million annually to ‚Äúaudit‚Äù Enron, was eventually charged (though never officially tried due to a technicality regarding jury instructions) by the DOJ for shredding documents related to Enron. In 2002, Congress passed the Sarbanes-Oxley Act that strengthened auditing standards, and more-or-less put the PCAOB in charge of enforcing them, along with some oversight from the SEC.\nIt‚Äôs an interesting read, but while the article does rely on some data analysis, it doesn‚Äôt do a full dive into each of the databases. Out of curiosity, I decided to scrape 20 years worth of inspection reports from the PCAOB to get a more data-driven view of the regulatory agency. Here‚Äôs what I found."
  },
  {
    "objectID": "posts/pcaob-part-1/index.html#results",
    "href": "posts/pcaob-part-1/index.html#results",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Results",
    "text": "Results\n\n# Load Libraries \nlibrary(arrow)\nlibrary(dplyr)\nlibrary(htmltools)\nlibrary(echarts4r)\nlibrary(reactable)\nlibrary(reactablefmtr)\n\n# Import Data \noverview <- read_csv_arrow(\"_data/overview.csv\")\ninspections <- read_csv_arrow(\"_data/inspections.csv\")\n\n# Theme Defaults \ne_common(\n  font_family = \"Georgia\"\n)\n\n\ninspections$year <- as.Date(as.character(inspections$year), format = \"%Y\")\ninspections$year <- substring(inspections$year, 1, 4)\n\nget_def_color <- function(def_rate) {\n  orange_pal <- function(x) rgb(colorRamp(c(\"#a43434\", \"#ffb150\"))(x), maxColorValue = 255)\n  normalized <- (def_rate - min(def_rate)) / (max(def_rate) - min(def_rate))\n  orange_pal(normalized)\n}\n\noverview <- overview |>\n  mutate(\n    fail_rate_domestic = domestic_audits_failed / domestic_audits_inspected,\n    fail_Rate_international = international_audits_failed / international_audits_inspected,\n    total_audits_inspected = domestic_audits_inspected + international_audits_inspected,\n    total_audits_failed = domestic_audits_failed + international_audits_failed,\n    total_fail_rate = total_audits_failed / total_audits_inspected\n  ) |>\n  mutate(\n    domestic_def_color = get_def_color(fail_rate_domestic),\n    international_def_color = get_def_color(fail_Rate_international),\n    total_def_color = get_def_color(total_fail_rate)\n  ) |>\n  select(Firm, domestic_audits_inspected, domestic_audits_failed, fail_rate_domestic, international_audits_inspected, international_audits_failed, fail_Rate_international, total_audits_inspected, total_audits_failed, total_fail_rate, domestic_def_color, international_def_color, total_def_color)\n\n\nfunction renderUserScore(cellInfo) {\n  return donutChart(cellInfo.value, cellInfo.row['score_color'])\n}\n\nfunction donutChart(value, color) {\n  // All units are in rem for relative scaling\n  const radius = 1.5\n  const diameter = 3.75\n  const center = diameter / 2\n  const width = 0.25\n  const sliceLength = 2 * Math.PI * radius\n  const sliceOffset = sliceLength * (1 - value / 100)\n  const donutChart = `\n    <svg width=\"${diameter}rem\" height=\"${diameter}rem\" style=\"transform: rotate(-90deg)\" focusable=\"false\">\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"rgba(0,0,0,0.1)\"></circle>\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"${color}\"\n       stroke-dasharray=\"${sliceLength}rem\" stroke-dashoffset=\"${sliceOffset}rem\"></circle>\n    </svg>\n  `\n  const label = `\n    <div style=\"position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%)\">\n      ${value}%\n    </div>\n  `\n  return `\n    <div style=\"display: inline-flex; position: relative\">\n      ${donutChart}\n      ${label}\n    </div>\n  `\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noverview |>\n  reactable(\n    columns = list(\n      Firm = colDef(cell = function(value) {\n        img_src <- knitr::image_uri(sprintf(\"_logos/%s.png\", value))\n        image <- img(src = img_src, height = \"31px\", alt = \"\")\n        tagList(\n          div(style = list(display = \"inline-block\", width = \"45px\"), image),\n          value\n        )\n      }),\n      domestic_audits_inspected = colDef(name = \"Inspected\"),\n      domestic_audits_failed = colDef(name = \"Failed\"),\n      fail_rate_domestic = colDef(\n        name = \"Def. %\", \n        format = colFormat(percent = TRUE, digits = 2),\n        html = TRUE,\n        align = \"center\",\n        width = 140,\n        class = \"user-score\",\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      international_audits_inspected = colDef(name = \"Inspected\"),\n      international_audits_failed = colDef(name = \"Failed\"),\n      fail_Rate_international = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2),\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      total_audits_inspected = colDef(name = \"Inspected\"),\n      total_audits_failed = colDef(name = \"Failed\"),\n      total_fail_rate = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2)\n      )\n    )\n  ) |>\n  add_title(\"PCAOB Inspection Results\")\n\n\nPCAOB Inspection Results\n\n\n\n\n\ninspections %>%\n  filter(country == \"United States\") %>%\n  filter(year > \"2008\") %>%\n  filter(firm != \"Marcum\") %>%\n  mutate(Fail_Rate = audits_failed / audits_inspected) %>%\n  group_by(firm) %>%\n  e_chart(year) %>%\n  e_line(Fail_Rate) %>%\n  e_y_axis(\n    formatter = e_axis_formatter(style = \"percent\")\n  ) %>%\n  e_legend(\n    type = \"scroll\",\n    bottom = 0.5\n  ) %>%\n  e_legend_unselect(\"Crowe\") %>%\n  e_legend_unselect(\"BDO\") %>%\n  e_legend_unselect(\"Grant Thornton\") %>%\n  e_legend_unselect(\"RSM\") %>%\n  e_color(\n    c(\"#1e1e1e\", \"#a29f00\", \"#249d24\", \"#ff7d08\", \"#68249d\", \"#1672b2\", \"#d52323\", \"#9d1774\")\n  ) %>%\n  e_title(\n    top = -5, \n    text = \"Domestic Big Four Firms Have An Average Deficiency Rate of 26.1%\",\n    left = \"center\",\n    textStyle = list(\n      fontSize = 20\n    )\n  ) %>%\n  e_axis_labels(\n    y = \"Deficiency Rate\"\n  ) %>%\n  e_toolbox_feature(\n    feature = \"saveAsImage\"\n  ) %>%\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    textStyle = list(\n      color = \"#fcfcfc\"\n    )\n  )"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Nathan States",
    "section": "",
    "text": "Reach out to me!\nComments are enabled in the archives, but disabled everywhere else. If you‚Äôd like to drop a comment about something in particular, or reach out to me for professional inquiries, message me on one of the following platforms.\n\n\n\n Twitter  Mastodon  GitHub\nOr you can send me an email at nathanstates@outlook.com."
  },
  {
    "objectID": "about.html#lets-work-together.",
    "href": "about.html#lets-work-together.",
    "title": "Nathan States",
    "section": "ü§ù Let‚Äôs work together.",
    "text": "ü§ù Let‚Äôs work together.\nIf you‚Äôre interested in collaborating together, or have other business inquiries, head over to my contact page and hit me up."
  },
  {
    "objectID": "about.html#resume-rundown.",
    "href": "about.html#resume-rundown.",
    "title": "Nathan States",
    "section": "üßæ Resume Rundown.",
    "text": "üßæ Resume Rundown.\nDownload a PDF of my resume here.\n\nSkills\n\nProgramming: R | Python | SQL | SAS | Javascript | D3\nTechnology: Tableau | Power BI | Apache Arrow | SPSS\nDomain Knowledge: Data Wrangling | Data Visualizations\n\n\n\nEducation\n\nUniversity of redacted, redacted | redacted, Florida\nBachelors of Science in Mathematics | Dec.¬†2022\n\n\n\nFeatured Projects\n\nSIPRI Dashboard:\nBengaluru Text Mining:\nPCAOB Data Analysis:"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Nathan States",
    "section": "",
    "text": "Projects\nA list of all of the datasets, dashboards, and other web applications I‚Äôve created.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nü™ñ SIPRI Dashboard\n\n\nView military expenditure around the world.\n\n\n\nNathan States\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/sipri-dashboard/index.html#sipri-dashboard",
    "href": "projects/sipri-dashboard/index.html#sipri-dashboard",
    "title": "ü™ñ SIPRI Dashboard",
    "section": "SIPRI Dashboard",
    "text": "SIPRI Dashboard\nThis web application allows users to view military spending across the world using several different metrics, including:\n\nConverted USD\nInflation-Adjusted USD\nMilitary Spending in GDP Percentage\nMilitary Spending in Government Percentage\nMilitary Spending in Capita Spending\n\nYou can view the dashboard by clicking here."
  },
  {
    "objectID": "projects/sipri-dashboard/index.html#data-source",
    "href": "projects/sipri-dashboard/index.html#data-source",
    "title": "ü™ñ SIPRI Dashboard",
    "section": "Data Source",
    "text": "Data Source\nThe data comes from the Stockholm International Peace Research Institute (SIPRI), a think tank that researches conflict, armaments, arms control, and disarmament around the world. SIPRI was created by the Swedish Parliament in July 1966, who gives them an annual grant to fund their operations. They also receive funding from a variety of other governments and NGOs, including the EU, Australian Government, the University of Notre Dame, the Norwegian Ministry, and many more.\nYou can download the database used for this project here.\n\nPhoto Credit\nPhoto by Juli Kosolapova.\n\n\nSidenote\nThis application is currently being hosted on shinyapps.io on a free plan. Performance can sometimes be slow, and applications are limited to 25 active hours a month. It‚Äôs possible to download the source code and launch the app locally to avoid those issues."
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "",
    "text": "To all my eight viewers who happened to stumble upon the initial release of my blog - you might of noticed it looks drastically different now. When I first built this site, I was using blogdown along with Hugo Apero, but I‚Äôm making the switch over to Quarto from now on.\nHere are some reasons why you should consider migrating as well."
  },
  {
    "objectID": "blog/welcome/index.html#quarto-is-easy.",
    "href": "blog/welcome/index.html#quarto-is-easy.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "1. Quarto is easy.",
    "text": "1. Quarto is easy.\nblogdown is essentially the same as Hugo, but with the ability to compile RMarkdown files. Some themes are very easy to work with, while others can be a bit of a pain. Quarto is extremely simple and comes with all the tools anyone would need for blogging.\nHugo Apero, the theme I used previously, is actually designed really well in this respect, so this may or may not be a problem for you - it‚Äôs very theme dependent."
  },
  {
    "objectID": "blog/welcome/index.html#not-just-for-blogging.",
    "href": "blog/welcome/index.html#not-just-for-blogging.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "2. Not just for blogging.",
    "text": "2. Not just for blogging.\nQuarto is capable of creating much more than blogs; presentations, PDF reports, interactive documents, and easy documentation pages are all supported. You can view some examples in their gallery.\nThere‚Äôs obviously a good amount of carryover between these formats and this blog, so it‚Äôs good practice for the real world down the line."
  },
  {
    "objectID": "blog/welcome/index.html#it-supports-multiple-languages",
    "href": "blog/welcome/index.html#it-supports-multiple-languages",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "3. It supports multiple languages",
    "text": "3. It supports multiple languages\nTechnically, blogdown does as well; you can use the reticulate package and hard-code HTML into RMarkdown to create D3 objects, for example. And by that nature, you can always just stick with RMarkdown. However, Quarto uses a custom .qmd format, which is specifically designed for cross-language support. That‚Äôs more or less the selling point of using Quarto."
  },
  {
    "objectID": "blog/welcome/index.html#awesome-visual-bar",
    "href": "blog/welcome/index.html#awesome-visual-bar",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "4. Awesome Visual Bar",
    "text": "4. Awesome Visual Bar\nThis probably falls under point #1, but it‚Äôs so neat that it deserves it‚Äôs own section. The visual tab makes creating documents incredibly intuitive and easy, and includes almost everything you‚Äôd want out of a modern text editor.\nIt‚Äôs pretty neat."
  },
  {
    "objectID": "blog/welcome/index.html#its-compatible-with-hugo.",
    "href": "blog/welcome/index.html#its-compatible-with-hugo.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "5. It‚Äôs compatible with Hugo.",
    "text": "5. It‚Äôs compatible with Hugo.\nMy old posts were written in RMarkdown, but Quarto only supports qmd. Hugo understands this format perfectly fine, though, so if I were to ever go back to blogdown, it would be incredibly easy to migrate."
  },
  {
    "objectID": "blog/welcome/index.html#superior-to-distill",
    "href": "blog/welcome/index.html#superior-to-distill",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "6. Superior to Distill?",
    "text": "6. Superior to Distill?\nI don‚Äôt think the advantages of Quarto are so significant that it mandates switching over to Distill. With that said, while both offer blogs that are extremely similar in style, Quarto has more features and support. If you are creating your first blog, there isn‚Äôt much of a reason (IMO) to choose Distill over Quarto."
  },
  {
    "objectID": "blog/welcome/index.html#posts-to-read-about-quarto",
    "href": "blog/welcome/index.html#posts-to-read-about-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "Posts to Read About Quarto",
    "text": "Posts to Read About Quarto\nHere is a github that has a ton of links related to making awesome things in Quarto. Here are some posts I‚Äôd recommend reading:\n\nAlison Hill: We don‚Äôt talk about Quarto..\nDanielle Navarro: Posting a Distill blog to Quarto.\nYihui Xie: With Quarto Coming, is RMarkdown Going Away? No.\n\nOn that last point, I‚Äôll briefly discuss the‚Ä¶"
  },
  {
    "objectID": "blog/welcome/index.html#disadvantages-of-quarto",
    "href": "blog/welcome/index.html#disadvantages-of-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "Disadvantages of Quarto",
    "text": "Disadvantages of Quarto\nReally, the main one is lack of customization. Hugo is far more complex, and can create some sophisticated websites to suit any need. Quarto, not so much. This does make blogging on this platform feel slightly generic in comparison, unfortunately. Optimized Hugo sites might also be faster, though admittedly, I have no idea if this is likely / true or not.\nWith that said, the pros outweigh the cons in my case, which is why I decided to make the jump. I hope you enjoy the new look."
  },
  {
    "objectID": "notes/echarts4r/index.html",
    "href": "notes/echarts4r/index.html",
    "title": "echarts4r Cookbook",
    "section": "",
    "text": "There‚Äôs a ton of different charting libraries in R, with a few of them being interactive. plotly is one of the most popular, having over 2.3k stars on GitHub. highcharter is another such package. With ggiraph, it‚Äôs possible to turn ggplot2 objects into interactive charts, so there‚Äôs quite a selection of tools to choose from.\nPersonally, I find echarts4r to be the best out of all of them specifically for exploratory data analysis. The charts are easy to create, they look nice out of the box, and it‚Äôs quick and easy to gain insight into your data. The interactive element makes identifying outliers or other specific values very easy. Compared to other packages, plotly is way too ugly, highcharter requires a license for paid applications, and ggiraph is too time consuming for regular charts. echarts4r offers a nice solution to all these problems (though if you need a deeper statistical breakdown, I‚Äôd recommend ggstatsplot).\nThere is a noticeable limit on customization, though. I‚Äôd still recommend ggplot2 for high quality static charts, and D3 for more advanced interactive ones.\necharts4r is a wrapper for the JS library developed by Apache Software Foundation, meaning changes in the original code will cause changes in this package, regardless of if echarts4r itself is updated. Apache ECharts is listed as one of Apache‚Äôs top projects, so hopefully, we see even more improvements in the future."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Nathan States",
    "section": "",
    "text": "Notes\n\n\nI‚Äôm currently in the process of moving my notes for various data tools from Notion into my blog. This page is entirely for personal use, but maybe someone can get some useful info out of it.\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\nSVG\n\n\nNathan States\n\n\n\n\nJan 10, 2023\n\n\necharts4r Cookbook\n\n\nNathan States\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/pcaob-part-1/index.html",
    "href": "blog/pcaob-part-1/index.html",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "",
    "text": "A couple years back, I remember reading a story published by the Project on Government Oversight titled, ‚ÄúHow an Agency You‚Äôve Never Heard of Is Leaving the Economy at Risk.‚Äù The agency in question is the Public Company Accounting Oversight Board (PCAOB), who is responsible for regulating the audit industry.\nThe article breaks down how the board has been influenced by politics, limited by budget restrictions, and directly manipulated by the accounting industry itself. Their conclusion - which they support by collecting data from two PCAOB databases - is that the agency isn‚Äôt effective, is lenient on enforcement, and likely not looking in the interests of investors.\nWhen the SEC was created in 1934, all publicly traded companies were required to be audited, but the audit industry itself remained almost entirely self-regulated. There were several noticeable incidents that cast doubt on such an arrangement, but none were greater than the collapse of Enron in 2002. At one point the 6th largest corporation in the United States, the energy giant would file for bankruptcy less than a year later once it was discovered that the vast majority of their profits were the result of fraud. Arthur Andersen, the firm who was receiving $52 million annually to ‚Äúaudit‚Äù Enron, was eventually charged (though never officially tried due to a technicality regarding jury instructions) by the DOJ for shredding documents related to Enron. In 2002, Congress passed the Sarbanes-Oxley Act that strengthened auditing standards, and more-or-less put the PCAOB in charge of enforcing them, along with some oversight from the SEC.\nIt‚Äôs an interesting read, but while the article does rely on some data analysis, it doesn‚Äôt do a full dive into each of the databases. Out of curiosity, I decided to scrape 20 years worth of inspection reports from the PCAOB to get a more data-driven view of the regulatory agency. Here‚Äôs what I found."
  },
  {
    "objectID": "blog/pcaob-part-1/index.html#results",
    "href": "blog/pcaob-part-1/index.html#results",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Results",
    "text": "Results\n\n# Load Libraries \nlibrary(arrow)\nlibrary(dplyr)\nlibrary(htmltools)\nlibrary(echarts4r)\nlibrary(reactable)\nlibrary(reactablefmtr)\n\n# Import Data \noverview <- read_csv_arrow(\"_data/overview.csv\")\ninspections <- read_csv_arrow(\"_data/inspections.csv\")\n\n# Theme Defaults \ne_common(\n  font_family = \"Georgia\"\n)\n\n\ninspections$year <- as.Date(as.character(inspections$year), format = \"%Y\")\ninspections$year <- substring(inspections$year, 1, 4)\n\nget_def_color <- function(def_rate) {\n  orange_pal <- function(x) rgb(colorRamp(c(\"#a43434\", \"#ffb150\"))(x), maxColorValue = 255)\n  normalized <- (def_rate - min(def_rate)) / (max(def_rate) - min(def_rate))\n  orange_pal(normalized)\n}\n\noverview <- overview |>\n  mutate(\n    fail_rate_domestic = domestic_audits_failed / domestic_audits_inspected,\n    fail_Rate_international = international_audits_failed / international_audits_inspected,\n    total_audits_inspected = domestic_audits_inspected + international_audits_inspected,\n    total_audits_failed = domestic_audits_failed + international_audits_failed,\n    total_fail_rate = total_audits_failed / total_audits_inspected\n  ) |>\n  mutate(\n    domestic_def_color = get_def_color(fail_rate_domestic),\n    international_def_color = get_def_color(fail_Rate_international),\n    total_def_color = get_def_color(total_fail_rate)\n  ) |>\n  select(Firm, domestic_audits_inspected, domestic_audits_failed, fail_rate_domestic, international_audits_inspected, international_audits_failed, fail_Rate_international, total_audits_inspected, total_audits_failed, total_fail_rate, domestic_def_color, international_def_color, total_def_color)\n\n\nfunction renderUserScore(cellInfo) {\n  return donutChart(cellInfo.value, cellInfo.row['score_color'])\n}\n\nfunction donutChart(value, color) {\n  // All units are in rem for relative scaling\n  const radius = 1.5\n  const diameter = 3.75\n  const center = diameter / 2\n  const width = 0.25\n  const sliceLength = 2 * Math.PI * radius\n  const sliceOffset = sliceLength * (1 - value / 100)\n  const donutChart = `\n    <svg width=\"${diameter}rem\" height=\"${diameter}rem\" style=\"transform: rotate(-90deg)\" focusable=\"false\">\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"rgba(0,0,0,0.1)\"></circle>\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"${color}\"\n       stroke-dasharray=\"${sliceLength}rem\" stroke-dashoffset=\"${sliceOffset}rem\"></circle>\n    </svg>\n  `\n  const label = `\n    <div style=\"position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%)\">\n      ${value}%\n    </div>\n  `\n  return `\n    <div style=\"display: inline-flex; position: relative\">\n      ${donutChart}\n      ${label}\n    </div>\n  `\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noverview |>\n  reactable(\n    columns = list(\n      Firm = colDef(cell = function(value) {\n        img_src <- knitr::image_uri(sprintf(\"_logos/%s.png\", value))\n        image <- img(src = img_src, height = \"31px\", alt = \"\")\n        tagList(\n          div(style = list(display = \"inline-block\", width = \"45px\"), image),\n          value\n        )\n      }),\n      domestic_audits_inspected = colDef(name = \"Inspected\"),\n      domestic_audits_failed = colDef(name = \"Failed\"),\n      fail_rate_domestic = colDef(\n        name = \"Def. %\", \n        format = colFormat(percent = TRUE, digits = 2),\n        html = TRUE,\n        align = \"center\",\n        width = 140,\n        class = \"user-score\",\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      international_audits_inspected = colDef(name = \"Inspected\"),\n      international_audits_failed = colDef(name = \"Failed\"),\n      fail_Rate_international = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2),\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      total_audits_inspected = colDef(name = \"Inspected\"),\n      total_audits_failed = colDef(name = \"Failed\"),\n      total_fail_rate = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2)\n      )\n    )\n  ) |>\n  add_title(\"PCAOB Inspection Results\")\n\n\nPCAOB Inspection Results\n\n\n\n\n\ninspections %>%\n  filter(country == \"United States\") %>%\n  filter(year > \"2008\") %>%\n  filter(firm != \"Marcum\") %>%\n  mutate(Fail_Rate = audits_failed / audits_inspected) %>%\n  group_by(firm) %>%\n  e_chart(year) %>%\n  e_line(Fail_Rate) %>%\n  e_y_axis(\n    formatter = e_axis_formatter(style = \"percent\")\n  ) %>%\n  e_legend(\n    type = \"scroll\",\n    bottom = 0.5\n  ) %>%\n  e_legend_unselect(\"Crowe\") %>%\n  e_legend_unselect(\"BDO\") %>%\n  e_legend_unselect(\"Grant Thornton\") %>%\n  e_legend_unselect(\"RSM\") %>%\n  e_color(\n    c(\"#1e1e1e\", \"#a29f00\", \"#249d24\", \"#ff7d08\", \"#68249d\", \"#1672b2\", \"#d52323\", \"#9d1774\")\n  ) %>%\n  e_title(\n    top = -5, \n    text = \"Domestic Big Four Firms Have An Average Deficiency Rate of 26.1%\",\n    left = \"center\",\n    textStyle = list(\n      fontSize = 20\n    )\n  ) %>%\n  e_axis_labels(\n    y = \"Deficiency Rate\"\n  ) %>%\n  e_toolbox_feature(\n    feature = \"saveAsImage\"\n  ) %>%\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    textStyle = list(\n      color = \"#fcfcfc\"\n    )\n  )"
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html",
    "href": "blog/2023/pcaob-part-1-inspections/index.html",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "",
    "text": "A couple years back, I remember reading a story published by the Project on Government Oversight (POGO) titled, ‚ÄúHow an Agency You‚Äôve Never Heard of Is Leaving the Economy at Risk.‚Äù The agency in question is the Public Company Accounting Oversight Board (PCAOB), who is responsible for regulating the audit industry. The audit industry was once almost entirely self-regulated, but that changed after the collapse of Enron in 2002.\nThe article starts by pointing out that, despite the PCAOB finding hundreds of audits performed deficiently among the four largest audits firms in the United States, the board rarely deals out punishment. While the board can fine firms up to $2,000,000 for each audit performed deficiently, they‚Äôve only fined the big four a total of $6,500,000 (not including their international affiliates). While the article was written in 2019, that number hasn‚Äôt changed as of February 2023.\nThe PCAOB classifies deficiencies into two categories:\nThe article continues by interviewing several former board members, asking them questions about the PCAOB budget, inner politics, and the overall lack of enforcement. They ultimately conclude that the board is weak and ineffective.\nIt‚Äôs an interesting read, but while the article does rely on some data analysis, it doesn‚Äôt fully analyze either of the databases. Out of curiosity, I decided to collect 20 years worth of inspection reports from the PCAOB to see if the data supports POGO‚Äôs conclusions. Here‚Äôs what I found."
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html#results",
    "href": "blog/2023/pcaob-part-1-inspections/index.html#results",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Results",
    "text": "Results\nCode for my programmers; scroll down to table if you don‚Äôt care.\nSetup: Here, we import the libraries we need along with the data.\n\n\nShow code\n# Load Libraries \nlibrary(arrow)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggiraph)\nlibrary(htmltools)\nlibrary(reactable)\nlibrary(reactablefmtr)\n\n# Import Data \noverview <- read_csv_arrow(\"_data/overview.csv\")\ninspections <- read_csv_arrow(\"_data/inspections.csv\")\n\n\nData Wrangling: Next, we fix the year column and create the inspection overview by summarizing the columns with dplyr. We also create a custom function for finding out color codes for inspection results, which will be used in the reactable table.\n\n\nShow code\n# Getting only the year from date column \ninspections$year <- as.Date(as.character(inspections$year), format = \"%Y\")\ninspections$year <- substring(inspections$year, 1, 4)\n\n# Defining a function for calculating color scale for def rates \nget_def_color <- function(def_rate) {\n  orange_pal <- function(x) rgb(colorRamp(c(\"#a43434\", \"#ffb150\"))(x), maxColorValue = 255)\n  normalized <- (def_rate - min(def_rate)) / (max(def_rate) - min(def_rate))\n  orange_pal(normalized)\n}\n\n# Calculating rates, totals, and select column order for domestic and international \noverview_domestic <- inspections |>\n  filter(!is.na(audits_inspected)) |>\n  filter(!is.na(audits_failed)) |>\n  filter(country == \"United States\") |>\n  group_by(firm) |>\n  summarise(\n    domestic_audits_inspected = sum(audits_inspected, na.rm = TRUE),\n    domestic_audits_failed = sum(audits_failed, na.rm = TRUE),\n    domestic_fail_rate = (domestic_audits_failed / domestic_audits_inspected) * 100,\n  ) |>\n  mutate(domestic_def_color = get_def_color(domestic_fail_rate))\n\noverview_international <- inspections |>\n  filter(!is.na(audits_inspected)) |>\n  filter(!is.na(audits_failed)) |>\n  filter(country != \"United States\") |>\n  group_by(firm) |>\n  summarise(\n    international_audits_inspected = sum(audits_inspected, na.rm = TRUE),\n    international_audits_failed = sum(audits_failed, na.rm = TRUE),\n    international_fail_rate = (international_audits_failed / international_audits_inspected) * 100,\n  ) |>\n  mutate(international_def_color = get_def_color(international_fail_rate))\n\noverview <- left_join(\n  x = overview_domestic,\n  y = overview_international,\n  by = \"firm\"\n)  |>\n  drop_na()\n\noverview <- overview |>\n  mutate(\n    total_audits_inspected = domestic_audits_inspected + international_audits_inspected,\n    total_audits_failed = domestic_audits_failed + international_audits_failed,\n    total_fail_rate = (total_audits_failed / total_audits_inspected) * 100,\n    total_def_color = get_def_color(total_fail_rate),\n    revenue = case_when(\n      firm == \"Deloitte\" ~ 59300000000,\n      firm == \"PricewaterhouseCoopers\" ~ 50300000000, \n      firm == \"Ernst & Young\" ~ 45400000000,\n      firm == \"KPMG\" ~ 34600000000,\n      firm == \"BDO\" ~ 12800000000,\n      firm == \"RSM\" ~ 8132000000,\n      firm == \"Grant Thornton\" ~ 2300000000,\n      firm == \"Crowe\" ~ 1062000000\n    ),\n    logo = case_when(\n            firm == \"Deloitte\" ~ \"_logos/deloitte.png\",\n      firm == \"PricewaterhouseCoopers\" ~ \"_logos/pwc.png\", \n      firm == \"Ernst & Young\" ~ \"_logos/e&y.png\",\n      firm == \"KPMG\" ~ \"_logos/kpmg.png\",\n      firm == \"BDO\" ~ \"_logos/bdo.png\",\n      firm == \"RSM\" ~ \"_logos/rsm.png\",\n      firm == \"Grant Thornton\" ~ \"_logos/thornton.png\",\n      firm == \"Crowe\" ~ \"_logos/crowe.png\"\n    )\n  ) |>\n  drop_na() |>\n  select(logo, firm, revenue, domestic_audits_inspected, domestic_audits_failed, domestic_fail_rate, international_audits_inspected, international_audits_failed, international_fail_rate, total_audits_inspected, total_audits_failed, total_fail_rate, domestic_def_color, international_def_color, total_def_color)\n\ninspections <- inspections |>\n  group_by(firm) |>\n  mutate(\n    Fail_Rate = round((audits_failed / audits_inspected) * 100, 2)\n  ) |>\n  mutate(tooltip = glue::glue(\n    \"<p><strong>{firm} {year}: </strong></p>: \",\n    \"<p>Audits Inspected: {audits_inspected}</p> \",\n    \"<p>Audits Deficient: {audits_failed}</p> \",\n    \"<p>Deficiency Rate: <strong>{Fail_Rate}%</strong></p> \"\n  )\n)\n\n\nJavascript for reactable Table: The donut charts created in the table are SVG generated using javascript. Taken from an example off the official documentation.\n\n\nShow code\nfunction renderUserScore(cellInfo) {\n  return donutChart(cellInfo.value, cellInfo.row['score_color'])\n}\n\nfunction donutChart(value, color) {\n  // All units are in rem for relative scaling\n  const radius = 1.5\n  const diameter = 3.75\n  const center = diameter / 2\n  const width = 0.25\n  const sliceLength = 2 * Math.PI * radius\n  const sliceOffset = sliceLength * (1 - value / 100)\n  const donutChart = `\n    <svg width=\"${diameter}rem\" height=\"${diameter}rem\" style=\"transform: rotate(-90deg)\" focusable=\"false\">\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"rgba(0,0,0,0.1)\"></circle>\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"${color}\"\n       stroke-dasharray=\"${sliceLength}rem\" stroke-dashoffset=\"${sliceOffset}rem\"></circle>\n    </svg>\n  `\n  const label = `\n    <div style=\"position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%)\">\n      ${value}%\n    </div>\n  `\n  return `\n    <div style=\"display: inline-flex; position: relative\">\n      ${donutChart}\n      ${label}\n    </div>\n  `\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReactable Theme\n\n\nShow code\nnytimes <- function(\n    font_size = 13,\n    font_color = \"#333333\",\n    header_font_size = 11,\n    header_font_color = \"#999999\",\n    background_color = NULL,\n    border_color = \"#e7e7e7\",\n    border_width = \"1px\",\n    cell_padding = 5,\n    centered = FALSE\n  ) {\n  reactableTheme(\n    cellStyle = centered_content,\n    color = font_color,\n    backgroundColor = background_color,\n    borderWidth = border_width,\n    borderColor = border_color,\n    stripedColor = \"#e7e7e7\",\n    highlightColor = \"#eeeeee\",\n    cellPadding = cell_padding,\n    tableStyle = list(fontSize = font_size),\n    headerStyle = list(\n      borderWidth = \"0px\",\n      padding = \"5px\",\n      background = \"#ffffff\",\n      borderColor = \"#ffffff\",\n      color = header_font_color,\n      fontWeight = \"500\",\n      textTransform = \"uppercase\",\n      fontSize = header_font_size\n    ),\n    groupHeaderStyle = list(\n      \"&:not(:empty)\" = list(\n        borderWidth = \"0px\",\n        backgroundColor = \"#ffffff\",\n        textTransform = \"uppercase\",\n        fontSize = header_font_size,\n        borderColor = \"#ffffff\",\n        color = font_color\n      )\n    ),\n    searchInputStyle = list(color = \"#333333\",\n                            fontSize = \"13px\"),\n    inputStyle = list(backgroundColor = \"#ffffff\", color = \"#333333\"),\n    rowSelectedStyle = list(backgroundColor = \"#e9edf0\"),\n    selectStyle = list(color = \"#333333\"),\n    pageButtonStyle = list(color = \"#333333\", fontSize = \"14px\"),\n    paginationStyle = list(color = \"#333333\", fontSize = \"14px\")\n  )\n}\n\n\nTable\n\n\nShow code\noverview |>\n  reactable(\n    columns = list(\n      Firm = colDef(cell = function(value) {\n        img_src <- knitr::image_uri(sprintf(\"_logos/%s.png\", value))\n        image <- img(src = img_src, width = \"60px\", height = \"32px\", alt = \"\")\n        tagList(\n          div(style = list(display = \"inline-block\", width = \"45px\"), image),\n          value\n        )\n      }),\n      domestic_audits_inspected = colDef(name = \"Inspected\"),\n      domestic_audits_failed = colDef(name = \"Failed\"),\n      fail_rate_domestic = colDef(\n        name = \"Def. %\", \n        format = colFormat(percent = TRUE, digits = 2),\n        html = TRUE,\n        align = \"center\",\n        width = 140,\n        class = \"user-score\",\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      international_audits_inspected = colDef(name = \"Inspected\"),\n      international_audits_failed = colDef(name = \"Failed\"),\n      fail_Rate_international = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2),\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      total_audits_inspected = colDef(name = \"Inspected\"),\n      total_audits_failed = colDef(name = \"Failed\"),\n      total_fail_rate = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2)\n      )\n    )\n  ) |>\n  add_title(\"PCAOB Inspection Results\")\n\n\nLet me give a quick breakdown of what you‚Äôre looking at.\nAbove are the five largest audit firms in the United States, who audit over half of all publicly traded companies, including every single company on the S&P 500. I‚Äôve also included their 2022 revenue amounts.\nThe PCAOB will issue annual reports for the largest firms, where they will investigate a select number of audits to see if they were conducted properly. They categorize deficiencies into two different types.\nType 1.A deficiencies are defined as:\n\nDeficiencies that were of such significance that we believe the firm, at the time it issued its audit report(s), had not obtained sufficient appropriate audit evidence to support its opinion(s) on the issuer‚Äôs financial statements and/or ICFR.\n\nType 1.B deficiencies are defined as:\n\nDeficiencies that do not relate directly to the sufficiency or appropriateness of evidence the firm obtained to support its opinion(s) but nevertheless relate to instances of non-compliance with PCAOB standards or rules.\n\nThe latter of these deficiencies aren‚Äôt disclosed in any meaningful way, which is common for the PCAOB. For example, tests related to Type 1.B deficiencies aren‚Äôt performed on all audits, so it‚Äôs not possible to determine how prevalent they are among all audits.\nIn the table, only Type 1.A deficiencies are listed, and they are broken down between each firm‚Äôs domestic and international affiliates. The amounts are from 2009 - 2021 unless stated otherwise. So, for example, to read this chart, domestic affiliates of BDO failed __% of PCAOB investigations from 2009 - 2020.\nOn that note, it should be stated that the PCAOB uses a risk-based selection process when choosing which audits to investigate. This means that deficiency rates aren‚Äôt representative of how often the average audit by a given firm will be deficient.\nThis does mean, though, that the PCAOB is quite accurate at assessing which audits are likely to be conducted deficiently or not. Being able to choose audits at a near 50% rate is insanely high regardless of the selection process, but we can‚Äôt say anything overall performance of the firm. After all, it could be something else. Maybe the PCAOB especially hates KPMG and BDO for some reason.\nSince 2016, the PCAOB began including 10 randomly selected audits among each of the big four firms (6 for BDO) for their reports. While the reports themselves don‚Äôt state whether or not a randomly selected audit turned out to be deficient or not, we can reasonably assume that audits chosen randomly are less deficient on average. The number of audits investigated has stayed roughly the same (around 52-58, on average), so deficiency rates from 2016 onwards are likely underestimates.\nSide note: why doesn‚Äôt the PCAOB disclose whether randomly selected audits were deficient or not? That would actually be useful to know,"
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html#breakdown-by-year",
    "href": "blog/2023/pcaob-part-1-inspections/index.html#breakdown-by-year",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Breakdown by Year",
    "text": "Breakdown by Year\nFirm deficiency rates have changed significantly from year to year since 2009. Reports prior to that year don‚Äôt include the total number of audits inspected, which is why they‚Äôre not included in the table above, and the chart below. Oddly, Deloitte reports from 2007-2008 and the PwC 2008 do include these numbers, though not for any other firm. These are the results for domestic firms.\n\n\nShow code\ninspections |>\n  filter(country == \"United States\") |>\n  filter(year > \"2008\") |>\n  filter(firm != \"Marcum\") |>\n  mutate(Fail_Rate = audits_failed / audits_inspected) |>\n  group_by(firm) |>\n  e_chart(year) |>\n  e_line(Fail_Rate) |>\n  e_y_axis(\n    formatter = e_axis_formatter(style = \"percent\")\n  ) |>\n  e_legend(\n    type = \"scroll\",\n    bottom = 0.5\n  ) |>\n  e_legend_unselect(\"Crowe\") |>\n  e_legend_unselect(\"BDO\") |>\n  e_legend_unselect(\"Grant Thornton\") |>\n  e_legend_unselect(\"RSM\") |>\n  e_color(\n    c(\"#1e2023\", \"#5f9fa0\", \"#49a84c\", \"#f6bc00\", \"#960ff9\", \"#4c86f9\", \"#e1432e\", \"#9d1774\")\n  ) |>\n  e_axis_labels(\n    y = \"Deficiency Rate\"\n  ) |>\n  e_title(\n     text = \"Domestic Inspection Results\",\n     right = 30,\n     textStyle = list(\n       fontSize = 26\n     )\n  ) |>\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    textStyle = list(\n      color = \"#fcfcff\"\n    )\n  )\n\n\n\n\n\n\n\nI disabled other firms so the chart isn‚Äôt too cluttered. You can activate them by clicking them in the legend.\nEach of the four largest firms have performed the worst in at least one year, but KPMG has shown some impressive consistency, finishing with the worst deficiency rate in 7 out of the last 8 years (besides in 2019, where they finished 1% better than PwC - the worst firm that year). Outside of them, BDO flies high above in terms of poor performance, while other firms follow somewhat similar trends, though RSM results have been sporadic. There aren‚Äôt many international reports on firms outside of the big four, so I‚Äôve left them out in the chart below. Here are the results for international firms.\n\n\nShow code\ninspections |>\n  filter(country != \"United States\") |>\n  filter(year > \"2006\") |>\n  filter(firm %in% c(\"Deloitte\", \"PricewaterhouseCoopers\", \"Ernst & Young\", \"KPMG\")) |>\n  filter(!is.na(audits_inspected)) |>\n  group_by(firm, year) |>\n  summarise(\n    audits_inspected = sum(audits_inspected, na.rm = TRUE),\n    audits_failed = sum(audits_failed, na.rm = TRUE)\n  ) |>\n  mutate(Fail_Rate = audits_failed / audits_inspected) |>\n  e_chart(year) |>\n  e_line(Fail_Rate) |>\n  e_y_axis(\n    formatter = e_axis_formatter(style = \"percent\")\n  ) |>\n  e_legend(\n    type = \"scroll\",\n    bottom = 0.5\n  ) |>\n  e_color(\n    c(\"#275b29\", \"#a98100\", \"#345cac\", \"#942c1e\")\n  ) |>\n  e_axis_labels(\n    y = \"Deficiency Rate\"\n  ) |>\n  e_title(\n    text = \"International Inspection Results\"\n  ) |>\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    textStyle = list(\n      color = \"#fcfcff\"\n    )\n  )\n\n\n\n\n\n\nIf domestic results were more steady and consistent, than international rates are much more irregular. This is partly because of fewer observations, but also because individual countries tended to tank performance. For example, PCAOB found that Deloitte Canada had performed 37 out of 48 audits deficiently from 2010-2015, or a 77% deficiency rate.\nCountry performance was somewhat consistent among the firms. Both KPMG and PwC Canadian affiliates also had 50%+ deficiency rates. Meanwhile, no Bermuda affiliates have deficiency rates above 25%.\nRemember when I said how inspection reports prior to 2009 don‚Äôt include the total number of audits inspected? For some reason, international affiliates have always included this number, even for reports dating back as early as 2005. Out of all the reports I‚Äôve analyzed, though, reports from KPMG Canada from 2005-2008 were the only international reports that did not include the total number of audits inspected. I‚Äôm not sure why they‚Äôre so inconsistent about this number."
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html#other-information",
    "href": "blog/2023/pcaob-part-1-inspections/index.html#other-information",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Other Information",
    "text": "Other Information\nI‚Äôm not really sure what purpose this is supposed to serve form an investor perspective, but reports after 2014 include the revenue range, and industry sector of each audit inspected. Given that company names aren‚Äôt disclosed, it‚Äôs hard to know if this information has literally any use at all, but here it is anyways.\nIndustry Sectors.\n\n\nShow code\ninspections_category |>\n  group_by(category) |>\n  summarise(\n    Audits_Inspected = sum(`Audits Inspected`, na.rm = TRUE),\n    Audits_Deficient = sum(`Audits Deficient`, na.rm = TRUE)\n  ) |> \n  mutate(total = Audits_Inspected + Audits_Deficient) |>\n  arrange(total) |>\n  e_chart(category) |>\n  e_bar(Audits_Inspected, stack = \"group\") |>\n  e_bar(Audits_Deficient, stack = \"group\") |>\n  e_legend(show = FALSE) |>\n  e_flip_coords() |>\n  e_title(\n    text = \"Industry Ranges\",\n    left = \"left\",\n    top = -5,\n    textStyle = list(\n      fontSize = 25\n    )\n  ) |>\n  e_color(c(\"#0e437c\", \"#7393ad\")) |>\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    textStyle = list(\n      color = \"#fcfcff\"\n    )\n  )\n\n\n\n\n\n\nRevenue Ranges.\n\n\nShow code\ninspections_revenue |>\n  group_by(category) |>\n  summarise(\n    Audits_Inspected = sum(`Audits Inspected`, na.rm = TRUE),\n    Audits_Deficient = sum(`Audits Deficient`, na.rm = TRUE)\n  ) |> \n  mutate(total = Audits_Inspected + Audits_Deficient) |>\n  arrange(total) |>\n  e_chart(category) |>\n  e_bar(Audits_Inspected, stack = \"group\") |>\n  e_bar(Audits_Deficient, stack = \"group\") |>\n  e_legend(show = FALSE) |>\n  e_flip_coords() |>\n  e_title(\n    text = \"Revenue Ranges\",\n    left = \"left\",\n    top = -5,\n    textStyle = list(\n      fontSize = 25\n    )\n  ) |>\n  e_color(c(\"#0e437c\", \"#7393ad\")) |>\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    textStyle = list(\n      color = \"#fcfcff\"\n    )\n  )\n\n\n\n\n\n\nBefore I say anything, I‚Äôd like to point out that one of the reports listed a category as ‚ÄúOther‚Äù for exactly one deficient audit. That category name has never been used again. Anyways;\nThere‚Äôs not much to say. There‚Äôs no data on the outcomes of companies who routinely receive deficient audits, so there isn‚Äôt any reason to choose your investing strategies based on these results."
  },
  {
    "objectID": "blog/2023/welcome/index.html",
    "href": "blog/2023/welcome/index.html",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "",
    "text": "To all my eight viewers who happened to stumble upon the initial release of my blog - you might of noticed it looks drastically different now. When I first built this site, I was using blogdown along with Hugo Ap√©ro, but I‚Äôm making the switch over to Quarto from now on.\nHere are some reasons why you should consider migrating as well."
  },
  {
    "objectID": "blog/2023/welcome/index.html#quarto-is-easy.",
    "href": "blog/2023/welcome/index.html#quarto-is-easy.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "1. Quarto is easy.",
    "text": "1. Quarto is easy.\nblogdown is essentially the same as Hugo, but with the ability to compile RMarkdown files. Some themes are very easy to work with, while others can be a bit of a pain. Quarto is extremely simple, and it comes with all the tools anyone would need for blogging. If you want a website where you won‚Äôt have to worry about compiling issues, Quarto is very appealing.\nHugo Ap√©ro, the theme I used previously, is actually designed really well in this respect, so this may or may not be a problem for you. It‚Äôs very theme dependent."
  },
  {
    "objectID": "blog/2023/welcome/index.html#not-just-for-blogging.",
    "href": "blog/2023/welcome/index.html#not-just-for-blogging.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "2. Not just for blogging.",
    "text": "2. Not just for blogging.\nQuarto is capable of creating much more than blogs; presentations, PDF reports, interactive documents, and easy documentation pages are all supported. You can view some examples in their gallery.\nThere‚Äôs obviously a good amount of carryover between these formats and this blog, so it‚Äôs good practice for the real world down the line."
  },
  {
    "objectID": "blog/2023/welcome/index.html#it-supports-multiple-languages",
    "href": "blog/2023/welcome/index.html#it-supports-multiple-languages",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "3. It supports multiple languages",
    "text": "3. It supports multiple languages\nTechnically, blogdown does as well; you can use reticulate, and RMarkdown can still compile Javascript. However, Quarto uses a custom .qmd format that includes these formats natively, which is nicer than using packages."
  },
  {
    "objectID": "blog/2023/welcome/index.html#awesome-visual-bar",
    "href": "blog/2023/welcome/index.html#awesome-visual-bar",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "4. Awesome Visual Bar",
    "text": "4. Awesome Visual Bar\nThis probably falls under point #1, but it‚Äôs still neat. The visual tab makes creating documents incredibly intuitive and easy, and includes almost everything you‚Äôd want out of a modern text editor.\n\n\n\nExample"
  },
  {
    "objectID": "blog/2023/welcome/index.html#its-compatible-with-hugo.",
    "href": "blog/2023/welcome/index.html#its-compatible-with-hugo.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "5. It‚Äôs compatible with Hugo.",
    "text": "5. It‚Äôs compatible with Hugo.\nMy old posts were written in RMarkdown, but Quarto only supports qmd. Hugo understands this format perfectly fine, though, so if I were to ever go back to blogdown, it would be incredibly easy to migrate. All it would take is some copy-and-paste with few revisions."
  },
  {
    "objectID": "blog/2023/welcome/index.html#superior-to-distill",
    "href": "blog/2023/welcome/index.html#superior-to-distill",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "6. Superior to Distill?",
    "text": "6. Superior to Distill?\nI don‚Äôt think the advantages of Quarto are so significant that it mandates switching over from Distill if you are currently using it for blogging. With that said, while both offer blogs that are similar in style, Quarto has more features and support. If you are creating your first blog, there isn‚Äôt much of a reason to choose Distill over Quarto in my opinion."
  },
  {
    "objectID": "blog/2023/welcome/index.html#posts-to-read-about-quarto",
    "href": "blog/2023/welcome/index.html#posts-to-read-about-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "Posts to Read About Quarto",
    "text": "Posts to Read About Quarto\nHere is a github that has a ton of links related to making awesome things in Quarto. Here are some posts I‚Äôd recommend reading:\n\nAlison Hill: We don‚Äôt talk about Quarto..\nDanielle Navarro: Posting a Distill blog to Quarto.\nYihui Xie: With Quarto Coming, is RMarkdown Going Away? No.\n\nOn that last point, I‚Äôll briefly discuss the‚Ä¶"
  },
  {
    "objectID": "blog/2023/welcome/index.html#disadvantages-of-quarto",
    "href": "blog/2023/welcome/index.html#disadvantages-of-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "Disadvantages of Quarto",
    "text": "Disadvantages of Quarto\nReally, the main one is lack of customization. Hugo is far more complex, and can create some sophisticated websites to suit any need. Quarto, not so much. This does make blogging on this platform feel slightly generic in comparison, unfortunately. Optimized Hugo sites might also be faster, though admittedly, I have no idea if this is likely / true or not.\nWith that said, the pros outweigh the cons in my case, which is why I decided to make the jump. I hope you enjoy the new look."
  },
  {
    "objectID": "blog/2023/sipri-dashboard-announcement/index.html",
    "href": "blog/2023/sipri-dashboard-announcement/index.html",
    "title": "SIPRI Dashboard Announcement",
    "section": "",
    "text": "I found it surprisingly hard to find a website that displayed military spending around the world in an easy and concise format, so I decided to make my own. Make sure to check it out here.\n\n\n\n\nPicture of Dashboard"
  },
  {
    "objectID": "blog/2023/covid-19-stimulus-database-part-1/index.html",
    "href": "blog/2023/covid-19-stimulus-database-part-1/index.html",
    "title": "Building a COVID-19 Economic Relief Database: Part 1 (usaspending.gov)",
    "section": "",
    "text": "Congress has awarded over $4 trillion in loans, grants, contracts, or other financial assistance as part of over 70 various COVID-19 related economic relief programs. Currently, there isn‚Äôt an easy way to view that data where all that money is going, but I want to change that by building an easily accessible database that makes viewing total and outlay amounts simple.\nThere are two primary ways we can go about this.\nThe first is to gather the data manually. Most agencies will let you download records from their websites in .csv format, but others are much more difficult, sometimes offering data in PDFs only. Some agencies don‚Äôt provide updates on their award data, and there‚Äôs often inconsistencies in how data is stored between agencies. This means tracking outlay amounts are impossible. While this method would give us much more control over how the data is inputted, it would ultimately be too time consuming to be realistic.\nThe other way is to use the ‚Äúcurated‚Äù database from usaspending.gov. You can find it here with accompanying instructions here."
  },
  {
    "objectID": "blog/2023/covid-19-stimulus-database-part-1/index.html#download-center",
    "href": "blog/2023/covid-19-stimulus-database-part-1/index.html#download-center",
    "title": "Building a COVID-19 Economic Relief Database: Part 1 (usaspending.gov)",
    "section": "Download Center",
    "text": "Download Center\nHere is a screenshot of the instructions on how to get the data.\n\nThere are three different file types to choose from, but as the page describes, File C data ‚Äúprovides a higher degree of granularity in breaking down award spending by several financial data dimensions.‚Äù This is because calculating awards given by Congress isn‚Äôt as simple as looking at total face amounts, which I‚Äôll explain more when constructing the database.\nIn total, the download is about 10GB compressed. Uncompressed, the data is spread across 139 Excel files that contain 1,000,000 rows. Excel can‚Äôt contain a spreadsheet with over a million rows, and .csv is already incredibly inefficient for analyzing such a large dataset, so we will need some special tools in order to properly analyze it.\nLike I said earlier, ‚Äúcurated‚Äù database my ass.\nNow, you might be thinking to yourself; if each Excel file contains a million rows, and there‚Äôs 139 of them, are their really 139,000,000 million loans/grants/contracts related to COVID-19? Not even close. Each row in our data doesn‚Äôt represent an individual recipient award, but an updated entry for that recipient award.\nSay I were to give you a loan for $10 million, and you were to take $2 million out of the bank. This would show up as two different entries in the database; one for the face loan amount, and one showing the extracted amount, or the award outlay. Now let‚Äôs say you pay $100,000 on interest on that loan. Then, it will show up as another entry in the database, but instead, for a negative amount.\nThis allows us to do two things. For one, we can track how much money has actually been given as opposed to how much has been outlayed. Furthermore, we can do things like calculate whether certain loans were forgiven or not, which is something you can actually already view using the ProPublica database. They got their data directly from the Small Business Administration, but because this database is collected from all other agencies, we can make our own.\nDifferent agencies record data slightly different, so information is not consistent across the database. Here‚Äôs the explanation given by usaspending.gov.\n\nIn order to understand the data surfaced in the ‚ÄúAward Spending‚Äù sections (detailed below), it is important to understand the concept of linking between Broker File C and FPDS/FABS award data. Broker File C serves as a bridge between data sourced from agency financial systems (i.e., the data in Broker File C itself) and award data sourced from FPDS and FABS. The actual link between these two datasets is an award ID (also known as award unique key). For various reasons, not every award ID in Broker File C has a corresponding award ID in FPDS or FABS data, which makes them unmatchable. If a Broker File C row cannot be matched to FPDS or FABS, we call it ‚Äúunlinked‚Äù. Unlinked Broker File C data cannot be supplemented by metadata from FPDS or FABS (including recipient information, CFDA program, and funding agency).\nThe rule of thumb for all award sections is to use complete Broker File C data where possible (containing both linked and unlinked awards); where not possible, only linked data will be used (representing a subset of the authoritative award spending total based on both linked and unlinked data in Broker File C).\n\nWhen we analyze the data, we can calculate the number of NAs to see where we are missing most of our information, but basically, it‚Äôs not all there.\n\nA Brief Demo\nThere are some special instructions to make sure that we get the right calculations when crunching the dataset.\n\nBasically, we group by the column, award_unique_key, and use dplyr::summarise to sum obligation and outlay amounts.\nLet‚Äôs load in one of the files to test this out on. We‚Äôll count the number of entries for each award_unique_key and sum the obligation amount.\n\n\nShow code\n# Reading in the data using Arrow; explained later \ntest <- arrow::read_csv_arrow(\"test.csv\")\n\ntest %>% \n  group_by(award_unique_key) %>%\n  summarise(\n    count = n(),\n    total = sum(transaction_obligated_amount)\n  ) %>%\n  arrange(desc(count))\n\n\nHere are the results printed to console.\n\nFor reference, there are close to 400,000 rows in the file, but when grouping by award_unique_key, we see that there are only 160,000 unique award keys in our dataset. One of the recipients has over 200 entries for award obligations totaling $110,309,034. For fun, let‚Äôs see who the recipient is.\n\n\nShow code\ntest %>%\n  filter(award_unique_key == \"ASST_NON_4488DRNJP00000001_7022\") %>% \n  group_by(award_unique_key) %>%\n  summarise(\n    Recipient = unique(recipient_parent_name),\n    State = unique(recipient_state),\n    Award_Category = unique(award_type),\n    Obligations = sum(transaction_obligated_amount),\n    Agency = unique(awarding_agency_name),\n    Description = unique(prime_award_base_transaction_description),\n    Last_Modified = unique(award_latest_action_date)\n  )\n\n\nThis results in a 1x1 tibble.\n\n\n\nSo, the recipient is to the local township of Berkeley Heights, New Jersey, and the award type was a grant. The award is from the Department of Homeland Security, specifically FEMA, for the purpose of the ‚Äúrepair or replacement of disaster damaged facilities.‚Äù The original award was handed out on April 8th, 2020, but as we can see, the city is still receiving payments as of recently.\nRemember, these amounts were compiled for only one file, but there are 139 of them in total. There are entries scattered across other files, which means there‚Äôs no guarantee that Berkeley Heights didn‚Äôt receive more in aid in another file. What we need to do is combine all these files together before doing any sort of computations on them.\nThis is a problem, though. The way R loads data is by storing it into RAM, which means that in order to perform computations on the entire dataset, we‚Äôd need about 140GB of RAM. If you‚Äôre not sure how much RAM your computer has, there‚Äôs a good chance it‚Äôs between 4GB - 16GB, and high end computers generally cap out at 64GB. My laptop has 16GB. In other words, even the most powerful commercial computers wouldn‚Äôt be able to load the data using base R.\nIs this the end? Well, luckily, there are plenty of big data tools that make it possible to load and perform data analysis on massive data sets without the need of external support. While there are many available, I‚Äôll be opting for Apache Arrow, which will be the focus of my next post. Peace."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "My posts, charts, and data are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\n  \n\nYou are free to:\n\nShare: copy and redistribute the material in any medium or format.\nAdapt: remix, transform, and build upon the material for any purpose, even commercially.\n\nUnder the following terms:\n\nAttribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\nShareAlike: If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original."
  },
  {
    "objectID": "notes/echarts4r/index.html#a.-bar-chart",
    "href": "notes/echarts4r/index.html#a.-bar-chart",
    "title": "echarts4r Cookbook",
    "section": "2a. Bar Chart",
    "text": "2a. Bar Chart\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_legend(show = FALSE) |>\n  e_axis_labels(\n    x = \"Island\",\n    y = \"Averge Mass\"\n  ) |>\n  e_title(\n    text = \"Some meaningful title.\",\n    subtext = \"Plus some more useful info.\"\n  ) |>\n  e_tooltip(axis = \"trigger\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#a.-stable-install",
    "href": "notes/echarts4r/index.html#a.-stable-install",
    "title": "echarts4r Cookbook",
    "section": "1.A. Stable Install",
    "text": "1.A. Stable Install\nGithub Page.\n\ninstall.packages(\"echarts4r\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#b.-developmental-build",
    "href": "notes/echarts4r/index.html#b.-developmental-build",
    "title": "echarts4r Cookbook",
    "section": "1.B. Developmental Build",
    "text": "1.B. Developmental Build\nRequites the remotes package to install.\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"JohnCoene/echarts4r\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#c.-complimentary-packages",
    "href": "notes/echarts4r/index.html#c.-complimentary-packages",
    "title": "echarts4r Cookbook",
    "section": "1.C. Complimentary Packages",
    "text": "1.C. Complimentary Packages\nNeither of these packages have been updated in years, but are still usable in their current formats. They allow for maps and image assets to be used while charting.\n\nremotes::install_github('JohnCoene/echarts4r.assets')\nremotes::install_github('JohnCoene/echarts4r.maps')"
  },
  {
    "objectID": "notes/echarts4r/index.html#d.-loading-packages",
    "href": "notes/echarts4r/index.html#d.-loading-packages",
    "title": "echarts4r Cookbook",
    "section": "1.D. Loading Packages",
    "text": "1.D. Loading Packages\nAlso loading dplyr for data manipulation and palmerpenguins for dummy data.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(palmerpenguins)\nlibrary(echarts4r)\nlibrary(echarts4r.maps)\n\ne_common(\n  font_family = \"Georgia\",\n  theme = \"dark-mushroom\"\n)"
  },
  {
    "objectID": "notes/echarts4r/index.html#a.-column-chart",
    "href": "notes/echarts4r/index.html#a.-column-chart",
    "title": "echarts4r Cookbook",
    "section": "2.A. Column Chart",
    "text": "2.A. Column Chart\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_legend(show = FALSE) |>\n  e_axis_labels(\n    x = \"Island\",\n    y = \"Averge Mass\"\n  ) |>\n  e_title(\n    text = \"Some meaningful title.\"\n  ) |>\n  e_tooltip(trigger = \" axis\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#b.-bar-chart",
    "href": "notes/echarts4r/index.html#b.-bar-chart",
    "title": "echarts4r Cookbook",
    "section": "2.B. Bar Chart",
    "text": "2.B. Bar Chart\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_flip_coords() |> # THIS LINE OF CODE # \n  e_tooltip()"
  },
  {
    "objectID": "notes/echarts4r/index.html#c.-scatter-chart",
    "href": "notes/echarts4r/index.html#c.-scatter-chart",
    "title": "echarts4r Cookbook",
    "section": "2.C. Scatter Chart",
    "text": "2.C. Scatter Chart\n\npenguins |>\n  group_by(species) |>\n  e_chart(bill_length_mm) |>\n  e_scatter(bill_depth_mm) |>\n  e_tooltip()"
  },
  {
    "objectID": "projects/sipri-dashboard/index.html",
    "href": "projects/sipri-dashboard/index.html",
    "title": "ü™ñ SIPRI Dashboard",
    "section": "",
    "text": "Launch  Source"
  },
  {
    "objectID": "archives.html",
    "href": "archives.html",
    "title": "Nathan States",
    "section": "",
    "text": "Archives\nüöß UNDER CONSTRUCTION üöß\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html",
    "href": "blog/2023/bengaluru-text-mining/index.html",
    "title": "Bengaluru Text Mining",
    "section": "",
    "text": "Aerial view of Bengaluru, India."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#background",
    "href": "blog/2023/bengaluru-text-mining/index.html#background",
    "title": "Bengaluru Text Mining",
    "section": "Background",
    "text": "Background\nText data can be difficult to analyze in large amounts, but raw text is invaluable in numerous different ways. Using simple Python libraries, modern machine learning models can parse thousands of rows in seconds, which can be used for a variety of purposes. One of the most common of these is classification, or categorizing text into different groups.\nThe Bruhat Bengaluru Mahangara Palike (BBMP) - an administrative body that oversees city development in Bengaluru, the largest tech city in India - created a web application that allows citizens to file grievances with the city. From February 8th, 2020 to February 21st, 2021, a total of 105,956 complaints were filed to BBMP, or about 280 a day. Exploring this data not only provides insight into the most common problems facing this city (or at least the complaints most likely to be sent), but also presents an opportunity to quantify and categorize them.\nIn the dataset, complaints have been manually categorized by the administrators who oversee the app at BBMP, but this is extremely inefficient. Usefully, though, the developers have already created categories that they felt best sorted the data, which means, assuming complaints don‚Äôt change substantially in the future, we can train a machine learning model that performs this task automatically. This could save hours and hours of time.\nBecause the categories are already defined, this will be a supervised classification model."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#data-wrangling",
    "href": "blog/2023/bengaluru-text-mining/index.html#data-wrangling",
    "title": "Bengaluru Text Mining",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nData wrangling and EDA done using R.\nFirst, we import the data and the libraries we will be using.\n\n\nShow code\n# Load Libraries\nlibrary(tidyverse) \nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(echarts4r)\nlibrary(here)\nlibrary(lubridate)\nlibrary(wordcloud2)\nlibrary(reticulate)\n\n# Set Directory \nhere::set_here()\n\n# Import Data\ngrievances <- readr::read_csv(\"bengaluru-grievances.csv\")\n\n\nThe admins at BBMP keep their data neat and tidy, so there‚Äôs not many problems to fix. There are a couple things to consider, however.\nThe first issue is that some descriptions are extremely short, making classifying them accurately near impossible. We can limit the number of characters that a complaint must have, though the appropriate number of rows to remove is debatable. Ideally, we don‚Äôt want to exclude too much of the data while still removing descriptions too short to be properly categorized.\nUsing str_length from the stringr package, we see 5,392 complaints contained fewer than 12 characters, meaning removing them would still preserve 95% of the original data. Using filter from dplyr, we can keep all complaints containing more than 12 characters.\n\n\nShow code\n# Counting number of rows \ngrievances %>%\n  filter(str_length(description) < 12) %>%\n  count() \n\n\n# A tibble: 1 x 1\n      n\n  <int>\n1  5392\n\n\n\n\nShow code\n# Removing those rows \ngrievances <- grievances %>%\n  filter(str_length(description) > 12)\n\n\nThe other consideration is whether the existing categories accurately reflect the data or not. It‚Äôs possible certain similarities between different categories would better be combined into one, and likewise, single categories that should be multiple ones. These changes might not only provide a better description of the data, but improve accuracy in the long run.\nFor now, we will leave the original categories intact and proceed, but future models may benefit from this step."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#exploratory-data-analysis",
    "href": "blog/2023/bengaluru-text-mining/index.html#exploratory-data-analysis",
    "title": "Bengaluru Text Mining",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nInteractive charts created using echarts4r.\n\nNumber of Grievances By Day\n\n\nShow code\n# Set Theme \ne_common(\n  font_family = \"Georgia\"\n)\n\n# Chart \ngrievances %>%\n  group_by(created_at = as.Date(created_at)) %>%\n  summarise(Total = n()) %>%\n  e_charts(created_at) %>%\n  e_line(Total, symbol = \"none\") %>%\n  e_x_axis(axisLabel = list(interval = 0)) %>%\n  e_title(\n    text = \"Total number of grievances by day\",\n    subtext = \"Data: BBMP\"\n  ) %>%\n  e_color(\n    \"#0a32d2\", \n    background = \"rgb(0, 0, 0, 0)\"\n  ) %>%\n  e_legend(show = FALSE) %>%\n  e_tooltip(\n    backgroundColor = \"rgba(20, 20, 20, 0.5)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    textStyle = list(\n      color = \"#ffffff\"\n    ),\n    trigger = \"axis\"\n  )\n\n\n\n\n\n\nOn most days, grievances would vary between 200 to 400 a day, with some spikes in September onwards, including a massive single-day one in March of 742. On average, 280 complaints were filed each day.\nBecause complaints don‚Äôt seem to fluctuate significantly, we can likely assume that the day the grievance was filed isn‚Äôt indicative of its contents.\n\n\nGrievances by Category\n\n\nShow code\ngrievances %>%\n  group_by(category) %>%\n  summarise(Total = n()) %>%\n  arrange(desc(Total)) %>%\n  filter(Total > 1000) %>%\n  slice(1:10) %>%\n  e_charts(category) %>%\n  e_bar(Total) %>%\n  e_x_axis(\n    axisLabel = list(\n      interval = 0, \n      rotate = 45,\n      fontSize = 9.25\n    )\n  ) %>%\n  e_title(\n    text = \"Most common grievances by category\", \n    subtext = \"Only categories above 1,000 visible\"\n  ) %>%\n  e_legend(show = FALSE) %>%\n  e_labels(show = FALSE) %>%\n  e_color(\n    \"#8c5ac8\", \n    background = \"rgb(0, 0, 0, 0)\"\n  ) %>%\n  e_tooltip(\n    trigger = \"axis\",\n    backgroundColor = \"rgba(20, 20, 20, 0.5)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    textStyle = list(\n      color = \"#ffffff\"\n    )\n  )\n\n\n\n\n\n\n81.89% of total grievances were categorized as electrical, solid waste or garbage related, or road maintenance. The next three largest categories make up an additional 11.09%, meaning the remaining categories have less than 1,350 occurrences combined.\nNote that this is not due to a lack of categories; in fact, there are a total of 20 categories in the data.\n\n\nShow code\nlength(unique(grievances$category))\n\n\n[1] 20\n\n\nLarge imbalances like this are important to consider when building machine learning models. Algorithms are specifically programmed to achieve the highest accuracy regardless of original purposes, and they tend to overestimate larger categories. If a model discovers it can restrict itself to three options while still recording 80%+ accuracy, it will almost always do so.\nThis means, though, machine learning models will tend to ignore minor categories, because - using our current data as an example - predicting a category outside the top three has an inherent 81.89% fail rate, so this will need to be addressed when creating the models.\nThere is a question as to whether certain smaller categories should exist at all, though.\n\n\nShow code\ngrievances %>%\n  group_by(category) %>%\n  summarise(total = n()) %>%\n  filter(total < 100)\n\n\n# A tibble: 5 x 2\n  category                   total\n  <chr>                      <int>\n1 Education                     20\n2 Estate                        75\n3 Markets                       38\n4 Optical Fiber Cables (OFC)    62\n5 Welfare Schemes               28\n\n\nFive categories have less than 100 complaints total, including two which have less than thirty. This is far too few complaints to reliably build a model with, especially considering we haven‚Äôt split the data yet.\n\n\nGrievances by Subcategory\n\n\nShow code\ngrievances %>%\n  group_by(subcategory) %>%\n  summarise(Total = n()) %>%\n  arrange(Total) %>%\n  filter(Total > 1000) %>%\n  e_charts(subcategory) %>%\n  e_bar(Total) %>%\n  e_legend(show = FALSE) %>%\n  e_title(\n    text = \"Most common grievances by subcategory\", \n    subtext = \"Only categories above 1,000 visible\"\n    ) %>%\n  e_color(\"#8c5ac8\", background = \"rgb(0,0,0,0)\") %>%\n  e_flip_coords() %>%\n  e_tooltip(\n    trigger = \"axis\",\n    backgroundColor = \"rgba(20, 20, 20, 0.5)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    textStyle = list(\n      color = \"#ffffff\"\n    )\n  )\n\n\n\n\n\n\nWhile the model being built is only to predict the main categories, by viewing subcategories, we see over 35% of total complaints were related specifically to street lights not working, comprising almost all of the electrical category. Solid waste related problems is divided into two subcategories; garbage vehicle not arriving, and ‚ÄúGarbage dump‚Äù (whatever that means). Meanwhile, road maintenance has been divided into three subcategories, those being potholes, road side drains, and debris removal.\n\n\nMost Common Complaint Words\nWe first have to get the 50 most common words‚Ä¶\n\n\nShow code\ntidy <- grievances %>%\n  unnest_tokens(word, description) %>%\n  anti_join(get_stopwords()) %>%\n  count(word) %>%\n  arrange(desc(n)) %>%\n  slice(1:50)\n\n\nThen chart them using wordcloud2.\n\n\nShow code\nwordcloud2(\n  tidy,\n  color = rep_len(c(\"#8c5ac8\", \"#0b0d21\", \"#0a32d2\"), nrow(tidy)),\n  backgroundColor = \"#fcfcfc\"\n)\n\n\n\n\n\n\nSeeing as over a third of the data was subcategorized as street lights not working, we see that they are the most common words across all the complaints.\nMissing from that is the word not, but this is because we removed all stop words from the data. Put simply, stop words are words that don‚Äôt add anything to the process of categorizing text. They usually include words like he, she, there, they, I, and so on, which is why they don‚Äôt appear in the chart.\nThe stop words included in the tidytext package (and the ones used in the function above) are meant to apply universally, but if you look closely at the word cloud, there are several words that almost certainly don‚Äôt apply to our problem. These include terms like please, kindly, last request, sir, and individual numbers like 1, 2, and 3. While there might exist some incidental correlation between some of these words and their respective categories (perhaps citizens filing animal control complaints are nicer on average, so the term please could be used to identify those complaints more accurately, for example), it‚Äôs likely this will just throw off our model‚Äôs accuracy in the long run.\n\nFrom the EDA, the primary factors that should be considered when building the models are:\n\nAccount for imbalances in number of occurrences per category.\nReduce the number of categories by combining them into existing ones.\nAdd additional stop words."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#mle-models",
    "href": "blog/2023/bengaluru-text-mining/index.html#mle-models",
    "title": "Bengaluru Text Mining",
    "section": "MLE Models",
    "text": "MLE Models\n\nPreparing the Data\nWe‚Äôll opt to use Python for creating the MLE models, as Python libraries are generally more efficient and developed than their R counterparts. Because creating models is computer intensive, the code here has been evaluated locally and presented here for demonstration.\nTo start, we‚Äôll import the necessary libraries and load the data using pandas. The models here will be built using sklearn.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns  \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n\ndf = pd.read_csv(\"bengaluru-grievances.csv\")\n\n\nOne of the packages imported here is TfidfVectorizer, which will be the algorithm used to create the models. I‚Äôll explain why I specifically chose this package later on.\nHere, I quickly apply the earlier data wrangling techniques by removing complaints less than 12 characters, this time using Python syntax.\n\n\nShow code\n# Reduce Character Limit  \ncharacter_limit = (df[\"description\"].str.len() > 12)\ndf = df.loc[character_limit]\n\n\nA model based on over 100,000 observations is extremely hardware intensive, and it causes my laptop to overheat a lot. For practical purposes, we‚Äôll cut down on our data by choosing 15,000 rows at random.\n\n\nShow code\ndf = df.sample(15000, random_state = 1).copy()\n\n\nIf we wanted to get another random 15,000 rows, we could change the random_state = 1 argument to any other number, like 42, 671, or 7.\nWith no other changes to be made, we can begin creating the models.\n\n\nText Preprocessing\nThere are several methods to building models, but the simplest method is to create a new column in our data - we‚Äôll call it category_id - that is a factor variable of all existing categories in our data. This essentially amounts to assigning each category a number (Electrical = 0, Road Engineering = 1, etc), which is necessary for getting our model to run properly, as sklearn will not understand strings as factors.\n\n\nShow code\ndf['category_id'] = df[\"category\"].factorize()[0]\ncategory_id_df = df[[\"category\", \"category_id\"]].drop_duplicates()\n\n\nNext, the description column (which stores the text for grievances) needs to be converted to vectors using a chosen algorithm. The algorithm chosen here is Term Frequency - Inverse Document Frequency (TF-IDF), which is the product of \\(TF\\) and \\(IDF\\) scores. This is the TfidfVectorizer function that we imported earlier.\nIt‚Äôs useful to present these terms mathematically.\n\nTerm Frequency: \\[ TF = \\frac{Number \\hspace{0.15cm} of \\hspace{0.15cm} times \\hspace{0.15cm} a \\hspace{0.15cm} term \\hspace{0.15cm} appears \\hspace{0.15cm} in \\hspace{0.15cm} the \\hspace{0.15cm} description}{Total \\hspace{0.15cm} number \\hspace{0.15cm} of \\hspace{0.15cm} words \\hspace{0.15cm} in \\hspace{0.15cm} the \\hspace{0.15cm} description} \\]\nInverse Document Frequency: \\[ IDF = log(\\frac{Number \\hspace{0.15cm} of \\hspace{0.15cm} rows \\hspace{0.15cm} in \\hspace{0.15cm} a \\hspace{0.15cm} data}{Number \\hspace{0.15cm} of \\hspace{0.15cm} rows \\hspace{0.15cm} a \\hspace{0.15cm} term \\hspace{0.15cm} appears \\hspace{0.15cm} in \\hspace{0.15cm} a \\hspace{0.15cm} data}) \\]\nTF-IDF: \\[ TF-IDF = TF * IDF \\]\n\nThe reason for choosing this algorithm was for the \\(IDF\\) component, which downsamples words that appear frequently across all complaints, while adding extra weight to terms that appear less often.\nAnalyzing it mathematically: as the denominator of the \\(IDF\\) variable increases, the closer it rapidly (more precisely, exponentially) approaches zero. Let \\(N\\) represent the total number of rows in the data, and let \\(t\\) represent a chosen term. If a certain word were to appear in every single complaint, then we would have \\(IDF(N, t) = log(\\frac{N}{t}) = log(\\frac{N}{N}) = log(1) = 0\\), which would mean that when calculating \\(TF-IDF\\), that specific word would have absolutely no weight attached to it when classifying complaints.\nNow; why do this? As discussed previously during the EDA section, almost 82% of all complaints fell into exactly three categories. Classification models will tend to stick to only a few categories, struggling to identify minor categories. While this may record higher accuracy scores on average, doing so means minor categories will rarely be classified at all, and in some instances, could lower overall accuracy if skew is significant enough. By ranking terms on an exponentially decreasing scale, we hope to reduce this issue.\nWe first setup our TfidfVectorizer and assign it to a variable, tfidf.\n\n\nShow code\ntfidf = TfidfVectorizer(\n  sublinear_tf = True, # Set term frequency to logarithmic scale\n  min_df = 5, # Remove terms that appear less than 'min_df' times\n  ngram_range = (1, 2), # Unigrams and bigrams are considered \n  stop_words = 'english' # Use common English stop words\n)\n\n\nFrom here, we can begin building our models, but before doing so, let‚Äôs see what the most common terms were for each category.\nTo do so, we use tfidf.get_feature_names_out() on each category and assign that to a variable that we‚Äôll call feature_names. This contains all of the most common words associated with each category, which we then split into two separate lists for unigrams and bigrams (fancy words for ‚Äúone word‚Äù and ‚Äútwo words‚Äù). From there, we print to console the \\(N = 3\\) most common terms from each list. We wrap all this in a for loop, automatically progressing through each category.\n\n\nShow code\n# Defaults \nfeatures = tfidf.fit_transform(df.description).toarray()\nlabels = df.category_id\nN = 3\n\n# For Loop\nfor category, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names_out())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"\\n==> %s:\" %(Product)) # Space for formatting\n  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))\n\n\nAs this part was performed offline, here is the output in screenshots.\n\n\n\nThe results are largely what we would expect, though there are some things to note.\nSome common phrases that appear for certain categories seemingly have nothing to do with them, such as the terms ‚Äú77‚Äù, ‚Äúkindly‚Äù, and ‚Äúplz look‚Äù, which is one of the most common bigrams for both ‚ÄúEducation‚Äù and ‚ÄúWelfare Schemes.‚Äù Remember, these were the categories that had less than 100 observations total. When we split the data to grab 15,000 random rows, these categories were split even further, which is probably why these nonsense phrases appear.\n\n\nBuilding the Models\nTo begin, we first split the data into a 75:25 training and test split. The model will ‚Äúlearn‚Äù how to classify grievances based on the training data, and then it will ‚Äútest‚Äù its accuracy on the remaining 25%.\n\n\nShow code\n# We define them here as independent variables \nX = df[\"description\"]\ny = df[\"category\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n\nThere are several different models to choose from, but it‚Äôs hard to know which will perform best before actually building them. That‚Äôs why we‚Äôll test several models simultaneously by storing them in a list and looping through each model.\n\n\nShow code\nmodels = [\n    RandomForestClassifier(n_estimators = 100, max_depth = 5, random_state = 0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state = 0)    \n]\n\n\nHere, we stored in a list the following:\n\nRandom Forest Model\nLinear Support Vector Classifier Model\nMultinomial Naive Bayes Model\nLogistic Regression Model\n\nAfter, we apply each model to the training data and record the results. The accuracy of each model is inherently random, as model performance is somewhat due to chance, so we‚Äôll use a five-fold cross-validation and take the mean average of each iteration to get a more balanced result. We store the results in a pandas dataframe for analysis.\n\n\nShow code\n# Copy and pasted from before \nfeatures = tfidf.fit_transform(df.description).toarray()\nlabels = df.category_id\n\nCV = 5 # Number of cross-validations\ncv_df = pd.DataFrame(index = range(CV * len(models))) # CV dataframe\nentries = [] # Array for storing model results \n\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\n    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n\n\n\n\nResults\nBecause we used a five-fold cross-validation, we have a total of 20 accuracy results - five for each model. We grab the mean accuracy and standard deviation for each model, storing them into a list.\n\n\nShow code\nmean_accuracy = cv_df.groupby('model_name').accuracy.mean()\nstd_accuracy = cv_df.groupby('model_name').accuracy.std()\n\naccuracy = pd.concat([mean_accuracy, std_accuracy], axis= 1, ignore_index=True)\naccuracy.columns = ['Mean Accuracy', 'Standard Deviation']\n\naccuracy\n\n\n\n\n\nModel\nMean Accuracy\nStandard Deviation\n\n\n\n\nLinear SVC\n88.773%\n0.368%\n\n\nLogistic Regression\n87.767%\n0.433%\n\n\nMultinomial NB\n85.720%\n0.117%\n\n\nRandom Forest\n66.213%\n1.411%\n\n\n\nThe top three models all performed similarly as well, all falling within 3.1% percentage points. The Linear Support Vector Classifier performed the best among the three, while the Random Forest performed atrociously. Standard deviation among the top three remained fairly low, but especially for multinomial naive bayes."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#improvements",
    "href": "blog/2023/bengaluru-text-mining/index.html#improvements",
    "title": "Bengaluru Text Mining",
    "section": "Improvements",
    "text": "Improvements\nWe will focus model improvement on the Linear SVC because it performed the best.\nAs a reminder, these were the three main considerations before going in.\n\nAccount for imbalances in number of occurrences per category.\nConsider reducing number of categories.\nConsider adding additional stopwords.\n\nTo get a better idea of how our model performed, we will plot a confusion matrix, which displays the total number of attempts our classification model made along with how many were accurately categorized.\n\n\nShow code\n# Recreating LinearSVC Model \nX_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, \n                                                               labels, \n                                                               df.index, test_size=0.25, \n                                                               random_state=1)\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Confusion Matrix Plot\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize = (8, 8))\nsns.heatmap(conf_mat, annot = True, cmap = \"Greens\", fmt = 'd',\n            xticklabels = category_id_df.category.values, \n            yticklabels = category_id_df.category.values)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix for LinearSVC \\n\", size = 20)\n\n\n\n\n\nOn the diagonal are the number of rows that the model correctly predicted for each category. The horizontals and verticals represent the number of incorrect guesses, with the vertical representing incorrect guesses for that specific category. For example, the model correctly classified 1,484 complaints as ‚ÄúElectrical,‚Äù incorrectly classified 2 as ‚ÄúElectrical‚Äù when they should of been classified as ‚ÄúCOVID-19,‚Äù and classified 10 as ‚ÄúRoad Maintenance‚Äù when they should of been classified as ‚ÄúElectrical.‚Äù\n\nModifications\nLooking at the chart, the top three categories dominate the total number of occurrences, comprising 2,969 rows out of 3,750 in our test data. Most of the incorrect predictions appear in the vertical of each of these three columns, meaning the model was incorrectly classifying complaints as them often. Even though we chose an algorithm to specifically downsample those categories, our model still has a tendency to over-predict them.\nA few categories have 12 or fewer observations: those being Markets, Estate, OFC, Welfare Schemes, Advertisement, Education, Town Planning, Lakes, and Parks and Playgrounds. Converting these will likely improve accuracy considering how poorly our model did at predicting them, but there isn‚Äôt clear category to merge them with. Many of these categories seem to have been falsely labeled as ‚ÄúRoad Maintenance.‚Äù While converting these columns over to this might lead to higher accuracy, it doesn‚Äôt really make any sense in this case, and likely would hurt performance in the future.\nWe could reassign these variables to ‚ÄúOthers,‚Äù but that category performed abysmally, only correctly predicting 3 out of 43 complaints. On one hand, moving them there probably won‚Äôt hurt, but it likely won‚Äôt improve ‚ÄúOthers‚Äù result either.\nLakes and Advertisements, which the model predicted quite a few correctly, will be left untouched for now. For the remaining categories under 12 test observations, they will be merged in with Others.\n\n\nShow code\n# Read in Data | Copy and Paste from Above\ndf = pd.read_csv(\"bengaluru-grievances.csv\")\npd.DataFrame(df.category.unique()).values \ncharacter_limit = (df[\"description\"].str.len() > 12)\ndf = df.loc[character_limit]\ndf = df.sample(15000, random_state = 2).copy() # Select New Rows \n\n# Convert Columns \ndf[\"category\"] = df[\"category\"].replace({'Markets': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Estate': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Welfare Schemes': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Education': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Town Planning': 'Others'})\n\n\n‚ÄúOptical Fiber Cables‚Äù and ‚ÄúStorm Water Drains,‚Äù though, are directly related to ‚ÄúRoad Maintenance,‚Äù and if we look at the chart, that‚Äôs what the model ended up incorrectly guessing the most of. For these categories, it makes more sense to convert them over to ‚ÄúRoad Maintenance‚Äù as opposed to ‚ÄúOthers.‚Äù\n\n\nShow code\ndf[\"category\"] = df[\"category\"].replace({'Optical Fiber Cables (OFC)': 'Road Maintenance(Engg)'})\ndf[\"category\"] = df[\"category\"].replace({'Storm  Water Drain(SWD)': 'Road Maintenance(Engg)'})\n\n\nWhile we‚Äôve converted the total number of categories down from 20 to 13, we‚Äôve only changed a total of 45 test rows. Even if the improved model were able to correctly predict all these observations now, we would only see an improvement of 1.2%. It‚Äôs certainly not insignificant, but hardly substantial. Improving our stop word list, on the other hand, will hypothetically improve the accuracy of the model overall.\nRecall earlier when we found the most common unigrams and bigrams for each category. Several terms that appeared most often had little or nothing to do with their respective grievances, and should be able to be removed while maintaining or improving original accuracy.\nThe original stop word list comes from another function in sklearn, and already contains over 300 words. We want to keep those words while adding to it, so we will union them together in a new list and use it in TfidfVectorizer.\n\n\nShow code\n# Import Function \nfrom sklearn.feature_extraction import text\n\n# Add Stop Words \nstop_words = text.ENGLISH_STOP_WORDS.union([\"please\", \"plz\", \"look\", \"help\", \"causing\", \"coming\", \"kindly\", \"refused\", \"senior\", \"help\", \"one\", \"two\", \"three\", \"also\", \"77\", \"1\", \"2\", \"3\", \"since\"])\n\n# TfidfVectorizer \ntfidf = TfidfVectorizer(\n  sublinear_tf = True, # Set term frequency to logarithmic scale\n  min_df = 5, # Remove terms that appear less than 'min_df' times\n  ngram_range = (1, 2), # Keep unigrams and bigrams\n  stop_words = stop_words # Use custom stop words \n)\n\n\n\n\nRedo Text Preprocessing\nWe have to redo the text preprocessing from earlier, so this is all copy-and-paste from before.\n\n\nShow code\n# Copy and Paste\ndf['category_id'] = df[\"category\"].factorize()[0]\ncategory_id_df = df[[\"category\", \"category_id\"]].drop_duplicates()\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[[\"category_id\", \"category\"]].values)\n\nfeatures = tfidf.fit_transform(df.description).toarray()\nlabels = df.category_id\nX = df[\"description\"]\ny = df[\"category\"]\n\nX_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(\n  features, \n  labels, \n  df.index, test_size = 0.25, \n  random_state = 2)\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n\n\n\nNew Results\nWe don‚Äôt need the complicated for loop from before because we only have one model this time. Therefore, we simply use cross_val_score as we did before and print the results to console.\n\n\nShow code\naccuracy_svc = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n\ncv_mean_accuracy_svc = accuracy_svc.mean()\ncv_mean_std_svc = accuracy_svc.std()\n\nprint(cv_mean_accuracy_svc * 100)\nprint(cv_mean_std_svc * 100)\n\n\n\n\n\n\nResults\n\n\n\nOur new Linear SVC model was able to achieve 91.987% accuracy with an average standard deviation of 0.282% across five iterations. That‚Äôs an improvement of 3.214% while also reducing variance within model performance by 0.086%.\nAnother way to look at these refinements; out of a possible 3,750 complaints, our model was able to correctly classify an additional 120 complaints, going from 3,328 correct predictions to 3,449."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#conclusions",
    "href": "blog/2023/bengaluru-text-mining/index.html#conclusions",
    "title": "Bengaluru Text Mining",
    "section": "Conclusions",
    "text": "Conclusions\nOnce again, the top three categories performed similarly as well as before. Road maintenance was able to correctly predict an additional 63 complaints on this iteration. These three categories also continue to make up most of the incorrect predictions.\nThe ‚ÄúOthers‚Äù categories once again performed dreadfully, only recording an additional two correct predictions despite even more chances. Given it‚Äôs a category meant to be all-emcompassing, it probably makes sense to manually reclassify those comaplaints into new or existing categories.\nMinor categories saw little or no improvement. The ‚ÄúHealth Dept‚Äù was the only category that performed worse, dropping from 69.7% to 54.6% accuracy. The model incorrectly chose ‚ÄúSolid Waste‚Äù and ‚ÄúRoad Maintenance‚Äù much more often than the previous model did, though it‚Äôs unclear as to why this is.\nIncreasing the stop word list seems to have improved accuracy overall. A more thorough list and additional adjustments might boost performance slightly more."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#results",
    "href": "blog/2023/bengaluru-text-mining/index.html#results",
    "title": "Bengaluru Text Mining",
    "section": "Results",
    "text": "Results\n\nOver 80% of complaints filed were related to street lights not working, road maintenance, or garbage collection issues.\nOf the four classification models, the linear support vector classifier performed the best, recording 88.773% accuracy. The top three models all performed similarly as well, though, all falling within three percentage points.\nImprovements were made by increasing the number of stop words, as well as combining smaller categories into larger ones. Using LinearSVC, these changes led to a 3.214% increase, ultimately recording an accuracy of 91.987%.\nFurther improvements can be made by adding to the stop word list, changing the contents of the ‚ÄúOthers‚Äù category, and adjusting the downsampling of the model."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#source",
    "href": "blog/2023/bengaluru-text-mining/index.html#source",
    "title": "Bengaluru Text Mining",
    "section": "Source",
    "text": "Source\nYou can check out the Python source code for the MLE model here. Download the full dataset here."
  },
  {
    "objectID": "blog/2023/covid-19-stimulus-database-part-1/index.html#quick-rant",
    "href": "blog/2023/covid-19-stimulus-database-part-1/index.html#quick-rant",
    "title": "Building a COVID-19 Economic Relief Database: Part 1 (usaspending.gov)",
    "section": "Quick Rant",
    "text": "Quick Rant\nWe live in 2023, in a country of over 330+ million people in it. We all collectively pay taxes, which our representatives then utilize several trillion dollars to give out insured loans, contracts, and award grants. Perhaps I only speak for myself, but such information should be easily available for every citizen to view in simple terms.\nAn official ‚Äúgovernment‚Äù database didn‚Äôt exist until 2007. 2007. It took 2007 to create a database to track subsidies, one of the basic blocks of our economic system!\nusaspending.gov was created from this bill, but the original database wasn‚Äôt created by the government. It was created by OMB Watch (now named the Project on Government Oversight), who was paid $600,000 to develop the existing database that OMB Watch had already created. To be fair, the article does say that they worked together with the OMB (as in the government agency, Office of Management and Budget), but shouldn‚Äôt something this important deserve more funding?\nAt the very least, OMB Watch is a non-profit. Several government databases are the result of contractual work, often with few bidders. One of the most difficult things when maintaining a database is consistency, making sure columns are filled in correctly, and that APIs / download links are easy and accessible. If you‚Äôre a private company, you are disincentivized to care about these issues because they‚Äôre irrelevant once the contract is over.\nusaspending.gov isn‚Äôt directly tracking data, but they‚Äôre compiling data from various different agencies. A Senate Permanent Subcommittee on Investigations found that from April-June 2017, 55 percent of the data submitted was inaccurate, incomplete, or both. A recent report by the Government Accountability Office found that agencies are often still slow with reporting data. And of course, data from usaspending.gov still requires a lot of work to use.\nThis database should be 100x better than what is currently is, and it wouldn‚Äôt be that difficult to improve it. The ability to view the parent company has still yet to be implemented; what the hell is that? It‚Äôs all very annoying."
  },
  {
    "objectID": "blog/2023/covid-19-stimulus-database-part-1/index.html#other-data",
    "href": "blog/2023/covid-19-stimulus-database-part-1/index.html#other-data",
    "title": "Building a COVID-19 Economic Relief Database: Part 1 (usaspending.gov)",
    "section": "Other Data",
    "text": "Other Data\nAlright, with that out of my system‚Ä¶\nThis database isn‚Äôt meant to be solely about subsidies, but to connect them with other relevant economic information that people might want to analyze. In addition to this information, in the future, I‚Äôd like to build databases on;\n\nStock Buybacks: For those unaware, stock buybacks is the practice of a company purchasing shares of their own stock. This reduces the number of shares on the market, which in turn, increases the share price for those already holding it. Prior to 1982, stock buybacks were subjected to harsh scrutiny from the SEC and was generally classified as market manipulation; however, Rule 10b-18 reversed this and made it much easier for large share repurchasing, which companies now devote significant capital to. The practice is controversial; when airline workers were on strike in September 2022, one of their demands was to extend a COVID-19 provision that prohibited airline companies from engaging in stock buybacks, which ultimately did not come to fruition. This information is clearly relevant, and would offer a more comprehensive overview of how companies affect the greater economy.\nWages and Employee Numbers: A sizable portion of aid went towards various payroll support programs to help businesses pay their workers. Different agencies have different formulas for figuring out how much to give each recipient, which is usually calculated based on employment size, average salary, etc., but this data isn‚Äôt publicly available, so there‚Äôs no real way to determine if funds are distributed equitably or not. Such a database would also allow for tracking company outcomes during and after the pandemic. An October 2020 congressional report found that at least 15 companies had fired or furloughed over 16,500 employees after receiving payroll support.\nLegal Violations: Many of the companies who received aid during the pandemic have also been sued for billions of dollars for a variety of infractions, including civil court cases, OSHA incidents, DOJ investigations, etc. Many loans came from some of the largest banks in America, who are also among the most sued companies in the country. For reference, the four largest domestic banks have been sued at least 882 times for over $167 billion since 1995. Rewarding companies who continually break the law is - in my personal opinion - bad, and a database that tracks those violations would allow citizens to see if their tax dollars are going towards responsible corporations.\nOther Subsidies: Companies who are already heavily subsidized are arguably less deserving of government aid. Since 2019, you can download a PostgreSQL database from usaspending.gov that contains all subsidy information since FY 2001. However, similar to this COVID-19 dataset, it‚Äôs in a format that‚Äôs essentially unusable, and requires a lot of work to make use of it.\n\nGathering the data for these projects will require a ton of additional work, and won‚Äôt be completed anytime in the near future. However, I‚Äôm laying this out now to consider what schematics this database will have, considering they will all directly relate to each other.\n\nWith that out of the way, let‚Äôs get the data."
  },
  {
    "objectID": "notes/echarts4r/index.html#a-tooltip-theming",
    "href": "notes/echarts4r/index.html#a-tooltip-theming",
    "title": "echarts4r Cookbook",
    "section": "4.A Tooltip Theming",
    "text": "4.A Tooltip Theming"
  },
  {
    "objectID": "notes/echarts4r/index.html#a.-column",
    "href": "notes/echarts4r/index.html#a.-column",
    "title": "echarts4r Cookbook",
    "section": "2.A. Column",
    "text": "2.A. Column\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_tooltip(trigger = \" axis\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#heatmap",
    "href": "notes/echarts4r/index.html#heatmap",
    "title": "echarts4r Cookbook",
    "section": "2. . Heatmap",
    "text": "2. . Heatmap"
  },
  {
    "objectID": "notes/echarts4r/index.html#candlestick",
    "href": "notes/echarts4r/index.html#candlestick",
    "title": "echarts4r Cookbook",
    "section": "2. . Candlestick",
    "text": "2. . Candlestick"
  },
  {
    "objectID": "notes/echarts4r/index.html#treemap",
    "href": "notes/echarts4r/index.html#treemap",
    "title": "echarts4r Cookbook",
    "section": "2. . Treemap",
    "text": "2. . Treemap"
  },
  {
    "objectID": "notes/echarts4r/index.html#calendars",
    "href": "notes/echarts4r/index.html#calendars",
    "title": "echarts4r Cookbook",
    "section": "2. . Calendars",
    "text": "2. . Calendars\n\ndates <- seq.Date(as.Date(\"2017-01-01\"), as.Date(\"2018-12-31\"), by = \"day\")\nvalues <- rnorm(length(dates), 20, 6)\n\nyear <- data.frame(date = dates, values = values)\n\nyear |> \n  e_charts(date) |> \n  e_calendar(range = \"2018\") |> \n  e_heatmap(values, coord_system = \"calendar\") |> \n  e_visual_map(max = 30) |> \n  e_title(\"Calendar\", \"Heatmap\") |>\n  e_tooltip()\n\n\n\n\n\nWhen charting by multiple years, call e_calendar again and group by year.\n\nyear |> \n  dplyr::mutate(year = format(date, \"%Y\")) |>\n  group_by(year) |> \n  e_charts(date) |> \n  e_calendar(range = \"2017\",top=\"40\") |> \n  e_calendar(range = \"2018\",top=\"260\") |> \n  e_heatmap(values, coord_system = \"calendar\") |> \n  e_visual_map(max = 30) |> \n  e_title(\"Calendar\", \"Heatmap\")|>\n  e_tooltip(\"item\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#j.-pie-donut-chart",
    "href": "notes/echarts4r/index.html#j.-pie-donut-chart",
    "title": "echarts4r Cookbook",
    "section": "2.J. Pie / Donut Chart",
    "text": "2.J. Pie / Donut Chart\nNo."
  },
  {
    "objectID": "notes/echarts4r/index.html#a.-set-common-theme",
    "href": "notes/echarts4r/index.html#a.-set-common-theme",
    "title": "echarts4r Cookbook",
    "section": "3.A. Set Common Theme",
    "text": "3.A. Set Common Theme\nSet theme for all charts on the page and set font.\n\ne_common(\n  theme = \"my-theme\",\n  font_family = \"my-font\"\n)"
  },
  {
    "objectID": "notes/echarts4r/index.html#b.-tooltip-theme",
    "href": "notes/echarts4r/index.html#b.-tooltip-theme",
    "title": "echarts4r Cookbook",
    "section": "3.B. Tooltip Theme",
    "text": "3.B. Tooltip Theme\nI always customize the tooltip to get rid of the ugly border and the white background.\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_legend(show = FALSE) |>\n  e_axis_labels(\n    x = \"Island\",\n    y = \"Averge Mass\"\n  ) |>\n  e_title(\n    text = \"Some meaningful title.\"\n  ) |>\n  e_tooltip(\n    trigger = \" axis\",\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    textStyle = list(\n      color = \"#fcfcff\"\n    )\n  )"
  },
  {
    "objectID": "notes/echarts4r/index.html#c.-remove-legend",
    "href": "notes/echarts4r/index.html#c.-remove-legend",
    "title": "echarts4r Cookbook",
    "section": "3.C. Remove Legend",
    "text": "3.C. Remove Legend\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_legend(show = FALSE) |>\n  e_axis_labels(\n    x = \"Island\",\n    y = \"Averge Mass\"\n  ) |>\n  e_legend(show = FALSE) |> # THIS LINE OF CODE # \n  e_title(\n    text = \"Some meaningful title.\"\n  ) |>\n  e_tooltip(trigger = \" axis\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#c.-default-tooltip-formatter",
    "href": "notes/echarts4r/index.html#c.-default-tooltip-formatter",
    "title": "echarts4r Cookbook",
    "section": "3.C. Default Tooltip Formatter",
    "text": "3.C. Default Tooltip Formatter\nOne of the arguments in e_tooltip is formatter. There are three different types of default formats that make it easy to display tooltips nicer:\n\nmy_echart |>\n  e_tooltip(\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    # OR #\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    # OR #\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n  )"
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html#gathering-the-data",
    "href": "blog/2023/pcaob-part-1-inspections/index.html#gathering-the-data",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Gathering the Data",
    "text": "Gathering the Data\nBefore diving into things, I‚Äôll make a quick note on how the data was collected.\nIf you go to pcaobus.org, you‚Äôll find two separate databases: inspections and enforcement actions. Both databases provide reports in PDF format, so I manually gathered the relevant details from each file, and stored them in two separate spreadsheets.\nThe PCAOB has performed over 3,500 inspection reports since its creation in 2002, with about 2,800 being on domestic firms. I collected data on the firm, year, number of audits inspected, and number of audits that were deficient from each of the report. This takes a long time, which is why I only have data on the largest auditing firms. Only 14 firms receive annual inspections from the PCAOB, so the data outside of them is sparse anyways. I also recorded industry and revenue range for the four largest firms, who were the focus of the article, and will be the focus here.\nThose Big Four - those being Deloitte, PricewaterhouseCoopers, KPMG, and Ernst & Young - audit almost half of all publicly traded companies, including almost every company on the S&P 500. They have a disproportionate influence on overall audit quality, which is also why the POGO article spends so much time focusing on them. This post will follow a similar approach."
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html#programming",
    "href": "blog/2023/pcaob-part-1-inspections/index.html#programming",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Programming",
    "text": "Programming\nCode for my programmers; scroll down to table if you don‚Äôt care.\nSetup: Here, we import the neccessary libraries and the data. The tables will be created using gt, while the interactive charts are created using ehcarts4r.\n\n\nShow code\n# Load Libraries ---- \nlibrary(tidyverse)\nlibrary(gt)\nlibrary(sysfonts)\nlibrary(ggtext)\nlibrary(gtExtras)\nlibrary(here)\nlibrary(extrafont)\nlibrary(echarts4r)\n\n# Set Directory ----\nhere::set_here()\n\n# Import Data ----\ninspections <- read_csv(\"_data/inspections.csv\")\ninspections_category <- read_csv(\"_data/inspections_category.csv\")\ninspections_revenue <- read_csv(\"_data/inspections_float.csv\")\n\n\nData Wrangling: In order to get the table, we need to create two dataframes containing both domestic and international summations. Then, we can join those dataframes together and calculate totals. I also manually added the logo path and 2022 revenue amounts using case_when.\n\n\nShow code\n# Extracting year from date \ninspections$year <- as.Date(as.character(inspections$year), format = \"%Y\")\ninspections$year <- substring(inspections$year, 1, 4)\n\n# Calculating rates, totals, and select column order for domestic and international \noverview_domestic <- inspections |>\n  filter(!is.na(audits_inspected)) |>\n  filter(!is.na(audits_failed)) |>\n  filter(country == \"United States\") |>\n  group_by(firm) |>\n  summarise(\n    domestic_audits_inspected = sum(audits_inspected, na.rm = TRUE),\n    domestic_audits_failed = sum(audits_failed, na.rm = TRUE),\n    domestic_fail_rate = domestic_audits_failed / domestic_audits_inspected,\n  )\n\noverview_international <- inspections |>\n  filter(!is.na(audits_inspected)) |>\n  filter(!is.na(audits_failed)) |>\n  filter(country != \"United States\") |>\n  group_by(firm) |>\n  summarise(\n    international_audits_inspected = sum(audits_inspected, na.rm = TRUE),\n    international_audits_failed = sum(audits_failed, na.rm = TRUE),\n    international_fail_rate = international_audits_failed / international_audits_inspected\n  )\n\n# Joining data together \noverview <- left_join(\n  x = overview_domestic,\n  y = overview_international,\n  by = \"firm\"\n)  |>\n  drop_na()\n\n# Calculating totals and adding revenue/logo path\noverview <- overview |>\n  mutate(\n    total_audits_inspected = domestic_audits_inspected + international_audits_inspected,\n    total_audits_failed = domestic_audits_failed + international_audits_failed,\n    total_fail_rate = total_audits_failed / total_audits_inspected,\n    revenue = case_when(\n      firm == \"Deloitte\" ~ 59300000000,\n      firm == \"PricewaterhouseCoopers\" ~ 50300000000, \n      firm == \"Ernst & Young\" ~ 45400000000,\n      firm == \"KPMG\" ~ 34600000000,\n      firm == \"BDO\" ~ 12800000000,\n      firm == \"RSM\" ~ 8132000000,\n      firm == \"Grant Thornton\" ~ 7200000000,\n      firm == \"Crowe\" ~ 3800000000\n    ),\n    logo = case_when(\n      firm == \"Deloitte\" ~ \"_logos/deloitte.png\",\n      firm == \"PricewaterhouseCoopers\" ~ \"_logos/pwc.png\", \n      firm == \"Ernst & Young\" ~ \"_logos/ernst-and-young.png\",\n      firm == \"KPMG\" ~ \"_logos/kpmg.png\",\n      firm == \"BDO\" ~ \"_logos/bdo.png\",\n      firm == \"RSM\" ~ \"_logos/rsm.png\",\n      firm == \"Grant Thornton\" ~ \"_logos/grant-thornton.png\",\n      firm == \"Crowe\" ~ \"_logos/crowe.png\"\n    )\n  ) |>\n  drop_na() |>\n  select(logo, firm, revenue, domestic_audits_inspected, domestic_audits_failed, domestic_fail_rate, international_audits_inspected, international_audits_failed, international_fail_rate, total_audits_inspected, total_audits_failed, total_fail_rate)\n\n\nTable: Finally, making the table with gt.\n\n\nShow code\noverview |>\n  arrange(desc(revenue)) |>\n  gt() |>\n  gt_img_rows(\n    columns = logo,\n    img_source = \"local\",\n    height = px(40)\n  ) |>\n  fmt_percent(\n    columns = c(domestic_fail_rate, international_fail_rate, total_fail_rate),\n    decimals = 1\n  ) |>\n  fmt_currency(\n    columns = revenue,\n    currency = \"USD\",\n    suffixing = TRUE\n  ) |>\n  gt_color_rows(\n    columns = c(domestic_fail_rate, international_fail_rate, total_fail_rate),\n    palette = \"ggsci::blue_material\",\n    domain = c(0.18, 0.54)\n  ) |>\n  gt_theme_nytimes() |>\n  cols_align(\n    align = \"center\",\n    columns = c(revenue, domestic_audits_inspected, domestic_audits_failed, domestic_fail_rate, international_audits_inspected, international_audits_failed, international_fail_rate, total_audits_inspected, total_audits_failed, total_fail_rate)\n  ) |>\n  tab_header(\n    title = md(\"PCAOB INSPECTION RESULTS\"),\n    subtitle = md(\"The Public Company Accounting Oversight Board is a quasi-governmental agency founded in 2002 by the Sarbanes-Oxley Act. They are responsible for setting guidelines, regulating, and investigating the audit industry. About 45% of the PCAOB's budget is dedicated to inspections, where the board investigates whether an audit was conducted properly or not. <br><br> When audit opinions lack credible evidence, or the audit was conducted improperly, the board will declare it **deficient**, meaning the audit lacked evidence to support its audit opinion. Below are the inspection results for the eight largest audit firms in the United States, broken down between domestic and international firms, from 2009 - 2021. <br>\")\n  ) |>\n  tab_source_note(md(\"**Data**: PCAOB\")) |>\n  tab_footnote(\n    \"Deloitte includes years 2007 & 2008.\",\n    locations = cells_body(columns = firm, rows = 1)\n  ) |>\n  tab_footnote(\n    \"PwC includes 2008.\",\n    locations = cells_body(columns = firm, rows = 2)\n  ) |>\n  tab_footnote(\n    \"RSM only includes years 2015 - 2021.\",\n    locations = cells_body(columns = firm, rows = 6)\n  ) |>\n  tab_footnote(\n    \"All data was collected from 2009 - 2021 unless stated otherwise below.\"\n  ) |>\n  tab_spanner(\n    label = \"Domestic\",\n    columns = c(domestic_audits_inspected, domestic_audits_failed, domestic_fail_rate)\n  ) |>\n  tab_spanner(\n    label = \"International\",\n    columns = c(international_audits_inspected, international_audits_failed, international_fail_rate)\n  ) |>\n  tab_spanner(\n    label = \"Totals\",\n    columns = c(total_audits_inspected, total_audits_failed, total_fail_rate)\n  ) |>\n  tab_style(\n    cell_text(\n      color = \"#212124\",\n      size = \"large\",\n      transform = \"uppercase\"\n    ),\n    locations = cells_column_spanners()\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"bottom\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_spanners()\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_spanners()\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_body(columns = c(domestic_fail_rate, international_fail_rate))\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_labels(columns = c(domestic_fail_rate, international_fail_rate))\n  ) |>\n  tab_style(\n    cell_text(\n      weight = \"bold\"\n    ),\n    locations = cells_body(columns = c(domestic_fail_rate, international_fail_rate, total_fail_rate))\n  ) |>\n  tab_options(\n    table.background.color = \"#fcfcff\",\n    column_labels.font.size = 9.5,\n    table.font.size = 18,\n    heading.title.font.size = 30\n  ) |>\n  cols_width(\n    logo ~ px(70),\n    firm ~ px(210),\n    revenue ~ px(105),\n    everything() ~ px(65)\n  ) |>\n  cols_label(\n    logo = \"\",\n    revenue = \"2022 REVENUE\",\n    domestic_audits_inspected = \"AUDITS INSP.\",\n    domestic_audits_failed = \"AUDITS DEF.\",\n    international_audits_inspected = \"AUDITS INSP.\",\n    international_audits_failed = \"AUDITS DEF.\",\n    total_audits_inspected = \"AUDITS INSP.\",\n    total_audits_failed = \"AUDITS DEF.\",\n    domestic_fail_rate = \"DEF. RATE\",\n    international_fail_rate = \"DEF. RATE\",\n    total_fail_rate = \"DEF. RATE\"\n  ) \n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      PCAOB INSPECTION RESULTS\n    \n    \n      The Public Company Accounting Oversight Board is a quasi-governmental agency founded in 2002 by the Sarbanes-Oxley Act. They are responsible for setting guidelines, regulating, and investigating the audit industry. About 45% of the PCAOB's budget is dedicated to inspections, where the board investigates whether an audit was conducted properly or not.  When audit opinions lack credible evidence, or the audit was conducted improperly, the board will declare it deficient, meaning the audit lacked evidence to support its audit opinion. Below are the inspection results for the eight largest audit firms in the United States, broken down between domestic and international firms, from 2009 - 2021. \n    \n  \n  \n    \n      \n      firm\n      2022 REVENUE\n      \n        Domestic\n      \n      \n        International\n      \n      \n        Totals\n      \n    \n    \n      AUDITS INSP.\n      AUDITS DEF.\n      DEF. RATE\n      AUDITS INSP.\n      AUDITS DEF.\n      DEF. RATE\n      AUDITS INSP.\n      AUDITS DEF.\n      DEF. RATE\n    \n  \n  \n    \nDeloitte1\n$59.30B\n835\n176\n21.1%\n317\n142\n44.8%\n1152\n318\n27.6%\n    \nPricewaterhouseCoopers2\n$50.30B\n824\n197\n23.9%\n406\n117\n28.8%\n1230\n314\n25.5%\n    \nErnst & Young\n$45.40B\n729\n204\n28.0%\n349\n107\n30.7%\n1078\n311\n28.8%\n    \nKPMG\n$34.60B\n691\n234\n33.9%\n380\n155\n40.8%\n1071\n389\n36.3%\n    \nBDO\n$12.80B\n325\n156\n48.0%\n61\n25\n41.0%\n386\n181\n46.9%\n    \nRSM3\n$8.13B\n109\n42\n38.5%\n6\n2\n33.3%\n115\n44\n38.3%\n    \nGrant Thornton\n$7.20B\n444\n138\n31.1%\n61\n20\n32.8%\n505\n158\n31.3%\n    \nCrowe\n$3.80B\n183\n60\n32.8%\n9\n2\n22.2%\n192\n62\n32.3%\n  \n  \n    \n      Data: PCAOB\n    \n  \n  \n    \n       All data was collected from 2009 - 2021 unless stated otherwise below.\n    \n    \n      1 Deloitte includes years 2007 & 2008.\n    \n    \n      2 PwC includes 2008.\n    \n    \n      3 RSM only includes years 2015 - 2021.\n    \n  \n\n\n\n\nLet me give a quick breakdown of what you‚Äôre looking at.\nAbove are the eight largest audit firms in the United States, who audit over half of all publicly traded companies, including every single company on the S&P 500. I‚Äôve also included their 2022 revenue amounts for comparison. As a reference, $59.3 billion would put Deloitte as the third largest privately owned company in the United States in terms of revenue, and top 50 overall.\nThe PCAOB will issue annual reports for the very largest firms, where they will inspect a select number of audits to see if they were conducted properly or not. They categorize deficiencies into two different types, though type B deficiencies are not recorded in any meaningful capacity. If the PCAOB finds an audit to be type A deficient, it means the audit was conducted so poorly that the results have no evidence to support their conclusions. This doesn‚Äôt mean that the company is engaged in fraud, or other accounting misconduct, but if they were, it likely would of gone undetected.\nIt should be noted that the PCAOB uses a risk-based selection process when choosing which audits to investigate. This means that deficiency rates aren‚Äôt representative of how often the average audit by a given firm will be deficient. Since 2016, the board began including a sample of randomly selected audits to include in their annual audit inspections, but the reports do not specify which audits were deficient or not.\nEach of these firms have affiliates all over the world, primarily in Canada, Mexico, the United Kingdom, and Bermuda. If a company/asset located in another country also appears on a US market, then is must audited to PCAOB standards.\nPOGO released another article talking about this, but even if we were to acknowledge that audits are selected based on risk, successfully predicting audits 33% to nearly 50% is insane. Even BDO‚Äôs most recent inspection report failed 16 out of 30 audits despite the fact 11 of them were selected randomly. Unfortunately, the reports don‚Äôt disclose the companies whose audits were found to be deficient, so it‚Äôs not possible to determine what tangible effect this has on the economy."
  },
  {
    "objectID": "notes/svg/index.html",
    "href": "notes/svg/index.html",
    "title": "SVG",
    "section": "",
    "text": "D3 For The Impatient by Philipp Janert."
  },
  {
    "objectID": "notes/svg/index.html#overview",
    "href": "notes/svg/index.html#overview",
    "title": "SVG",
    "section": "1. Overview",
    "text": "1. Overview\nSVG stands for Scalabe Vector Graphics, and is a vector image format for creating graphics. SVG uses XML-based text files that produces not flat images, but DOM trees.\nSVG is almost never used alone; it‚Äôs usually used in HTML in combination with D3 to make interactive charts."
  },
  {
    "objectID": "notes/svg/index.html#shapes",
    "href": "notes/svg/index.html#shapes",
    "title": "SVG",
    "section": "2. Shapes",
    "text": "2. Shapes\nThere are four primary shape types in SVG.\n\n\n\nTag\nAttributes\nDescription\n\n\n\n\n\nx, y\nCoordinates of Upper-Left Corner\n\n\n\nwidth, height\nWidth and Height\n\n\n\nrx, ry\nHorizontal and Vertical Corner Radius\n\n\n\ncx, cy\nCenter Coordinates\n\n\n\nr\nRadius\n\n\n\ncx, cy\nCenter Coordinates\n\n\n\nrx, ry\nHorizontal and Vertical Radius\n\n\n\nx1, y1\nStarting Point Coordinates\n\n\n\nx2, y2\nEnding Point Coordinates"
  },
  {
    "objectID": "notes/svg/index.html#path",
    "href": "notes/svg/index.html#path",
    "title": "SVG",
    "section": "3. Path",
    "text": "3. Path\nThe  element is capable of drawing shapes, and it has one specific attribute; d. The value of d consists of a string consisting of numbers such as;\n\n```{ojs}\n<path d=\"M40 50 L70 60 L70 40 Z\" />\n```\n\n\n\n\n\n\n\nOJS Syntax Error (line 38, column 1)Unexpected token"
  },
  {
    "objectID": "blog/2023/what-the-inside-of-a-flooded-house-looks-like/index.html",
    "href": "blog/2023/what-the-inside-of-a-flooded-house-looks-like/index.html",
    "title": "What the Inside of a Flooded House Looks Like",
    "section": "",
    "text": "Yes, I do want to blog more. It‚Äôs just kind of a chore, going from looking at one screen for 8 hours to looking at another one long enough to not procrastinate indefinitely to the point I end up accomplishing nothing at all.\nToday, my six April viewers, will be a little more personal. I‚Äôve had these photos saved to my desktop for about seven months now, and I‚Äôve been meaning to get rid of them. What better way to solve both problems then to share my suffering digitally."
  },
  {
    "objectID": "blog/2023/what-the-inside-of-a-flooded-house-looks-like/index.html#ian",
    "href": "blog/2023/what-the-inside-of-a-flooded-house-looks-like/index.html#ian",
    "title": "What the Inside of a Flooded House Looks Like",
    "section": "Ian",
    "text": "Ian\nI remember lying on my bed, reading on my Chromebook, when the first waves started smashing against the doors. We had some extra mops in case there was any water damage, which‚Ä¶ yeah. When I say ‚Äúwe,‚Äù it was my Dad, my dog Wyatt, and myself. By this point, a strategic decision was made that the only thing we could do was prioritize saving as many valuables as we could. As a result, I don‚Äôt have any pictures/videos of the flood as it was increasing, but luckily, I do have some pictures at it‚Äôs absolute worst.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOh yeah, I block out stickers on my overturned refrigerator cause I‚Äôm overly paranoid."
  },
  {
    "objectID": "blog/2023/what-the-inside-of-a-flooded-house-looks-like/index.html#after-the-storm",
    "href": "blog/2023/what-the-inside-of-a-flooded-house-looks-like/index.html#after-the-storm",
    "title": "What the Inside of a Flooded House Looks Like",
    "section": "After the Storm",
    "text": "After the Storm\nThe FEMA inspector who came to inspect our house said the water level reached about 46 inches. We ended up going back the next day to check the damage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvasive fucks ü¶é\nPictures of the outside:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThanks for reading. My next post will be a major update and will undoubtedly be much more depressing than this post. Stay tuned."
  }
]