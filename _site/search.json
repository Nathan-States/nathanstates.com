[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nathan States",
    "section": "",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplaining the CFO Act of 1990\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2023\n\n\nNathan States\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a COVID-19 Economic Relief Database: Part 1 (usaspending.gov)\n\n\n\n\n\nLaying the groundwork for the database and talking about the problem with public data on subsidies as a whole.\n\n\n\n\n\n\nFeb 1, 2023\n\n\nNathan States\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing 20 Years of PCAOB Data: Part 2 (Enforcement)\n\n\n\n\n\n\n\npcaob\n\n\ngovernment\n\n\n\n\nThe PCAOB can fine firms as much as $15 million for infractions, but how often are they really disciplined?\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing 20 Years of PCAOB Data: Part 1 (Inspections)\n\n\n\n\n\n\n\npcaob\n\n\ngovernment\n\n\n\n\nBreaking down inspection reports from eight of the largest audit firms, including international affiliates.\n\n\n\n\n\n\nJan 27, 2023\n\n\nNathan States\n\n\n\n\n\n\n  \n\n\n\n\nBengaluru Text Mining\n\n\n\n\n\n\n\ntext_mining\n\n\nmachine_learning\n\n\n\n\nAnalyzing over 100,000 complaints from the Silicon Valley of India.\n\n\n\n\n\n\nJan 22, 2023\n\n\nNathan States\n\n\n\n\n\n\n  \n\n\n\n\nSIPRI Dashboard Announcement\n\n\n\n\n\n\n\nmilitary\n\n\ndashboards\n\n\nproject_announcements\n\n\n\n\nAnnouncing my new RShiny Dashboard that allows you to view military spending around the world.\n\n\n\n\n\n\nJan 15, 2023\n\n\nNathan States\n\n\n\n\n\n\n  \n\n\n\n\nSwitching Over to Quarto (And Why You Should Consider Too)\n\n\n\n\n\n\n\nupdates\n\n\n\n\nExplaining the new look.\n\n\n\n\n\n\nJan 10, 2023\n\n\nNathan States\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nathan States",
    "section": "",
    "text": "I’m a data scientist who is passionate about statistics, government, and socioeconomic issues, focusing primarily on the United States. This blog hosts all my work I’ve done related to those topics. While this site is geared towards other data people, it’s also meant to be accessible to anyone who also shares my passions. No matter your background, I hope you find some use out of this site.\nCheck out my recent posts, new projects, and updates to the archives. I also have a notes tab that has info on various data tools I use in real life.\n\n\nIf you’re interested in collaborating together, or have other business inquiries, head over to my contact page and hit me up.\n\n\n\nGet a PDF of my resume here."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "",
    "text": "To all my eight viewers who happened to stumble upon the initial release of my blog - you might of noticed it looks drastically different now. When I first built this site, I was using blogdown along with Hugo Apero, but I’m making the switch over to Quarto from now on.\nHere are some reasons why you should consider migrating as well."
  },
  {
    "objectID": "posts/welcome/index.html#quarto-is-easy.",
    "href": "posts/welcome/index.html#quarto-is-easy.",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "1. Quarto is easy.",
    "text": "1. Quarto is easy.\nblogdown is essentially the same as Hugo, but with the ability to compile RMarkdown files. Some themes are very easy to work with, while others can be a bit of a pain. Quarto is extremely simple and comes with all the tools anyone would need for blogging.\nHugo Apero, the theme I used previously, is actually designed really well in this respect, so this may or may not be a problem for you - it’s very theme dependent."
  },
  {
    "objectID": "posts/welcome/index.html#not-just-for-blogging.",
    "href": "posts/welcome/index.html#not-just-for-blogging.",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "2. Not just for blogging.",
    "text": "2. Not just for blogging.\nQuarto is capable of creating much more than blogs; presentations, PDF reports, interactive documents, and easy documentation pages are all supported. You can view some examples in their gallery.\nThere’s obviously a good amount of carryover between these formats and this blog, so it’s good practice for the real world down the line."
  },
  {
    "objectID": "posts/welcome/index.html#it-supports-multiple-languages",
    "href": "posts/welcome/index.html#it-supports-multiple-languages",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "3. It supports multiple languages",
    "text": "3. It supports multiple languages\nTechnically, blogdown does as well; you can use the reticulate package and hard-code HTML into RMarkdown to create D3 objects, for example. And by that nature, you can always just stick with RMarkdown. However, Quarto uses a custom .qmd format, which is specifically designed for cross-language support. That’s more or less the selling point of using Quarto."
  },
  {
    "objectID": "posts/welcome/index.html#awesome-visual-bar",
    "href": "posts/welcome/index.html#awesome-visual-bar",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "4. Awesome Visual Bar",
    "text": "4. Awesome Visual Bar\nThis probably falls under point #1, but it’s so neat that it deserves it’s own section. The visual tab makes creating documents incredibly intuitive and easy, and includes almost everything you’d want out of a modern text editor.\nIt’s pretty neat."
  },
  {
    "objectID": "posts/welcome/index.html#its-compatible-with-hugo.",
    "href": "posts/welcome/index.html#its-compatible-with-hugo.",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "5. It’s compatible with Hugo.",
    "text": "5. It’s compatible with Hugo.\nMy old posts were written in RMarkdown, but Quarto only supports qmd. Hugo understands this format perfectly fine, though, so if I were to ever go back to blogdown, it would be incredibly easy to migrate."
  },
  {
    "objectID": "posts/welcome/index.html#superior-to-distill",
    "href": "posts/welcome/index.html#superior-to-distill",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "6. Superior to Distill?",
    "text": "6. Superior to Distill?\nI don’t think the advantages of Quarto are so significant that it mandates switching over to Distill. With that said, while both offer blogs that are extremely similar in style, Quarto has more features and support. If you are creating your first blog, there isn’t much of a reason (IMO) to choose Distill over Quarto."
  },
  {
    "objectID": "posts/welcome/index.html#posts-to-read-about-quarto",
    "href": "posts/welcome/index.html#posts-to-read-about-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "Posts to Read About Quarto",
    "text": "Posts to Read About Quarto\nHere is a github that has a ton of links related to making awesome things in Quarto. Here are some posts I’d recommend reading:\n\nAlison Hill: We don’t talk about Quarto..\nDanielle Navarro: Posting a Distill blog to Quarto.\nYihui Xie: With Quarto Coming, is RMarkdown Going Away? No.\n\nOn that last point, I’ll briefly discuss the…"
  },
  {
    "objectID": "posts/welcome/index.html#disadvantages-of-quarto",
    "href": "posts/welcome/index.html#disadvantages-of-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider)",
    "section": "Disadvantages of Quarto",
    "text": "Disadvantages of Quarto\nReally, the main one is lack of customization. Hugo is far more complex, and can create some sophisticated websites to suit any need. Quarto, not so much. This does make blogging on this platform feel slightly generic in comparison, unfortunately. Optimized Hugo sites might also be faster, though admittedly, I have no idea if this is likely / true or not.\nWith that said, the pros outweigh the cons in my case, which is why I decided to make the jump. I hope you enjoy the new look."
  },
  {
    "objectID": "posts/pcaob-part-1/index.html",
    "href": "posts/pcaob-part-1/index.html",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "",
    "text": "A couple years back, I remember reading a story published by the Project on Government Oversight titled, “How an Agency You’ve Never Heard of Is Leaving the Economy at Risk.” The agency in question is the Public Company Accounting Oversight Board (PCAOB), who is responsible for regulating the audit industry.\nThe article breaks down how the board has been influenced by politics, limited by budget restrictions, and directly manipulated by the accounting industry itself. Their conclusion - which they support by collecting data from two PCAOB databases - is that the agency isn’t effective, is lenient on enforcement, and likely not looking in the interests of investors.\nWhen the SEC was created in 1934, all publicly traded companies were required to be audited, but the audit industry itself remained almost entirely self-regulated. There were several noticeable incidents that cast doubt on such an arrangement, but none were greater than the collapse of Enron in 2002. At one point the 6th largest corporation in the United States, the energy giant would file for bankruptcy less than a year later once it was discovered that the vast majority of their profits were the result of fraud. Arthur Andersen, the firm who was receiving $52 million annually to “audit” Enron, was eventually charged (though never officially tried due to a technicality regarding jury instructions) by the DOJ for shredding documents related to Enron. In 2002, Congress passed the Sarbanes-Oxley Act that strengthened auditing standards, and more-or-less put the PCAOB in charge of enforcing them, along with some oversight from the SEC.\nIt’s an interesting read, but while the article does rely on some data analysis, it doesn’t do a full dive into each of the databases. Out of curiosity, I decided to scrape 20 years worth of inspection reports from the PCAOB to get a more data-driven view of the regulatory agency. Here’s what I found."
  },
  {
    "objectID": "posts/pcaob-part-1/index.html#results",
    "href": "posts/pcaob-part-1/index.html#results",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Results",
    "text": "Results\n\n# Load Libraries \nlibrary(arrow)\nlibrary(dplyr)\nlibrary(htmltools)\nlibrary(echarts4r)\nlibrary(reactable)\nlibrary(reactablefmtr)\n\n# Import Data \noverview <- read_csv_arrow(\"_data/overview.csv\")\ninspections <- read_csv_arrow(\"_data/inspections.csv\")\n\n# Theme Defaults \ne_common(\n  font_family = \"Georgia\"\n)\n\n\ninspections$year <- as.Date(as.character(inspections$year), format = \"%Y\")\ninspections$year <- substring(inspections$year, 1, 4)\n\nget_def_color <- function(def_rate) {\n  orange_pal <- function(x) rgb(colorRamp(c(\"#a43434\", \"#ffb150\"))(x), maxColorValue = 255)\n  normalized <- (def_rate - min(def_rate)) / (max(def_rate) - min(def_rate))\n  orange_pal(normalized)\n}\n\noverview <- overview |>\n  mutate(\n    fail_rate_domestic = domestic_audits_failed / domestic_audits_inspected,\n    fail_Rate_international = international_audits_failed / international_audits_inspected,\n    total_audits_inspected = domestic_audits_inspected + international_audits_inspected,\n    total_audits_failed = domestic_audits_failed + international_audits_failed,\n    total_fail_rate = total_audits_failed / total_audits_inspected\n  ) |>\n  mutate(\n    domestic_def_color = get_def_color(fail_rate_domestic),\n    international_def_color = get_def_color(fail_Rate_international),\n    total_def_color = get_def_color(total_fail_rate)\n  ) |>\n  select(Firm, domestic_audits_inspected, domestic_audits_failed, fail_rate_domestic, international_audits_inspected, international_audits_failed, fail_Rate_international, total_audits_inspected, total_audits_failed, total_fail_rate, domestic_def_color, international_def_color, total_def_color)\n\n\nfunction renderUserScore(cellInfo) {\n  return donutChart(cellInfo.value, cellInfo.row['score_color'])\n}\n\nfunction donutChart(value, color) {\n  // All units are in rem for relative scaling\n  const radius = 1.5\n  const diameter = 3.75\n  const center = diameter / 2\n  const width = 0.25\n  const sliceLength = 2 * Math.PI * radius\n  const sliceOffset = sliceLength * (1 - value / 100)\n  const donutChart = `\n    <svg width=\"${diameter}rem\" height=\"${diameter}rem\" style=\"transform: rotate(-90deg)\" focusable=\"false\">\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"rgba(0,0,0,0.1)\"></circle>\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"${color}\"\n       stroke-dasharray=\"${sliceLength}rem\" stroke-dashoffset=\"${sliceOffset}rem\"></circle>\n    </svg>\n  `\n  const label = `\n    <div style=\"position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%)\">\n      ${value}%\n    </div>\n  `\n  return `\n    <div style=\"display: inline-flex; position: relative\">\n      ${donutChart}\n      ${label}\n    </div>\n  `\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noverview |>\n  reactable(\n    columns = list(\n      Firm = colDef(cell = function(value) {\n        img_src <- knitr::image_uri(sprintf(\"_logos/%s.png\", value))\n        image <- img(src = img_src, height = \"31px\", alt = \"\")\n        tagList(\n          div(style = list(display = \"inline-block\", width = \"45px\"), image),\n          value\n        )\n      }),\n      domestic_audits_inspected = colDef(name = \"Inspected\"),\n      domestic_audits_failed = colDef(name = \"Failed\"),\n      fail_rate_domestic = colDef(\n        name = \"Def. %\", \n        format = colFormat(percent = TRUE, digits = 2),\n        html = TRUE,\n        align = \"center\",\n        width = 140,\n        class = \"user-score\",\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      international_audits_inspected = colDef(name = \"Inspected\"),\n      international_audits_failed = colDef(name = \"Failed\"),\n      fail_Rate_international = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2),\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      total_audits_inspected = colDef(name = \"Inspected\"),\n      total_audits_failed = colDef(name = \"Failed\"),\n      total_fail_rate = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2)\n      )\n    )\n  ) |>\n  add_title(\"PCAOB Inspection Results\")\n\n\nPCAOB Inspection Results\n\n\n\n\n\ninspections %>%\n  filter(country == \"United States\") %>%\n  filter(year > \"2008\") %>%\n  filter(firm != \"Marcum\") %>%\n  mutate(Fail_Rate = audits_failed / audits_inspected) %>%\n  group_by(firm) %>%\n  e_chart(year) %>%\n  e_line(Fail_Rate) %>%\n  e_y_axis(\n    formatter = e_axis_formatter(style = \"percent\")\n  ) %>%\n  e_legend(\n    type = \"scroll\",\n    bottom = 0.5\n  ) %>%\n  e_legend_unselect(\"Crowe\") %>%\n  e_legend_unselect(\"BDO\") %>%\n  e_legend_unselect(\"Grant Thornton\") %>%\n  e_legend_unselect(\"RSM\") %>%\n  e_color(\n    c(\"#1e1e1e\", \"#a29f00\", \"#249d24\", \"#ff7d08\", \"#68249d\", \"#1672b2\", \"#d52323\", \"#9d1774\")\n  ) %>%\n  e_title(\n    top = -5, \n    text = \"Domestic Big Four Firms Have An Average Deficiency Rate of 26.1%\",\n    left = \"center\",\n    textStyle = list(\n      fontSize = 20\n    )\n  ) %>%\n  e_axis_labels(\n    y = \"Deficiency Rate\"\n  ) %>%\n  e_toolbox_feature(\n    feature = \"saveAsImage\"\n  ) %>%\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    textStyle = list(\n      color = \"#fcfcfc\"\n    )\n  )"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Nathan States",
    "section": "",
    "text": "Reach out to me!\nComments are enabled in the archives, but disabled everywhere else. If you’d like to drop a comment about something in particular, or reach out to me for professional inquiries, message me on one of the following platforms.\n\n\n\n Twitter  Mastodon  GitHub\nOr you can send me an email at nathanstates@outlook.com."
  },
  {
    "objectID": "about.html#lets-work-together.",
    "href": "about.html#lets-work-together.",
    "title": "Nathan States",
    "section": "🤝 Let’s work together.",
    "text": "🤝 Let’s work together.\nIf you’re interested in collaborating together, or have other business inquiries, head over to my contact page and hit me up."
  },
  {
    "objectID": "about.html#resume-rundown.",
    "href": "about.html#resume-rundown.",
    "title": "Nathan States",
    "section": "🧾 Resume Rundown.",
    "text": "🧾 Resume Rundown.\nDownload a PDF of my resume here.\n\nSkills\n\nProgramming: R | Python | SQL | SAS | Javascript | D3\nTechnology: Tableau | Power BI | Apache Arrow | SPSS\nDomain Knowledge: Data Wrangling | Data Visualizations\n\n\n\nEducation\n\nUniversity of redacted, redacted | redacted, Florida\nBachelors of Science in Mathematics | Dec. 2022\n\n\n\nFeatured Projects\n\nSIPRI Dashboard:\nBengaluru Text Mining:\nPCAOB Data Analysis:"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Nathan States",
    "section": "",
    "text": "Projects\nA list of all of the datasets, dashboards, and other web applications I’ve created.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🪖 SIPRI Dashboard\n\n\nView military expenditure around the world.\n\n\n\nNathan States\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/sipri-dashboard/index.html#sipri-dashboard",
    "href": "projects/sipri-dashboard/index.html#sipri-dashboard",
    "title": "🪖 SIPRI Dashboard",
    "section": "SIPRI Dashboard",
    "text": "SIPRI Dashboard\nThis web application allows users to view military spending across the world using several different metrics, including:\n\nConverted USD\nInflation-Adjusted USD\nMilitary Spending in GDP Percentage\nMilitary Spending in Government Percentage\nMilitary Spending in Capita Spending\n\nYou can view the dashboard by clicking here."
  },
  {
    "objectID": "projects/sipri-dashboard/index.html#data-source",
    "href": "projects/sipri-dashboard/index.html#data-source",
    "title": "🪖 SIPRI Dashboard",
    "section": "Data Source",
    "text": "Data Source\nThe data comes from the Stockholm International Peace Research Institute (SIPRI), a think tank that researches conflict, armaments, arms control, and disarmament around the world. SIPRI was created by the Swedish Parliament in July 1966, who gives them an annual grant to fund their operations. They also receive funding from a variety of other governments and NGOs, including the EU, Australian Government, the University of Notre Dame, the Norwegian Ministry, and many more.\nYou can download the database used for this project here.\n\nPhoto Credit\nPhoto by Juli Kosolapova.\n\n\nSidenote\nThis application is currently being hosted on shinyapps.io on a free plan. Performance can sometimes be slow, and applications are limited to 25 active hours a month. It’s possible to download the source code and launch the app locally to avoid those issues."
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "",
    "text": "To all my eight viewers who happened to stumble upon the initial release of my blog - you might of noticed it looks drastically different now. When I first built this site, I was using blogdown along with Hugo Apero, but I’m making the switch over to Quarto from now on.\nHere are some reasons why you should consider migrating as well."
  },
  {
    "objectID": "blog/welcome/index.html#quarto-is-easy.",
    "href": "blog/welcome/index.html#quarto-is-easy.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "1. Quarto is easy.",
    "text": "1. Quarto is easy.\nblogdown is essentially the same as Hugo, but with the ability to compile RMarkdown files. Some themes are very easy to work with, while others can be a bit of a pain. Quarto is extremely simple and comes with all the tools anyone would need for blogging.\nHugo Apero, the theme I used previously, is actually designed really well in this respect, so this may or may not be a problem for you - it’s very theme dependent."
  },
  {
    "objectID": "blog/welcome/index.html#not-just-for-blogging.",
    "href": "blog/welcome/index.html#not-just-for-blogging.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "2. Not just for blogging.",
    "text": "2. Not just for blogging.\nQuarto is capable of creating much more than blogs; presentations, PDF reports, interactive documents, and easy documentation pages are all supported. You can view some examples in their gallery.\nThere’s obviously a good amount of carryover between these formats and this blog, so it’s good practice for the real world down the line."
  },
  {
    "objectID": "blog/welcome/index.html#it-supports-multiple-languages",
    "href": "blog/welcome/index.html#it-supports-multiple-languages",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "3. It supports multiple languages",
    "text": "3. It supports multiple languages\nTechnically, blogdown does as well; you can use the reticulate package and hard-code HTML into RMarkdown to create D3 objects, for example. And by that nature, you can always just stick with RMarkdown. However, Quarto uses a custom .qmd format, which is specifically designed for cross-language support. That’s more or less the selling point of using Quarto."
  },
  {
    "objectID": "blog/welcome/index.html#awesome-visual-bar",
    "href": "blog/welcome/index.html#awesome-visual-bar",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "4. Awesome Visual Bar",
    "text": "4. Awesome Visual Bar\nThis probably falls under point #1, but it’s so neat that it deserves it’s own section. The visual tab makes creating documents incredibly intuitive and easy, and includes almost everything you’d want out of a modern text editor.\nIt’s pretty neat."
  },
  {
    "objectID": "blog/welcome/index.html#its-compatible-with-hugo.",
    "href": "blog/welcome/index.html#its-compatible-with-hugo.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "5. It’s compatible with Hugo.",
    "text": "5. It’s compatible with Hugo.\nMy old posts were written in RMarkdown, but Quarto only supports qmd. Hugo understands this format perfectly fine, though, so if I were to ever go back to blogdown, it would be incredibly easy to migrate."
  },
  {
    "objectID": "blog/welcome/index.html#superior-to-distill",
    "href": "blog/welcome/index.html#superior-to-distill",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "6. Superior to Distill?",
    "text": "6. Superior to Distill?\nI don’t think the advantages of Quarto are so significant that it mandates switching over to Distill. With that said, while both offer blogs that are extremely similar in style, Quarto has more features and support. If you are creating your first blog, there isn’t much of a reason (IMO) to choose Distill over Quarto."
  },
  {
    "objectID": "blog/welcome/index.html#posts-to-read-about-quarto",
    "href": "blog/welcome/index.html#posts-to-read-about-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "Posts to Read About Quarto",
    "text": "Posts to Read About Quarto\nHere is a github that has a ton of links related to making awesome things in Quarto. Here are some posts I’d recommend reading:\n\nAlison Hill: We don’t talk about Quarto..\nDanielle Navarro: Posting a Distill blog to Quarto.\nYihui Xie: With Quarto Coming, is RMarkdown Going Away? No.\n\nOn that last point, I’ll briefly discuss the…"
  },
  {
    "objectID": "blog/welcome/index.html#disadvantages-of-quarto",
    "href": "blog/welcome/index.html#disadvantages-of-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "Disadvantages of Quarto",
    "text": "Disadvantages of Quarto\nReally, the main one is lack of customization. Hugo is far more complex, and can create some sophisticated websites to suit any need. Quarto, not so much. This does make blogging on this platform feel slightly generic in comparison, unfortunately. Optimized Hugo sites might also be faster, though admittedly, I have no idea if this is likely / true or not.\nWith that said, the pros outweigh the cons in my case, which is why I decided to make the jump. I hope you enjoy the new look."
  },
  {
    "objectID": "notes/echarts4r/index.html",
    "href": "notes/echarts4r/index.html",
    "title": "echarts4r Cookbook",
    "section": "",
    "text": "There’s a ton of different charting libraries in R, with a few of them being interactive. plotly is one of the most popular, having over 2.3k stars on GitHub. highcharter is another such package. With ggiraph, it’s possible to turn ggplot2 objects into interactive charts, so there’s quite a selection of tools to choose from.\nPersonally, I find echarts4r to be the best out of all of them specifically for exploratory data analysis. The charts are easy to create, they look nice out of the box, and it’s quick and easy to gain insight into your data. The interactive element makes identifying outliers or other specific values very easy. Compared to other packages, plotly is way too ugly, highcharter requires a license for paid applications, and ggiraph is too time consuming for regular charts. echarts4r offers a nice solution to all these problems (though if you need a deeper statistical breakdown, I’d recommend ggstatsplot).\nThere is a noticeable limit on customization, though. I’d still recommend ggplot2 for high quality static charts, and D3 for more advanced interactive ones.\necharts4r is a wrapper for the JS library developed by Apache Software Foundation, meaning changes in the original code will cause changes in this package, regardless of if echarts4r itself is updated. Apache ECharts is listed as one of Apache’s top projects, so hopefully, we see even more improvements in the future."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Nathan States",
    "section": "",
    "text": "Notes\n\n\nI’m currently in the process of moving my notes for various data tools from Notion into my blog. This page is entirely for personal use, but maybe someone can get some useful info out of it.\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 10, 2023\n\n\necharts4r Cookbook\n\n\nNathan States\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/pcaob-part-1/index.html",
    "href": "blog/pcaob-part-1/index.html",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "",
    "text": "A couple years back, I remember reading a story published by the Project on Government Oversight titled, “How an Agency You’ve Never Heard of Is Leaving the Economy at Risk.” The agency in question is the Public Company Accounting Oversight Board (PCAOB), who is responsible for regulating the audit industry.\nThe article breaks down how the board has been influenced by politics, limited by budget restrictions, and directly manipulated by the accounting industry itself. Their conclusion - which they support by collecting data from two PCAOB databases - is that the agency isn’t effective, is lenient on enforcement, and likely not looking in the interests of investors.\nWhen the SEC was created in 1934, all publicly traded companies were required to be audited, but the audit industry itself remained almost entirely self-regulated. There were several noticeable incidents that cast doubt on such an arrangement, but none were greater than the collapse of Enron in 2002. At one point the 6th largest corporation in the United States, the energy giant would file for bankruptcy less than a year later once it was discovered that the vast majority of their profits were the result of fraud. Arthur Andersen, the firm who was receiving $52 million annually to “audit” Enron, was eventually charged (though never officially tried due to a technicality regarding jury instructions) by the DOJ for shredding documents related to Enron. In 2002, Congress passed the Sarbanes-Oxley Act that strengthened auditing standards, and more-or-less put the PCAOB in charge of enforcing them, along with some oversight from the SEC.\nIt’s an interesting read, but while the article does rely on some data analysis, it doesn’t do a full dive into each of the databases. Out of curiosity, I decided to scrape 20 years worth of inspection reports from the PCAOB to get a more data-driven view of the regulatory agency. Here’s what I found."
  },
  {
    "objectID": "blog/pcaob-part-1/index.html#results",
    "href": "blog/pcaob-part-1/index.html#results",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Results",
    "text": "Results\n\n# Load Libraries \nlibrary(arrow)\nlibrary(dplyr)\nlibrary(htmltools)\nlibrary(echarts4r)\nlibrary(reactable)\nlibrary(reactablefmtr)\n\n# Import Data \noverview <- read_csv_arrow(\"_data/overview.csv\")\ninspections <- read_csv_arrow(\"_data/inspections.csv\")\n\n# Theme Defaults \ne_common(\n  font_family = \"Georgia\"\n)\n\n\ninspections$year <- as.Date(as.character(inspections$year), format = \"%Y\")\ninspections$year <- substring(inspections$year, 1, 4)\n\nget_def_color <- function(def_rate) {\n  orange_pal <- function(x) rgb(colorRamp(c(\"#a43434\", \"#ffb150\"))(x), maxColorValue = 255)\n  normalized <- (def_rate - min(def_rate)) / (max(def_rate) - min(def_rate))\n  orange_pal(normalized)\n}\n\noverview <- overview |>\n  mutate(\n    fail_rate_domestic = domestic_audits_failed / domestic_audits_inspected,\n    fail_Rate_international = international_audits_failed / international_audits_inspected,\n    total_audits_inspected = domestic_audits_inspected + international_audits_inspected,\n    total_audits_failed = domestic_audits_failed + international_audits_failed,\n    total_fail_rate = total_audits_failed / total_audits_inspected\n  ) |>\n  mutate(\n    domestic_def_color = get_def_color(fail_rate_domestic),\n    international_def_color = get_def_color(fail_Rate_international),\n    total_def_color = get_def_color(total_fail_rate)\n  ) |>\n  select(Firm, domestic_audits_inspected, domestic_audits_failed, fail_rate_domestic, international_audits_inspected, international_audits_failed, fail_Rate_international, total_audits_inspected, total_audits_failed, total_fail_rate, domestic_def_color, international_def_color, total_def_color)\n\n\nfunction renderUserScore(cellInfo) {\n  return donutChart(cellInfo.value, cellInfo.row['score_color'])\n}\n\nfunction donutChart(value, color) {\n  // All units are in rem for relative scaling\n  const radius = 1.5\n  const diameter = 3.75\n  const center = diameter / 2\n  const width = 0.25\n  const sliceLength = 2 * Math.PI * radius\n  const sliceOffset = sliceLength * (1 - value / 100)\n  const donutChart = `\n    <svg width=\"${diameter}rem\" height=\"${diameter}rem\" style=\"transform: rotate(-90deg)\" focusable=\"false\">\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"rgba(0,0,0,0.1)\"></circle>\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"${color}\"\n       stroke-dasharray=\"${sliceLength}rem\" stroke-dashoffset=\"${sliceOffset}rem\"></circle>\n    </svg>\n  `\n  const label = `\n    <div style=\"position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%)\">\n      ${value}%\n    </div>\n  `\n  return `\n    <div style=\"display: inline-flex; position: relative\">\n      ${donutChart}\n      ${label}\n    </div>\n  `\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noverview |>\n  reactable(\n    columns = list(\n      Firm = colDef(cell = function(value) {\n        img_src <- knitr::image_uri(sprintf(\"_logos/%s.png\", value))\n        image <- img(src = img_src, height = \"31px\", alt = \"\")\n        tagList(\n          div(style = list(display = \"inline-block\", width = \"45px\"), image),\n          value\n        )\n      }),\n      domestic_audits_inspected = colDef(name = \"Inspected\"),\n      domestic_audits_failed = colDef(name = \"Failed\"),\n      fail_rate_domestic = colDef(\n        name = \"Def. %\", \n        format = colFormat(percent = TRUE, digits = 2),\n        html = TRUE,\n        align = \"center\",\n        width = 140,\n        class = \"user-score\",\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      international_audits_inspected = colDef(name = \"Inspected\"),\n      international_audits_failed = colDef(name = \"Failed\"),\n      fail_Rate_international = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2),\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      total_audits_inspected = colDef(name = \"Inspected\"),\n      total_audits_failed = colDef(name = \"Failed\"),\n      total_fail_rate = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2)\n      )\n    )\n  ) |>\n  add_title(\"PCAOB Inspection Results\")\n\n\nPCAOB Inspection Results\n\n\n\n\n\ninspections %>%\n  filter(country == \"United States\") %>%\n  filter(year > \"2008\") %>%\n  filter(firm != \"Marcum\") %>%\n  mutate(Fail_Rate = audits_failed / audits_inspected) %>%\n  group_by(firm) %>%\n  e_chart(year) %>%\n  e_line(Fail_Rate) %>%\n  e_y_axis(\n    formatter = e_axis_formatter(style = \"percent\")\n  ) %>%\n  e_legend(\n    type = \"scroll\",\n    bottom = 0.5\n  ) %>%\n  e_legend_unselect(\"Crowe\") %>%\n  e_legend_unselect(\"BDO\") %>%\n  e_legend_unselect(\"Grant Thornton\") %>%\n  e_legend_unselect(\"RSM\") %>%\n  e_color(\n    c(\"#1e1e1e\", \"#a29f00\", \"#249d24\", \"#ff7d08\", \"#68249d\", \"#1672b2\", \"#d52323\", \"#9d1774\")\n  ) %>%\n  e_title(\n    top = -5, \n    text = \"Domestic Big Four Firms Have An Average Deficiency Rate of 26.1%\",\n    left = \"center\",\n    textStyle = list(\n      fontSize = 20\n    )\n  ) %>%\n  e_axis_labels(\n    y = \"Deficiency Rate\"\n  ) %>%\n  e_toolbox_feature(\n    feature = \"saveAsImage\"\n  ) %>%\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    textStyle = list(\n      color = \"#fcfcfc\"\n    )\n  )"
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html",
    "href": "blog/2023/pcaob-part-1-inspections/index.html",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "",
    "text": "A couple years back, I remember reading a story published by the Project on Government Oversight (POGO) titled, “How an Agency You’ve Never Heard of Is Leaving the Economy at Risk.” The agency in question is the Public Company Accounting Oversight Board (PCAOB), who is responsible for regulating the audit industry. The audit industry was once almost entirely self-regulated, but that changed after the collapse of Enron in 2002.\nThe article starts by pointing out that, despite the PCAOB finding hundreds of audits performed deficiently among the four largest audits firms in the United States, the board rarely deals out punishment. While the board can fine firms up to $2,000,000 for each audit performed deficiently, they’ve only fined the big four a total of $6,500,000 (not including their international affiliates). While the article was written in 2019, that number hasn’t changed as of February 2023.\nThe PCAOB classifies deficiencies into two categories:\nThe article continues by interviewing several former board members, asking them questions about the PCAOB budget, inner politics, and the overall lack of enforcement. They ultimately conclude that the board is weak and ineffective.\nIt’s an interesting read, but while the article does rely on some data analysis, it doesn’t fully analyze either of the databases. Out of curiosity, I decided to collect 20 years worth of inspection reports from the PCAOB to see if the data supports POGO’s conclusions. Here’s what I found."
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html#results",
    "href": "blog/2023/pcaob-part-1-inspections/index.html#results",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Results",
    "text": "Results\nCode for my programmers; scroll down to table if you don’t care.\nSetup: Here, we import the libraries we need along with the data.\n\n\nShow code\n# Load Libraries \nlibrary(arrow)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggiraph)\nlibrary(htmltools)\nlibrary(reactable)\nlibrary(reactablefmtr)\n\n# Import Data \noverview <- read_csv_arrow(\"_data/overview.csv\")\ninspections <- read_csv_arrow(\"_data/inspections.csv\")\n\n\nData Wrangling: Next, we fix the year column and create the inspection overview by summarizing the columns with dplyr. We also create a custom function for finding out color codes for inspection results, which will be used in the reactable table.\n\n\nShow code\n# Getting only the year from date column \ninspections$year <- as.Date(as.character(inspections$year), format = \"%Y\")\ninspections$year <- substring(inspections$year, 1, 4)\n\n# Defining a function for calculating color scale for def rates \nget_def_color <- function(def_rate) {\n  orange_pal <- function(x) rgb(colorRamp(c(\"#a43434\", \"#ffb150\"))(x), maxColorValue = 255)\n  normalized <- (def_rate - min(def_rate)) / (max(def_rate) - min(def_rate))\n  orange_pal(normalized)\n}\n\n# Calculating rates, totals, and select column order for domestic and international \noverview_domestic <- inspections |>\n  filter(!is.na(audits_inspected)) |>\n  filter(!is.na(audits_failed)) |>\n  filter(country == \"United States\") |>\n  group_by(firm) |>\n  summarise(\n    domestic_audits_inspected = sum(audits_inspected, na.rm = TRUE),\n    domestic_audits_failed = sum(audits_failed, na.rm = TRUE),\n    domestic_fail_rate = (domestic_audits_failed / domestic_audits_inspected) * 100,\n  ) |>\n  mutate(domestic_def_color = get_def_color(domestic_fail_rate))\n\noverview_international <- inspections |>\n  filter(!is.na(audits_inspected)) |>\n  filter(!is.na(audits_failed)) |>\n  filter(country != \"United States\") |>\n  group_by(firm) |>\n  summarise(\n    international_audits_inspected = sum(audits_inspected, na.rm = TRUE),\n    international_audits_failed = sum(audits_failed, na.rm = TRUE),\n    international_fail_rate = (international_audits_failed / international_audits_inspected) * 100,\n  ) |>\n  mutate(international_def_color = get_def_color(international_fail_rate))\n\noverview <- left_join(\n  x = overview_domestic,\n  y = overview_international,\n  by = \"firm\"\n)  |>\n  drop_na()\n\noverview <- overview |>\n  mutate(\n    total_audits_inspected = domestic_audits_inspected + international_audits_inspected,\n    total_audits_failed = domestic_audits_failed + international_audits_failed,\n    total_fail_rate = (total_audits_failed / total_audits_inspected) * 100,\n    total_def_color = get_def_color(total_fail_rate),\n    revenue = case_when(\n      firm == \"Deloitte\" ~ 59300000000,\n      firm == \"PricewaterhouseCoopers\" ~ 50300000000, \n      firm == \"Ernst & Young\" ~ 45400000000,\n      firm == \"KPMG\" ~ 34600000000,\n      firm == \"BDO\" ~ 12800000000,\n      firm == \"RSM\" ~ 8132000000,\n      firm == \"Grant Thornton\" ~ 2300000000,\n      firm == \"Crowe\" ~ 1062000000\n    ),\n    logo = case_when(\n            firm == \"Deloitte\" ~ \"_logos/deloitte.png\",\n      firm == \"PricewaterhouseCoopers\" ~ \"_logos/pwc.png\", \n      firm == \"Ernst & Young\" ~ \"_logos/e&y.png\",\n      firm == \"KPMG\" ~ \"_logos/kpmg.png\",\n      firm == \"BDO\" ~ \"_logos/bdo.png\",\n      firm == \"RSM\" ~ \"_logos/rsm.png\",\n      firm == \"Grant Thornton\" ~ \"_logos/thornton.png\",\n      firm == \"Crowe\" ~ \"_logos/crowe.png\"\n    )\n  ) |>\n  drop_na() |>\n  select(logo, firm, revenue, domestic_audits_inspected, domestic_audits_failed, domestic_fail_rate, international_audits_inspected, international_audits_failed, international_fail_rate, total_audits_inspected, total_audits_failed, total_fail_rate, domestic_def_color, international_def_color, total_def_color)\n\ninspections <- inspections |>\n  group_by(firm) |>\n  mutate(\n    Fail_Rate = round((audits_failed / audits_inspected) * 100, 2)\n  ) |>\n  mutate(tooltip = glue::glue(\n    \"<p><strong>{firm} {year}: </strong></p>: \",\n    \"<p>Audits Inspected: {audits_inspected}</p> \",\n    \"<p>Audits Deficient: {audits_failed}</p> \",\n    \"<p>Deficiency Rate: <strong>{Fail_Rate}%</strong></p> \"\n  )\n)\n\n\nJavascript for reactable Table: The donut charts created in the table are SVG generated using javascript. Taken from an example off the official documentation.\n\n\nShow code\nfunction renderUserScore(cellInfo) {\n  return donutChart(cellInfo.value, cellInfo.row['score_color'])\n}\n\nfunction donutChart(value, color) {\n  // All units are in rem for relative scaling\n  const radius = 1.5\n  const diameter = 3.75\n  const center = diameter / 2\n  const width = 0.25\n  const sliceLength = 2 * Math.PI * radius\n  const sliceOffset = sliceLength * (1 - value / 100)\n  const donutChart = `\n    <svg width=\"${diameter}rem\" height=\"${diameter}rem\" style=\"transform: rotate(-90deg)\" focusable=\"false\">\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"rgba(0,0,0,0.1)\"></circle>\n      <circle cx=\"${center}rem\" cy=\"${center}rem\" r=\"${radius}rem\" fill=\"none\" stroke-width=\"${width}rem\" stroke=\"${color}\"\n       stroke-dasharray=\"${sliceLength}rem\" stroke-dashoffset=\"${sliceOffset}rem\"></circle>\n    </svg>\n  `\n  const label = `\n    <div style=\"position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%)\">\n      ${value}%\n    </div>\n  `\n  return `\n    <div style=\"display: inline-flex; position: relative\">\n      ${donutChart}\n      ${label}\n    </div>\n  `\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReactable Theme\n\n\nShow code\nnytimes <- function(\n    font_size = 13,\n    font_color = \"#333333\",\n    header_font_size = 11,\n    header_font_color = \"#999999\",\n    background_color = NULL,\n    border_color = \"#e7e7e7\",\n    border_width = \"1px\",\n    cell_padding = 5,\n    centered = FALSE\n  ) {\n  reactableTheme(\n    cellStyle = centered_content,\n    color = font_color,\n    backgroundColor = background_color,\n    borderWidth = border_width,\n    borderColor = border_color,\n    stripedColor = \"#e7e7e7\",\n    highlightColor = \"#eeeeee\",\n    cellPadding = cell_padding,\n    tableStyle = list(fontSize = font_size),\n    headerStyle = list(\n      borderWidth = \"0px\",\n      padding = \"5px\",\n      background = \"#ffffff\",\n      borderColor = \"#ffffff\",\n      color = header_font_color,\n      fontWeight = \"500\",\n      textTransform = \"uppercase\",\n      fontSize = header_font_size\n    ),\n    groupHeaderStyle = list(\n      \"&:not(:empty)\" = list(\n        borderWidth = \"0px\",\n        backgroundColor = \"#ffffff\",\n        textTransform = \"uppercase\",\n        fontSize = header_font_size,\n        borderColor = \"#ffffff\",\n        color = font_color\n      )\n    ),\n    searchInputStyle = list(color = \"#333333\",\n                            fontSize = \"13px\"),\n    inputStyle = list(backgroundColor = \"#ffffff\", color = \"#333333\"),\n    rowSelectedStyle = list(backgroundColor = \"#e9edf0\"),\n    selectStyle = list(color = \"#333333\"),\n    pageButtonStyle = list(color = \"#333333\", fontSize = \"14px\"),\n    paginationStyle = list(color = \"#333333\", fontSize = \"14px\")\n  )\n}\n\n\nTable\n\n\nShow code\noverview |>\n  reactable(\n    columns = list(\n      Firm = colDef(cell = function(value) {\n        img_src <- knitr::image_uri(sprintf(\"_logos/%s.png\", value))\n        image <- img(src = img_src, width = \"60px\", height = \"32px\", alt = \"\")\n        tagList(\n          div(style = list(display = \"inline-block\", width = \"45px\"), image),\n          value\n        )\n      }),\n      domestic_audits_inspected = colDef(name = \"Inspected\"),\n      domestic_audits_failed = colDef(name = \"Failed\"),\n      fail_rate_domestic = colDef(\n        name = \"Def. %\", \n        format = colFormat(percent = TRUE, digits = 2),\n        html = TRUE,\n        align = \"center\",\n        width = 140,\n        class = \"user-score\",\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      international_audits_inspected = colDef(name = \"Inspected\"),\n      international_audits_failed = colDef(name = \"Failed\"),\n      fail_Rate_international = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2),\n        style = list(borderRight = \"1px solid rgba(0, 0, 0, 0.2)\")\n      ),\n      total_audits_inspected = colDef(name = \"Inspected\"),\n      total_audits_failed = colDef(name = \"Failed\"),\n      total_fail_rate = colDef(\n        name = \"Def. %\",\n        format = colFormat(percent = TRUE, digits = 2)\n      )\n    )\n  ) |>\n  add_title(\"PCAOB Inspection Results\")\n\n\nLet me give a quick breakdown of what you’re looking at.\nAbove are the five largest audit firms in the United States, who audit over half of all publicly traded companies, including every single company on the S&P 500. I’ve also included their 2022 revenue amounts.\nThe PCAOB will issue annual reports for the largest firms, where they will investigate a select number of audits to see if they were conducted properly. They categorize deficiencies into two different types.\nType 1.A deficiencies are defined as:\n\nDeficiencies that were of such significance that we believe the firm, at the time it issued its audit report(s), had not obtained sufficient appropriate audit evidence to support its opinion(s) on the issuer’s financial statements and/or ICFR.\n\nType 1.B deficiencies are defined as:\n\nDeficiencies that do not relate directly to the sufficiency or appropriateness of evidence the firm obtained to support its opinion(s) but nevertheless relate to instances of non-compliance with PCAOB standards or rules.\n\nThe latter of these deficiencies aren’t disclosed in any meaningful way, which is common for the PCAOB. For example, tests related to Type 1.B deficiencies aren’t performed on all audits, so it’s not possible to determine how prevalent they are among all audits.\nIn the table, only Type 1.A deficiencies are listed, and they are broken down between each firm’s domestic and international affiliates. The amounts are from 2009 - 2021 unless stated otherwise. So, for example, to read this chart, domestic affiliates of BDO failed __% of PCAOB investigations from 2009 - 2020.\nOn that note, it should be stated that the PCAOB uses a risk-based selection process when choosing which audits to investigate. This means that deficiency rates aren’t representative of how often the average audit by a given firm will be deficient.\nThis does mean, though, that the PCAOB is quite accurate at assessing which audits are likely to be conducted deficiently or not. Being able to choose audits at a near 50% rate is insanely high regardless of the selection process, but we can’t say anything overall performance of the firm. After all, it could be something else. Maybe the PCAOB especially hates KPMG and BDO for some reason.\nSince 2016, the PCAOB began including 10 randomly selected audits among each of the big four firms (6 for BDO) for their reports. While the reports themselves don’t state whether or not a randomly selected audit turned out to be deficient or not, we can reasonably assume that audits chosen randomly are less deficient on average. The number of audits investigated has stayed roughly the same (around 52-58, on average), so deficiency rates from 2016 onwards are likely underestimates.\nSide note: why doesn’t the PCAOB disclose whether randomly selected audits were deficient or not? That would actually be useful to know,"
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html#breakdown-by-year",
    "href": "blog/2023/pcaob-part-1-inspections/index.html#breakdown-by-year",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Breakdown by Year",
    "text": "Breakdown by Year\nFirm deficiency rates have changed significantly from year to year since 2009. Reports prior to that year don’t include the total number of audits inspected, which is why they’re not included in the table above, and the chart below. Oddly, Deloitte reports from 2007-2008 and the PwC 2008 do include these numbers, though not for any other firm. These are the results for domestic firms.\n\n\nShow code\ninspections |>\n  filter(country == \"United States\") |>\n  filter(year > \"2008\") |>\n  filter(firm != \"Marcum\") |>\n  mutate(Fail_Rate = audits_failed / audits_inspected) |>\n  group_by(firm) |>\n  e_chart(year) |>\n  e_line(Fail_Rate) |>\n  e_y_axis(\n    formatter = e_axis_formatter(style = \"percent\")\n  ) |>\n  e_legend(\n    type = \"scroll\",\n    bottom = 0.5\n  ) |>\n  e_legend_unselect(\"Crowe\") |>\n  e_legend_unselect(\"BDO\") |>\n  e_legend_unselect(\"Grant Thornton\") |>\n  e_legend_unselect(\"RSM\") |>\n  e_color(\n    c(\"#1e2023\", \"#5f9fa0\", \"#49a84c\", \"#f6bc00\", \"#960ff9\", \"#4c86f9\", \"#e1432e\", \"#9d1774\")\n  ) |>\n  e_axis_labels(\n    y = \"Deficiency Rate\"\n  ) |>\n  e_title(\n     text = \"Domestic Inspection Results\",\n     right = 30,\n     textStyle = list(\n       fontSize = 26\n     )\n  ) |>\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    textStyle = list(\n      color = \"#fcfcff\"\n    )\n  )\n\n\n\n\n\n\n\nI disabled other firms so the chart isn’t too cluttered. You can activate them by clicking them in the legend.\nEach of the four largest firms have performed the worst in at least one year, but KPMG has shown some impressive consistency, finishing with the worst deficiency rate in 7 out of the last 8 years (besides in 2019, where they finished 1% better than PwC - the worst firm that year). Outside of them, BDO flies high above in terms of poor performance, while other firms follow somewhat similar trends, though RSM results have been sporadic. There aren’t many international reports on firms outside of the big four, so I’ve left them out in the chart below. Here are the results for international firms.\n\n\nShow code\ninspections |>\n  filter(country != \"United States\") |>\n  filter(year > \"2006\") |>\n  filter(firm %in% c(\"Deloitte\", \"PricewaterhouseCoopers\", \"Ernst & Young\", \"KPMG\")) |>\n  filter(!is.na(audits_inspected)) |>\n  group_by(firm, year) |>\n  summarise(\n    audits_inspected = sum(audits_inspected, na.rm = TRUE),\n    audits_failed = sum(audits_failed, na.rm = TRUE)\n  ) |>\n  mutate(Fail_Rate = audits_failed / audits_inspected) |>\n  e_chart(year) |>\n  e_line(Fail_Rate) |>\n  e_y_axis(\n    formatter = e_axis_formatter(style = \"percent\")\n  ) |>\n  e_legend(\n    type = \"scroll\",\n    bottom = 0.5\n  ) |>\n  e_color(\n    c(\"#275b29\", \"#a98100\", \"#345cac\", \"#942c1e\")\n  ) |>\n  e_axis_labels(\n    y = \"Deficiency Rate\"\n  ) |>\n  e_title(\n    text = \"International Inspection Results\"\n  ) |>\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    textStyle = list(\n      color = \"#fcfcff\"\n    )\n  )\n\n\n\n\n\n\nIf domestic results were more steady and consistent, than international rates are much more irregular. This is partly because of fewer observations, but also because individual countries tended to tank performance. For example, PCAOB found that Deloitte Canada had performed 37 out of 48 audits deficiently from 2010-2015, or a 77% deficiency rate.\nCountry performance was somewhat consistent among the firms. Both KPMG and PwC Canadian affiliates also had 50%+ deficiency rates. Meanwhile, no Bermuda affiliates have deficiency rates above 25%.\nRemember when I said how inspection reports prior to 2009 don’t include the total number of audits inspected? For some reason, international affiliates have always included this number, even for reports dating back as early as 2005. Out of all the reports I’ve analyzed, though, reports from KPMG Canada from 2005-2008 were the only international reports that did not include the total number of audits inspected. I’m not sure why they’re so inconsistent about this number."
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html#other-information",
    "href": "blog/2023/pcaob-part-1-inspections/index.html#other-information",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Other Information",
    "text": "Other Information\nI’m not really sure what purpose this is supposed to serve form an investor perspective, but reports after 2014 include the revenue range, and industry sector of each audit inspected. Given that company names aren’t disclosed, it’s hard to know if this information has literally any use at all, but here it is anyways.\nIndustry Sectors.\n\n\nShow code\ninspections_category |>\n  group_by(category) |>\n  summarise(\n    Audits_Inspected = sum(`Audits Inspected`, na.rm = TRUE),\n    Audits_Deficient = sum(`Audits Deficient`, na.rm = TRUE)\n  ) |> \n  mutate(total = Audits_Inspected + Audits_Deficient) |>\n  arrange(total) |>\n  e_chart(category) |>\n  e_bar(Audits_Inspected, stack = \"group\") |>\n  e_bar(Audits_Deficient, stack = \"group\") |>\n  e_legend(show = FALSE) |>\n  e_flip_coords() |>\n  e_title(\n    text = \"Industry Ranges\",\n    left = \"left\",\n    top = -5,\n    textStyle = list(\n      fontSize = 25\n    )\n  ) |>\n  e_color(c(\"#0e437c\", \"#7393ad\")) |>\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    textStyle = list(\n      color = \"#fcfcff\"\n    )\n  )\n\n\n\n\n\n\nRevenue Ranges.\n\n\nShow code\ninspections_revenue |>\n  group_by(category) |>\n  summarise(\n    Audits_Inspected = sum(`Audits Inspected`, na.rm = TRUE),\n    Audits_Deficient = sum(`Audits Deficient`, na.rm = TRUE)\n  ) |> \n  mutate(total = Audits_Inspected + Audits_Deficient) |>\n  arrange(total) |>\n  e_chart(category) |>\n  e_bar(Audits_Inspected, stack = \"group\") |>\n  e_bar(Audits_Deficient, stack = \"group\") |>\n  e_legend(show = FALSE) |>\n  e_flip_coords() |>\n  e_title(\n    text = \"Revenue Ranges\",\n    left = \"left\",\n    top = -5,\n    textStyle = list(\n      fontSize = 25\n    )\n  ) |>\n  e_color(c(\"#0e437c\", \"#7393ad\")) |>\n  e_tooltip(\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    trigger = \"axis\",\n    textStyle = list(\n      color = \"#fcfcff\"\n    )\n  )\n\n\n\n\n\n\nBefore I say anything, I’d like to point out that one of the reports listed a category as “Other” for exactly one deficient audit. That category name has never been used again. Anyways;\nThere’s not much to say. There’s no data on the outcomes of companies who routinely receive deficient audits, so there isn’t any reason to choose your investing strategies based on these results."
  },
  {
    "objectID": "blog/2023/welcome/index.html",
    "href": "blog/2023/welcome/index.html",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "",
    "text": "To all my eight viewers who happened to stumble upon the initial release of my blog - you might of noticed it looks drastically different now. When I first built this site, I was using blogdown along with Hugo Apero, but I’m making the switch over to Quarto from now on.\nHere are some reasons why you should consider migrating as well."
  },
  {
    "objectID": "blog/2023/welcome/index.html#quarto-is-easy.",
    "href": "blog/2023/welcome/index.html#quarto-is-easy.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "1. Quarto is easy.",
    "text": "1. Quarto is easy.\nblogdown is essentially the same as Hugo, but with the ability to compile RMarkdown files. Some themes are very easy to work with, while others can be a bit of a pain. Quarto is extremely simple and comes with all the tools anyone would need for blogging.\nHugo Apero, the theme I used previously, is actually designed really well in this respect, so this may or may not be a problem for you - it’s very theme dependent."
  },
  {
    "objectID": "blog/2023/welcome/index.html#not-just-for-blogging.",
    "href": "blog/2023/welcome/index.html#not-just-for-blogging.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "2. Not just for blogging.",
    "text": "2. Not just for blogging.\nQuarto is capable of creating much more than blogs; presentations, PDF reports, interactive documents, and easy documentation pages are all supported. You can view some examples in their gallery.\nThere’s obviously a good amount of carryover between these formats and this blog, so it’s good practice for the real world down the line."
  },
  {
    "objectID": "blog/2023/welcome/index.html#it-supports-multiple-languages",
    "href": "blog/2023/welcome/index.html#it-supports-multiple-languages",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "3. It supports multiple languages",
    "text": "3. It supports multiple languages\nTechnically, blogdown does as well; you can use the reticulate package and hard-code HTML into RMarkdown to create D3 objects, for example. And by that nature, you can always just stick with RMarkdown. However, Quarto uses a custom .qmd format, which is specifically designed for cross-language support. That’s more or less the selling point of using Quarto."
  },
  {
    "objectID": "blog/2023/welcome/index.html#awesome-visual-bar",
    "href": "blog/2023/welcome/index.html#awesome-visual-bar",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "4. Awesome Visual Bar",
    "text": "4. Awesome Visual Bar\nThis probably falls under point #1, but it’s so neat that it deserves it’s own section. The visual tab makes creating documents incredibly intuitive and easy, and includes almost everything you’d want out of a modern text editor.\nIt’s pretty neat."
  },
  {
    "objectID": "blog/2023/welcome/index.html#its-compatible-with-hugo.",
    "href": "blog/2023/welcome/index.html#its-compatible-with-hugo.",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "5. It’s compatible with Hugo.",
    "text": "5. It’s compatible with Hugo.\nMy old posts were written in RMarkdown, but Quarto only supports qmd. Hugo understands this format perfectly fine, though, so if I were to ever go back to blogdown, it would be incredibly easy to migrate."
  },
  {
    "objectID": "blog/2023/welcome/index.html#superior-to-distill",
    "href": "blog/2023/welcome/index.html#superior-to-distill",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "6. Superior to Distill?",
    "text": "6. Superior to Distill?\nI don’t think the advantages of Quarto are so significant that it mandates switching over to Distill. With that said, while both offer blogs that are extremely similar in style, Quarto has more features and support. If you are creating your first blog, there isn’t much of a reason (IMO) to choose Distill over Quarto."
  },
  {
    "objectID": "blog/2023/welcome/index.html#posts-to-read-about-quarto",
    "href": "blog/2023/welcome/index.html#posts-to-read-about-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "Posts to Read About Quarto",
    "text": "Posts to Read About Quarto\nHere is a github that has a ton of links related to making awesome things in Quarto. Here are some posts I’d recommend reading:\n\nAlison Hill: We don’t talk about Quarto..\nDanielle Navarro: Posting a Distill blog to Quarto.\nYihui Xie: With Quarto Coming, is RMarkdown Going Away? No.\n\nOn that last point, I’ll briefly discuss the…"
  },
  {
    "objectID": "blog/2023/welcome/index.html#disadvantages-of-quarto",
    "href": "blog/2023/welcome/index.html#disadvantages-of-quarto",
    "title": "Switching Over to Quarto (And Why You Should Consider Too)",
    "section": "Disadvantages of Quarto",
    "text": "Disadvantages of Quarto\nReally, the main one is lack of customization. Hugo is far more complex, and can create some sophisticated websites to suit any need. Quarto, not so much. This does make blogging on this platform feel slightly generic in comparison, unfortunately. Optimized Hugo sites might also be faster, though admittedly, I have no idea if this is likely / true or not.\nWith that said, the pros outweigh the cons in my case, which is why I decided to make the jump. I hope you enjoy the new look."
  },
  {
    "objectID": "blog/2023/sipri-dashboard-announcement/index.html",
    "href": "blog/2023/sipri-dashboard-announcement/index.html",
    "title": "SIPRI Dashboard Announcement",
    "section": "",
    "text": "I found it surprisingly hard to find a website that displayed military spending around the world in an easy and concise format, so I decided to make my own. Make sure to check it out here.\n\n\n\n\nPicture of Dashboard"
  },
  {
    "objectID": "blog/2023/pcaob-part-2-enforcement/index.html",
    "href": "blog/2023/pcaob-part-2-enforcement/index.html",
    "title": "Analyzing 20 Years of PCAOB Data: Part 2 (Enforcement)",
    "section": "",
    "text": "If you haven’t read part one, go read that first.\nAccording to Section 1.04 of Sarbanes-Oxley, the PCAOB has the ability to sanction firms/auditors as follows.\nSanctions can include censure, suspension from the accounting industry, limitation of financial activities, required additional education or training, and/or a monetary penalty. Fines are broken down into two categories; regular fines, which can range from;\nParagraph (5) refers to more serious violations, and are characterized by;\nThe PCAOB serves under the SEC, which means the Federal Civil Penalties Inflation Adjustment Improvements Act of 2015 also applies to the board. This Act adjusts monetary fines for inflation, which means maximum penalties are actually about 40% higher than the amounts listed above."
  },
  {
    "objectID": "blog/2023/pcaob-part-2-enforcement/index.html#results",
    "href": "blog/2023/pcaob-part-2-enforcement/index.html#results",
    "title": "Analyzing 20 Years of PCAOB Data: Part 2 (Enforcement)",
    "section": "Results",
    "text": "Results\nI’ve broken the tables down between domestic and international sanctions.\n\nDomestic Sanctions\ntable\n\n\nInternational Sanctions\ntable\n\nIf you remember from our first section, the PCAOB issues annual investigation reports for each of the largest firms, and one would expect that firms who perform worse on these investigations would be more likely to be fined.\nA quick glance at the tables shows this is almost entirely not the case. Despite both KPMG and BDO performing the worse on inspection reports among their domestic affiliates, they’ve been sanctioned the least among the eight largest domestic firms. In fact, neither domestic firm has been fined a single time in PCAOB history.\nOutside of performance, we’d expect enforcement to be skewed towards the largest firms due to their size. If the largest firms are auditing most the companies, it would be logical to think they’d also get most of the enforcement. The PCAOB seemingly has other ideas. Grant Thornton, who recorded a 1/6 of the revenue of KPMG in 2022, has been fined $2,250,000 by the board, the second most of any domestic firm - and all while having a much lower deficiency rate of ___ compared to ___ of KPMG.\nIn total, while the eight largest firms have been sanctioned 8 times for $8,765,000, everyone else has been sanctioned 193 times for $2,767,500. So while larger firms receive harsher fines, the PCAOB is clearly focused on smaller firms. Specifically, 70 of those sanctions were against sole proprietors, with most cases involving penny stocks.\nSanctions against international affiliates looks much different comparatively. Deloitte and KPMG have been absolutely hammered on the international stage, though most of these fines are the result of single incidents. In 2016, Deloitte was fined $8 million regarding its audit work of the Brazilian GOL airlines that also included 12 auditors receiving sanctions. Recently, on 12/06/22, the PCAOB went on a rampage against KPMG, sanctioning four international affiliates for a total of $6.8 million.\nIt’s interesting that the PCAOB would target international affiliates so aggressively considering that they are much less important than their domestic counterparts. It could be argued that it’s because the international affiliates perform worse, but that logic doesn’t seem to apply to KPMG and BDO at all. As a result, it’s hard to look at PCAOB enforcement as anything other than arbitrary and random.\nMore fundamentally, if the board can fine firms over $15 million for “repeated instances of negligent conduct”, why are the amounts so low given such high deficiency rates? If BDO can fail almost half of their audit inspections and not be subjected to a single fine, or if KPMG can fail a third of their audits while also trying to cheat PCAOB investigations which eventually gets them sanctioned by the SEC for $50 million - what the hell does “repeated instances of negligent conduct” really mean?"
  },
  {
    "objectID": "blog/2023/pcaob-part-2-enforcement/index.html#enforcement-through-time",
    "href": "blog/2023/pcaob-part-2-enforcement/index.html#enforcement-through-time",
    "title": "Analyzing 20 Years of PCAOB Data: Part 2 (Enforcement)",
    "section": "Enforcement Through Time",
    "text": "Enforcement Through Time\n\n\nShow code\nenforcement |>\n  dplyr::mutate(date = lubridate::mdy(date)) |>\n  dplyr::mutate(date = substring(date, 1, 4)) |>\n  dplyr::mutate(date = factor(date)) |>\n  dplyr::group_by(date, location) |>\n  summarise(total = n()) |>\n  mutate(date = as.numeric(as.character(date))) |>\n  ggplot() + \n  geom_col(\n    aes(x = date, y = total, fill = location),\n    width = 1,\n    color = \"#212124\"\n  ) + \n  geom_text(\n    aes(x = date , y = total, label = total, group = location),\n    position = position_stack(vjust = .56),\n    color = \"#fcfcff\",\n    family = \"1955\",\n    fontface = \"bold\", \n    size = 5.25\n  ) + \n  annotate(\n    \"segment\",\n    x = 2014.5,\n    xend = 2017.5,\n    y = 80,\n    yend = 80,\n    color = \"#44444a\",\n    size = 1.1,\n    alpha = .8\n  ) + \n  annotate(\n    \"segment\",\n    x = 2014.5,\n    xend = 2014.5,\n    y = 80,\n    yend = 79.5,\n    color = \"#44444a\",\n    size = 1.1,\n    alpha = .8\n  ) + \n  annotate(\n    \"segment\",\n    x = 2017.5,\n    xend = 2017.5,\n    y = 80,\n    yend = 79.5,\n    color = \"#44444a\",\n    size = 1.1,\n    alpha = .8\n  ) + \n  annotate(\n    \"segment\",\n    x = 2010.3,\n    xend = 2014.3,\n    y = 62,\n    yend = 78,\n    linetype = \"dotted\", \n    color = \"#44444a\",\n    size = 0.7\n  ) + \n  annotate(\n    \"text\",\n    x = 2010,\n    y = 58.5,\n    size = 3.2,\n    color = \"#212124\",\n    family = \"1955\",\n    label = \"40.7% of Enforcements\"\n  ) +\n    annotate(\n    \"text\",\n    x = 2010,\n    y = 54.5,\n    size = 3.2,\n    color = \"#212124\",\n    family = \"1955\",\n    label = \"Occured During 2015-2017\"\n  ) +\n  scale_x_continuous(\n    expand = c(0, 0),\n    breaks = seq(2005, 2022, by = 1)\n  ) + \n  scale_y_continuous(\n    expand = c(0, 0),\n    limits = c(0, 85)\n  ) + \n  labs(\n    x = \"\", y = \"\",\n    title=\"Number of PCAOB Sanctions by Year\",\n    subtitle = \"The number of PCAOB enforcement actions has been very inconsistent from year to year, with their first sanction given three years after the board's <br> creation.  The PCAOB sanctioned 138 firms/auditors from 2005-2014, but then issued **210** in the next three years *alone*. <br><br> Recent years have seen an increase in enforcement actions against international firms/auditors as the board continues to increase the number of its <br>cooperative arrangements with other countries. In total, <b style='color:#081c3c'>domestic</b> firms have received **377** sanctions, with the remaining **143** being against <br> <b style='color:#164d64'>international</b> firms.\",\n    caption=\"<br>**Data**: *PCAOB*<br>Data from 05/24/05 - 12/06/22\"\n  ) + \n  scale_fill_manual(values = c(\"#06142a\", \"#247ba0\")) + \n  theme_minimal() + \n  theme(\n    axis.text = element_text(\n      family = \"1955\",\n      color = \"#3c3c3c\"\n    ),\n    axis.text.x = element_text(\n      size = 6,\n      face = \"bold\"\n    ),\n    axis.text.y = element_text(\n      size = 8\n    ),\n    axis.title = element_text(\n      family = \"1955\",\n      color = \"#1e1e1e\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(\n      size = 14\n    ),\n    axis.ticks = element_line(\n      color = \"#3c3c3c\",\n      size = .6\n    ),\n    panel.grid.major.x = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    plot.background = element_rect(\n      fill = \"#fcfcff\",\n      color = NA\n    ),\n    panel.background = element_rect(\n      fill = \"#fcfcff\",\n      color = NA\n    ),\n    plot.title = element_markdown(\n      family = \"1955\",\n      face = \"bold\",\n      color = \"#212124\",\n      size = 27\n    ),\n    plot.subtitle = element_markdown(\n      family = \"1955\",\n      face = \"plain\",\n      color = \"#212124\",\n      size = 12.5\n    ),\n    plot.caption = element_markdown(\n      family = \"1955\",\n      face = \"plain\",\n      color = \"#6c757d\",\n      size = 6.75\n    ),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\",\n    legend.position = c(0.1, 0.45),\n    legend.title = element_markdown(\n      color = \"#fcfcff\"\n    ),\n    legend.text = element_markdown(\n      family = \"1955\",\n      size = 10\n    )\n  )\n\n\n\n\n\n\n\n\n\nWell, enforcement is a lot less random when we view things through the political. Without getting too much into the details, in 2010, a case titled Free Enterprise Fund vs. PCAOB was brought to the Supreme Court that challenged the constitutionality of the board. While it did not pass, the Supreme Court did rule that the provision that PCAOB board members could only be removed “for just cause only” was unconstitutional and struct it down. What this means in practical terms is that once a new administration is in office, they’ll usually remove all the board members and hire a new one, and each iteration of the board clearly has its own interpretation of enforcement.\nJames Doty was PCAOB Chairman from 2011 - 2017, and during his last three years, he decided to go on a tear, sanctioning over 200 firms/auditors, which makes up over 40% of all PCAOB sanctions. In February 2018, an entire new board was elected, and sanctions fell by about half the following years. In a PCAOB Webinar hosted for audit committee members, newly elected Chairman Bill Duhnke stated that the board had “discovered from feedback” that;\n\n“The PCAOB wasn’t necessarily playing well with others, that it was rarely receptive to feedback from stakeholders […] Consequently we’ve been actively trying to change that […]\n– Bill Duhnke, quote at 1:38\n\nThe Wall Street Journal would report in October 2019 that the board had “slowed its work amid board infighting, multiple senior staff departures, and allegations that the chairman has created a ‘sense of fear’ according to a [May 2019] whistle-blower letter and people familiar with the situation.” Dunkhe would later be investigated by the SEC for his handling of internal complaints and employee harassment, which eventually led Dunkhe and the entire board getting canned in 2021 (though that didn’t stop him from returning to Capitol Hill as a Senate Aide).\nUnder the new leadership of Chairman Erica Williams, the total number of sanctions has stayed mostly the same, though they do differ in a unique way. Prior to 2015, only 35.51% of sanctions involved a monetary fine; under Dunkhe, that number was to 65.74%. Under Williams, all sanctions have involved a monetary penalty outside of two incidents. In both those reports, they state they “would have imposed a civil money penalty of $25,000” had they not “taken [their] financial resources into consideration.”\nI’d imagine in 2024, if a Republican administration is in control of the White House, there is an extremely high likelihood that they will announce a brand new board, just like past administrations have done. With that will come a new and slightly different enforcement policy."
  },
  {
    "objectID": "blog/2023/covid-19-stimulus-database-part-1/index.html",
    "href": "blog/2023/covid-19-stimulus-database-part-1/index.html",
    "title": "Building a COVID-19 Economic Relief Database: Part 1 (usaspending.gov)",
    "section": "",
    "text": "Congress has awarded over $4 trillion in loans, grants, contracts, or other financial assistance as part of over 70 various COVID-19 related economic relief programs. Currently, there isn’t an easy way to view that data where all that money is going, but I want to change that by building an easily accessible database that makes viewing total and outlay amounts simple.\nThere are two primary ways we can go about this.\nThe first is to gather the data manually. Most agencies will let you download records from their websites in .csv format, but others are much more difficult, sometimes offering data in PDFs only. Some agencies don’t provide updates on their award data, and there’s often inconsistencies in how data is stored between agencies. This means tracking outlay amounts are impossible. While this method would give us much more control over how the data is inputted, it would ultimately be too time consuming to be realistic.\nThe other way is to use the “curated” database from usaspending.gov. You can find it here with accompanying instructions here."
  },
  {
    "objectID": "blog/2023/covid-19-stimulus-database-part-1/index.html#download-center",
    "href": "blog/2023/covid-19-stimulus-database-part-1/index.html#download-center",
    "title": "Building a COVID-19 Economic Relief Database: Part 1 (usaspending.gov)",
    "section": "Download Center",
    "text": "Download Center\nHere is a screenshot of the instructions on how to get the data.\n\nThere are three different file types to choose from, but as the page describes, File C data “provides a higher degree of granularity in breaking down award spending by several financial data dimensions.” This is because calculating awards given by Congress isn’t as simple as looking at total face amounts, which I’ll explain more when constructing the database.\nIn total, the download is about 10GB compressed. Uncompressed, the data is spread across 139 Excel files that contain 1,000,000 rows. Excel can’t contain a spreadsheet with over a million rows, and .csv is already incredibly inefficient for analyzing such a large dataset, so we will need some special tools in order to properly analyze it.\nLike I said earlier, “curated” database my ass.\nNow, you might be thinking to yourself; if each Excel file contains a million rows, and there’s 139 of them, are their really 139,000,000 million loans/grants/contracts related to COVID-19? Not even close. Each row in our data doesn’t represent an individual recipient award, but an updated entry for that recipient award.\nSay I were to give you a loan for $10 million, and you were to take $2 million out of the bank. This would show up as two different entries in the database; one for the face loan amount, and one showing the extracted amount, or the award outlay. Now let’s say you pay $100,000 on interest on that loan. Then, it will show up as another entry in the database, but instead, for a negative amount.\nThis allows us to do two things. For one, we can track how much money has actually been given as opposed to how much has been outlayed. Furthermore, we can do things like calculate whether certain loans were forgiven or not, which is something you can actually already view using the ProPublica database. They got their data directly from the Small Business Administration, but because this database is collected from all other agencies, we can make our own.\nDifferent agencies record data slightly different, so information is not consistent across the database. Here’s the explanation given by usaspending.gov.\n\nIn order to understand the data surfaced in the “Award Spending” sections (detailed below), it is important to understand the concept of linking between Broker File C and FPDS/FABS award data. Broker File C serves as a bridge between data sourced from agency financial systems (i.e., the data in Broker File C itself) and award data sourced from FPDS and FABS. The actual link between these two datasets is an award ID (also known as award unique key). For various reasons, not every award ID in Broker File C has a corresponding award ID in FPDS or FABS data, which makes them unmatchable. If a Broker File C row cannot be matched to FPDS or FABS, we call it “unlinked”. Unlinked Broker File C data cannot be supplemented by metadata from FPDS or FABS (including recipient information, CFDA program, and funding agency).\nThe rule of thumb for all award sections is to use complete Broker File C data where possible (containing both linked and unlinked awards); where not possible, only linked data will be used (representing a subset of the authoritative award spending total based on both linked and unlinked data in Broker File C).\n\nWhen we analyze the data, we can calculate the number of NAs to see where we are missing most of our information, but basically, it’s not all there.\n\nA Brief Demo\nThere are some special instructions to make sure that we get the right calculations when crunching the dataset.\n\nBasically, we group by the column, award_unique_key, and use dplyr::summarise to sum obligation and outlay amounts.\nLet’s load in one of the files to test this out on. We’ll count the number of entries for each award_unique_key and sum the obligation amount.\n\n\nShow code\n# Reading in the data using Arrow; explained later \ntest <- arrow::read_csv_arrow(\"test.csv\")\n\ntest %>% \n  group_by(award_unique_key) %>%\n  summarise(\n    count = n(),\n    total = sum(transaction_obligated_amount)\n  ) %>%\n  arrange(desc(count))\n\n\nHere are the results printed to console.\n\nFor reference, there are close to 400,000 rows in the file, but when grouping by award_unique_key, we see that there are only 160,000 unique award keys in our dataset. One of the recipients has over 200 entries for award obligations totaling $110,309,034. For fun, let’s see who the recipient is.\n\n\nShow code\ntest %>%\n  filter(award_unique_key == \"ASST_NON_4488DRNJP00000001_7022\") %>% \n  group_by(award_unique_key) %>%\n  summarise(\n    Recipient = unique(recipient_parent_name),\n    State = unique(recipient_state),\n    Award_Category = unique(award_type),\n    Obligations = sum(transaction_obligated_amount),\n    Agency = unique(awarding_agency_name),\n    Description = unique(prime_award_base_transaction_description),\n    Last_Modified = unique(award_latest_action_date)\n  )\n\n\nThis results in a 1x1 tibble.\n\n\n\nSo, the recipient is to the local township of Berkeley Heights, New Jersey, and the award type was a grant. The award is from the Department of Homeland Security, specifically FEMA, for the purpose of the “repair or replacement of disaster damaged facilities.” The original award was handed out on April 8th, 2020, but as we can see, the city is still receiving payments as of recently.\nRemember, these amounts were compiled for only one file, but there are 139 of them in total. There are entries scattered across other files, which means there’s no guarantee that Berkeley Heights didn’t receive more in aid in another file. What we need to do is combine all these files together before doing any sort of computations on them.\nThis is a problem, though. The way R loads data is by storing it into RAM, which means that in order to perform computations on the entire dataset, we’d need about 140GB of RAM. If you’re not sure how much RAM your computer has, there’s a good chance it’s between 4GB - 16GB, and high end computers generally cap out at 64GB. My laptop has 16GB. In other words, even the most powerful commercial computers wouldn’t be able to load the data using base R.\nIs this the end? Well, luckily, there are plenty of big data tools that make it possible to load and perform data analysis on massive data sets without the need of external support. While there are many available, I’ll be opting for Apache Arrow, which will be the focus of my next post. Peace."
  },
  {
    "objectID": "blog/2023/pcaob-part-2-enforcement/index.html#small-fries",
    "href": "blog/2023/pcaob-part-2-enforcement/index.html#small-fries",
    "title": "Analyzing 20 Years of PCAOB Data: Part 2 (Enforcement)",
    "section": "Small Fries",
    "text": "Small Fries\n\nchart -\n\nKind of. The PCAOB still mostly loves picking on small audit firms, especially sole proprietors. Granted, these are not small mom-and-pop shops; to be able to audit a publicly traded company all by yourself requires some reputation. A lot of these cases involve penny stocks in which the auditors - either directly or indirectly - had something to gain from the company’s valuation. Over the last few years, the number of penny stocks on the market has risen significantly, so we can expect more auditors to be fined in the future.\nTo return to an earlier question; why is the PCAOB focused so much more on smaller firms, compared to multinational firms who have a massive impact on the global economy? There’s a whole bunch of reasons why, but one I’d like to draw attention to are budgetary restrictions."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "My posts, charts, and data are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\n  \n\nYou are free to:\n\nShare: copy and redistribute the material in any medium or format.\nAdapt: remix, transform, and build upon the material for any purpose, even commercially.\n\nUnder the following terms:\n\nAttribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\nShareAlike: If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original."
  },
  {
    "objectID": "notes/echarts4r/index.html#a.-bar-chart",
    "href": "notes/echarts4r/index.html#a.-bar-chart",
    "title": "echarts4r Cookbook",
    "section": "2a. Bar Chart",
    "text": "2a. Bar Chart\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_legend(show = FALSE) |>\n  e_axis_labels(\n    x = \"Island\",\n    y = \"Averge Mass\"\n  ) |>\n  e_title(\n    text = \"Some meaningful title.\",\n    subtext = \"Plus some more useful info.\"\n  ) |>\n  e_tooltip(axis = \"trigger\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#a.-stable-install",
    "href": "notes/echarts4r/index.html#a.-stable-install",
    "title": "echarts4r Cookbook",
    "section": "1.A. Stable Install",
    "text": "1.A. Stable Install\nGithub Page.\n\ninstall.packages(\"echarts4r\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#b.-developmental-build",
    "href": "notes/echarts4r/index.html#b.-developmental-build",
    "title": "echarts4r Cookbook",
    "section": "1.B. Developmental Build",
    "text": "1.B. Developmental Build\nRequites the remotes package to install.\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"JohnCoene/echarts4r\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#c.-complimentary-packages",
    "href": "notes/echarts4r/index.html#c.-complimentary-packages",
    "title": "echarts4r Cookbook",
    "section": "1.C. Complimentary Packages",
    "text": "1.C. Complimentary Packages\nNeither of these packages have been updated in years, but are still usable in their current formats. They allow for maps and image assets to be used while charting.\n\nremotes::install_github('JohnCoene/echarts4r.assets')\nremotes::install_github('JohnCoene/echarts4r.maps')"
  },
  {
    "objectID": "notes/echarts4r/index.html#d.-loading-packages",
    "href": "notes/echarts4r/index.html#d.-loading-packages",
    "title": "echarts4r Cookbook",
    "section": "1.D. Loading Packages",
    "text": "1.D. Loading Packages\nAlso loading dplyr for data manipulation and palmerpenguins for dummy data.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(palmerpenguins)\nlibrary(echarts4r)\nlibrary(echarts4r.maps)\n\ne_common(\n  font_family = \"Georgia\",\n  theme = \"dark-mushroom\"\n)"
  },
  {
    "objectID": "notes/echarts4r/index.html#a.-column-chart",
    "href": "notes/echarts4r/index.html#a.-column-chart",
    "title": "echarts4r Cookbook",
    "section": "2.A. Column Chart",
    "text": "2.A. Column Chart\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_legend(show = FALSE) |>\n  e_axis_labels(\n    x = \"Island\",\n    y = \"Averge Mass\"\n  ) |>\n  e_title(\n    text = \"Some meaningful title.\"\n  ) |>\n  e_tooltip(trigger = \" axis\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#b.-bar-chart",
    "href": "notes/echarts4r/index.html#b.-bar-chart",
    "title": "echarts4r Cookbook",
    "section": "2.B. Bar Chart",
    "text": "2.B. Bar Chart\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_flip_coords() |> # THIS LINE OF CODE # \n  e_tooltip()"
  },
  {
    "objectID": "notes/echarts4r/index.html#c.-scatter-chart",
    "href": "notes/echarts4r/index.html#c.-scatter-chart",
    "title": "echarts4r Cookbook",
    "section": "2.C. Scatter Chart",
    "text": "2.C. Scatter Chart\n\npenguins |>\n  group_by(species) |>\n  e_chart(bill_length_mm) |>\n  e_scatter(bill_depth_mm) |>\n  e_tooltip()"
  },
  {
    "objectID": "projects/sipri-dashboard/index.html",
    "href": "projects/sipri-dashboard/index.html",
    "title": "🪖 SIPRI Dashboard",
    "section": "",
    "text": "Launch  Source"
  },
  {
    "objectID": "archives.html",
    "href": "archives.html",
    "title": "Nathan States",
    "section": "",
    "text": "Archives\n🚧 UNDER CONSTRUCTION 🚧\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html",
    "href": "blog/2023/bengaluru-text-mining/index.html",
    "title": "Bengaluru Text Mining",
    "section": "",
    "text": "Aerial view of Bengaluru, India."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#background",
    "href": "blog/2023/bengaluru-text-mining/index.html#background",
    "title": "Bengaluru Text Mining",
    "section": "Background",
    "text": "Background\nText data can be difficult to analyze in large amounts, but raw text is invaluable in numerous different ways. Using simple Python libraries, modern machine learning models can parse thousands of rows in seconds, which can be used for a variety of purposes. One of the most common of these is classification, or categorizing text into different groups.\nThe Bruhat Bengaluru Mahangara Palike (BBMP) - an administrative body that oversees city development in Bengaluru, the largest tech city in India - created a web application that allows citizens to file grievances with the city. From February 8th, 2020 to February 21st, 2021, a total of 105,956 complaints were filed to BBMP, or about 280 a day. Exploring this data not only provides insight into the most common problems facing this city (or at least the complaints most likely to be sent), but also presents an opportunity to quantify and categorize them.\nIn the dataset, complaints have been manually categorized by the administrators who oversee the app at BBMP, but this is extremely inefficient. Usefully, though, the developers have already created categories that they felt best sorted the data, which means, assuming complaints don’t change substantially in the future, we can train a machine learning model that performs this task automatically. This could save hours and hours of time.\nBecause the categories are already defined, this will be a supervised classification model."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#data-wrangling",
    "href": "blog/2023/bengaluru-text-mining/index.html#data-wrangling",
    "title": "Bengaluru Text Mining",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nData wrangling and EDA done using R.\nFirst, we import the data and the libraries we will be using.\n\n\nShow code\n# Load Libraries\nlibrary(tidyverse) \nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(echarts4r)\nlibrary(here)\nlibrary(lubridate)\nlibrary(wordcloud2)\nlibrary(reticulate)\n\n# Set Directory \nhere::set_here()\n\n# Import Data\ngrievances <- readr::read_csv(\"bengaluru-grievances.csv\")\n\n\nThe admins at BBMP keep their data neat and tidy, so there’s not many problems to fix. There are a couple things to consider, however.\nThe first issue is that some descriptions are extremely short, making classifying them accurately near impossible. We can limit the number of characters that a complaint must have, though the appropriate number of rows to remove is debatable. Ideally, we don’t want to exclude too much of the data while still removing descriptions too short to be properly categorized.\nUsing str_length from the stringr package, we see 5,392 complaints contained fewer than 12 characters, meaning removing them would still preserve 95% of the original data. Using filter from dplyr, we can keep all complaints containing more than 12 characters.\n\n\nShow code\n# Counting number of rows \ngrievances %>%\n  filter(str_length(description) < 12) %>%\n  count() \n\n\n# A tibble: 1 x 1\n      n\n  <int>\n1  5392\n\n\n\n\nShow code\n# Removing those rows \ngrievances <- grievances %>%\n  filter(str_length(description) > 12)\n\n\nThe other consideration is whether the existing categories accurately reflect the data or not. It’s possible certain similarities between different categories would better be combined into one, and likewise, single categories that should be multiple ones. These changes might not only provide a better description of the data, but improve accuracy in the long run.\nFor now, we will leave the original categories intact and proceed, but future models may benefit from this step."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#exploratory-data-analysis",
    "href": "blog/2023/bengaluru-text-mining/index.html#exploratory-data-analysis",
    "title": "Bengaluru Text Mining",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nInteractive charts created using echarts4r.\n\nNumber of Grievances By Day\n\n\nShow code\n# Set Theme \ne_common(\n  font_family = \"Georgia\"\n)\n\n# Chart \ngrievances %>%\n  group_by(created_at = as.Date(created_at)) %>%\n  summarise(Total = n()) %>%\n  e_charts(created_at) %>%\n  e_line(Total, symbol = \"none\") %>%\n  e_x_axis(axisLabel = list(interval = 0)) %>%\n  e_title(\n    text = \"Total number of grievances by day\",\n    subtext = \"Data: BBMP\"\n  ) %>%\n  e_color(\n    \"#0a32d2\", \n    background = \"rgb(0, 0, 0, 0)\"\n  ) %>%\n  e_legend(show = FALSE) %>%\n  e_tooltip(\n    backgroundColor = \"rgba(20, 20, 20, 0.5)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    textStyle = list(\n      color = \"#ffffff\"\n    ),\n    trigger = \"axis\"\n  )\n\n\n\n\n\n\nOn most days, grievances would vary between 200 to 400 a day, with some spikes in September onwards, including a massive single-day one in March of 742. On average, 280 complaints were filed each day.\nBecause complaints don’t seem to fluctuate significantly, we can likely assume that the day the grievance was filed isn’t indicative of its contents.\n\n\nGrievances by Category\n\n\nShow code\ngrievances %>%\n  group_by(category) %>%\n  summarise(Total = n()) %>%\n  arrange(desc(Total)) %>%\n  filter(Total > 1000) %>%\n  slice(1:10) %>%\n  e_charts(category) %>%\n  e_bar(Total) %>%\n  e_x_axis(\n    axisLabel = list(\n      interval = 0, \n      rotate = 45,\n      fontSize = 9.25\n    )\n  ) %>%\n  e_title(\n    text = \"Most common grievances by category\", \n    subtext = \"Only categories above 1,000 visible\"\n  ) %>%\n  e_legend(show = FALSE) %>%\n  e_labels(show = FALSE) %>%\n  e_color(\n    \"#8c5ac8\", \n    background = \"rgb(0, 0, 0, 0)\"\n  ) %>%\n  e_tooltip(\n    trigger = \"axis\",\n    backgroundColor = \"rgba(20, 20, 20, 0.5)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    textStyle = list(\n      color = \"#ffffff\"\n    )\n  )\n\n\n\n\n\n\n81.89% of total grievances were categorized as electrical, solid waste or garbage related, or road maintenance. The next three largest categories make up an additional 11.09%, meaning the remaining categories have less than 1,350 occurrences combined.\nNote that this is not due to a lack of categories; in fact, there are a total of 20 categories in the data.\n\n\nShow code\nlength(unique(grievances$category))\n\n\n[1] 20\n\n\nLarge imbalances like this are important to consider when building machine learning models. Algorithms are specifically programmed to achieve the highest accuracy regardless of original purposes, and they tend to overestimate larger categories. If a model discovers it can restrict itself to three options while still recording 80%+ accuracy, it will almost always do so.\nThis means, though, machine learning models will tend to ignore minor categories, because - using our current data as an example - predicting a category outside the top three has an inherent 81.89% fail rate, so this will need to be addressed when creating the models.\nThere is a question as to whether certain smaller categories should exist at all, though.\n\n\nShow code\ngrievances %>%\n  group_by(category) %>%\n  summarise(total = n()) %>%\n  filter(total < 100)\n\n\n# A tibble: 5 x 2\n  category                   total\n  <chr>                      <int>\n1 Education                     20\n2 Estate                        75\n3 Markets                       38\n4 Optical Fiber Cables (OFC)    62\n5 Welfare Schemes               28\n\n\nFive categories have less than 100 complaints total, including two which have less than thirty. This is far too few complaints to reliably build a model with, especially considering we haven’t split the data yet.\n\n\nGrievances by Subcategory\n\n\nShow code\ngrievances %>%\n  group_by(subcategory) %>%\n  summarise(Total = n()) %>%\n  arrange(Total) %>%\n  filter(Total > 1000) %>%\n  e_charts(subcategory) %>%\n  e_bar(Total) %>%\n  e_legend(show = FALSE) %>%\n  e_title(\n    text = \"Most common grievances by subcategory\", \n    subtext = \"Only categories above 1,000 visible\"\n    ) %>%\n  e_color(\"#8c5ac8\", background = \"rgb(0,0,0,0)\") %>%\n  e_flip_coords() %>%\n  e_tooltip(\n    trigger = \"axis\",\n    backgroundColor = \"rgba(20, 20, 20, 0.5)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    textStyle = list(\n      color = \"#ffffff\"\n    )\n  )\n\n\n\n\n\n\nWhile the model being built is only to predict the main categories, by viewing subcategories, we see over 35% of total complaints were related specifically to street lights not working, comprising almost all of the electrical category. Solid waste related problems is divided into two subcategories; garbage vehicle not arriving, and “Garbage dump” (whatever that means). Meanwhile, road maintenance has been divided into three subcategories, those being potholes, road side drains, and debris removal.\n\n\nMost Common Complaint Words\nWe first have to get the 50 most common words…\n\n\nShow code\ntidy <- grievances %>%\n  unnest_tokens(word, description) %>%\n  anti_join(get_stopwords()) %>%\n  count(word) %>%\n  arrange(desc(n)) %>%\n  slice(1:50)\n\n\nThen chart them using wordcloud2.\n\n\nShow code\nwordcloud2(\n  tidy,\n  color = rep_len(c(\"#8c5ac8\", \"#0b0d21\", \"#0a32d2\"), nrow(tidy)),\n  backgroundColor = \"#fcfcfc\"\n)\n\n\n\n\n\n\nSeeing as over a third of the data was subcategorized as street lights not working, we see that they are the most common words across all the complaints.\nMissing from that is the word not, but this is because we removed all stop words from the data. Put simply, stop words are words that don’t add anything to the process of categorizing text. They usually include words like he, she, there, they, I, and so on, which is why they don’t appear in the chart.\nThe stop words included in the tidytext package (and the ones used in the function above) are meant to apply universally, but if you look closely at the word cloud, there are several words that almost certainly don’t apply to our problem. These include terms like please, kindly, last request, sir, and individual numbers like 1, 2, and 3. While there might exist some incidental correlation between some of these words and their respective categories (perhaps citizens filing animal control complaints are nicer on average, so the term please could be used to identify those complaints more accurately, for example), it’s likely this will just throw off our model’s accuracy in the long run.\n\nFrom the EDA, the primary factors that should be considered when building the models are:\n\nAccount for imbalances in number of occurrences per category.\nReduce the number of categories by combining them into existing ones.\nAdd additional stop words."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#mle-models",
    "href": "blog/2023/bengaluru-text-mining/index.html#mle-models",
    "title": "Bengaluru Text Mining",
    "section": "MLE Models",
    "text": "MLE Models\n\nPreparing the Data\nWe’ll opt to use Python for creating the MLE models, as Python libraries are generally more efficient and developed than their R counterparts. Because creating models is computer intensive, the code here has been evaluated locally and presented here for demonstration.\nTo start, we’ll import the necessary libraries and load the data using pandas. The models here will be built using sklearn.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns  \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n\ndf = pd.read_csv(\"bengaluru-grievances.csv\")\n\n\nOne of the packages imported here is TfidfVectorizer, which will be the algorithm used to create the models. I’ll explain why I specifically chose this package later on.\nHere, I quickly apply the earlier data wrangling techniques by removing complaints less than 12 characters, this time using Python syntax.\n\n\nShow code\n# Reduce Character Limit  \ncharacter_limit = (df[\"description\"].str.len() > 12)\ndf = df.loc[character_limit]\n\n\nA model based on over 100,000 observations is extremely hardware intensive, and it causes my laptop to overheat a lot. For practical purposes, we’ll cut down on our data by choosing 15,000 rows at random.\n\n\nShow code\ndf = df.sample(15000, random_state = 1).copy()\n\n\nIf we wanted to get another random 15,000 rows, we could change the random_state = 1 argument to any other number, like 42, 671, or 7.\nWith no other changes to be made, we can begin creating the models.\n\n\nText Preprocessing\nThere are several methods to building models, but the simplest method is to create a new column in our data - we’ll call it category_id - that is a factor variable of all existing categories in our data. This essentially amounts to assigning each category a number (Electrical = 0, Road Engineering = 1, etc), which is necessary for getting our model to run properly, as sklearn will not understand strings as factors.\n\n\nShow code\ndf['category_id'] = df[\"category\"].factorize()[0]\ncategory_id_df = df[[\"category\", \"category_id\"]].drop_duplicates()\n\n\nNext, the description column (which stores the text for grievances) needs to be converted to vectors using a chosen algorithm. The algorithm chosen here is Term Frequency - Inverse Document Frequency (TF-IDF), which is the product of \\(TF\\) and \\(IDF\\) scores. This is the TfidfVectorizer function that we imported earlier.\nIt’s useful to present these terms mathematically.\n\nTerm Frequency: \\[ TF = \\frac{Number \\hspace{0.15cm} of \\hspace{0.15cm} times \\hspace{0.15cm} a \\hspace{0.15cm} term \\hspace{0.15cm} appears \\hspace{0.15cm} in \\hspace{0.15cm} the \\hspace{0.15cm} description}{Total \\hspace{0.15cm} number \\hspace{0.15cm} of \\hspace{0.15cm} words \\hspace{0.15cm} in \\hspace{0.15cm} the \\hspace{0.15cm} description} \\]\nInverse Document Frequency: \\[ IDF = log(\\frac{Number \\hspace{0.15cm} of \\hspace{0.15cm} rows \\hspace{0.15cm} in \\hspace{0.15cm} a \\hspace{0.15cm} data}{Number \\hspace{0.15cm} of \\hspace{0.15cm} rows \\hspace{0.15cm} a \\hspace{0.15cm} term \\hspace{0.15cm} appears \\hspace{0.15cm} in \\hspace{0.15cm} a \\hspace{0.15cm} data}) \\]\nTF-IDF: \\[ TF-IDF = TF * IDF \\]\n\nThe reason for choosing this algorithm was for the \\(IDF\\) component, which downsamples words that appear frequently across all complaints, while adding extra weight to terms that appear less often.\nAnalyzing it mathematically: as the denominator of the \\(IDF\\) variable increases, the closer it rapidly (more precisely, exponentially) approaches zero. Let \\(N\\) represent the total number of rows in the data, and let \\(t\\) represent a chosen term. If a certain word were to appear in every single complaint, then we would have \\(IDF(N, t) = log(\\frac{N}{t}) = log(\\frac{N}{N}) = log(1) = 0\\), which would mean that when calculating \\(TF-IDF\\), that specific word would have absolutely no weight attached to it when classifying complaints.\nNow; why do this? As discussed previously during the EDA section, almost 82% of all complaints fell into exactly three categories. Classification models will tend to stick to only a few categories, struggling to identify minor categories. While this may record higher accuracy scores on average, doing so means minor categories will rarely be classified at all, and in some instances, could lower overall accuracy if skew is significant enough. By ranking terms on an exponentially decreasing scale, we hope to reduce this issue.\nWe first setup our TfidfVectorizer and assign it to a variable, tfidf.\n\n\nShow code\ntfidf = TfidfVectorizer(\n  sublinear_tf = True, # Set term frequency to logarithmic scale\n  min_df = 5, # Remove terms that appear less than 'min_df' times\n  ngram_range = (1, 2), # Unigrams and bigrams are considered \n  stop_words = 'english' # Use common English stop words\n)\n\n\nFrom here, we can begin building our models, but before doing so, let’s see what the most common terms were for each category.\nTo do so, we use tfidf.get_feature_names_out() on each category and assign that to a variable that we’ll call feature_names. This contains all of the most common words associated with each category, which we then split into two separate lists for unigrams and bigrams (fancy words for “one word” and “two words”). From there, we print to console the \\(N = 3\\) most common terms from each list. We wrap all this in a for loop, automatically progressing through each category.\n\n\nShow code\n# Defaults \nfeatures = tfidf.fit_transform(df.description).toarray()\nlabels = df.category_id\nN = 3\n\n# For Loop\nfor category, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names_out())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"\\n==> %s:\" %(Product)) # Space for formatting\n  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))\n\n\nAs this part was performed offline, here is the output in screenshots.\n\n\n\nThe results are largely what we would expect, though there are some things to note.\nSome common phrases that appear for certain categories seemingly have nothing to do with them, such as the terms “77”, “kindly”, and “plz look”, which is one of the most common bigrams for both “Education” and “Welfare Schemes.” Remember, these were the categories that had less than 100 observations total. When we split the data to grab 15,000 random rows, these categories were split even further, which is probably why these nonsense phrases appear.\n\n\nBuilding the Models\nTo begin, we first split the data into a 75:25 training and test split. The model will “learn” how to classify grievances based on the training data, and then it will “test” its accuracy on the remaining 25%.\n\n\nShow code\n# We define them here as independent variables \nX = df[\"description\"]\ny = df[\"category\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n\nThere are several different models to choose from, but it’s hard to know which will perform best before actually building them. That’s why we’ll test several models simultaneously by storing them in a list and looping through each model.\n\n\nShow code\nmodels = [\n    RandomForestClassifier(n_estimators = 100, max_depth = 5, random_state = 0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state = 0)    \n]\n\n\nHere, we stored in a list the following:\n\nRandom Forest Model\nLinear Support Vector Classifier Model\nMultinomial Naive Bayes Model\nLogistic Regression Model\n\nAfter, we apply each model to the training data and record the results. The accuracy of each model is inherently random, as model performance is somewhat due to chance, so we’ll use a five-fold cross-validation and take the mean average of each iteration to get a more balanced result. We store the results in a pandas dataframe for analysis.\n\n\nShow code\n# Copy and pasted from before \nfeatures = tfidf.fit_transform(df.description).toarray()\nlabels = df.category_id\n\nCV = 5 # Number of cross-validations\ncv_df = pd.DataFrame(index = range(CV * len(models))) # CV dataframe\nentries = [] # Array for storing model results \n\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\n    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n\n\n\n\nResults\nBecause we used a five-fold cross-validation, we have a total of 20 accuracy results - five for each model. We grab the mean accuracy and standard deviation for each model, storing them into a list.\n\n\nShow code\nmean_accuracy = cv_df.groupby('model_name').accuracy.mean()\nstd_accuracy = cv_df.groupby('model_name').accuracy.std()\n\naccuracy = pd.concat([mean_accuracy, std_accuracy], axis= 1, ignore_index=True)\naccuracy.columns = ['Mean Accuracy', 'Standard Deviation']\n\naccuracy\n\n\n\n\n\nModel\nMean Accuracy\nStandard Deviation\n\n\n\n\nLinear SVC\n88.773%\n0.368%\n\n\nLogistic Regression\n87.767%\n0.433%\n\n\nMultinomial NB\n85.720%\n0.117%\n\n\nRandom Forest\n66.213%\n1.411%\n\n\n\nThe top three models all performed similarly as well, all falling within 3.1% percentage points. The Linear Support Vector Classifier performed the best among the three, while the Random Forest performed atrociously. Standard deviation among the top three remained fairly low, but especially for multinomial naive bayes."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#improvements",
    "href": "blog/2023/bengaluru-text-mining/index.html#improvements",
    "title": "Bengaluru Text Mining",
    "section": "Improvements",
    "text": "Improvements\nWe will focus model improvement on the Linear SVC because it performed the best.\nAs a reminder, these were the three main considerations before going in.\n\nAccount for imbalances in number of occurrences per category.\nConsider reducing number of categories.\nConsider adding additional stopwords.\n\nTo get a better idea of how our model performed, we will plot a confusion matrix, which displays the total number of attempts our classification model made along with how many were accurately categorized.\n\n\nShow code\n# Recreating LinearSVC Model \nX_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, \n                                                               labels, \n                                                               df.index, test_size=0.25, \n                                                               random_state=1)\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Confusion Matrix Plot\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize = (8, 8))\nsns.heatmap(conf_mat, annot = True, cmap = \"Greens\", fmt = 'd',\n            xticklabels = category_id_df.category.values, \n            yticklabels = category_id_df.category.values)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix for LinearSVC \\n\", size = 20)\n\n\n\n\n\nOn the diagonal are the number of rows that the model correctly predicted for each category. The horizontals and verticals represent the number of incorrect guesses, with the vertical representing incorrect guesses for that specific category. For example, the model correctly classified 1,484 complaints as “Electrical,” incorrectly classified 2 as “Electrical” when they should of been classified as “COVID-19,” and classified 10 as “Road Maintenance” when they should of been classified as “Electrical.”\n\nModifications\nLooking at the chart, the top three categories dominate the total number of occurrences, comprising 2,969 rows out of 3,750 in our test data. Most of the incorrect predictions appear in the vertical of each of these three columns, meaning the model was incorrectly classifying complaints as them often. Even though we chose an algorithm to specifically downsample those categories, our model still has a tendency to over-predict them.\nA few categories have 12 or fewer observations: those being Markets, Estate, OFC, Welfare Schemes, Advertisement, Education, Town Planning, Lakes, and Parks and Playgrounds. Converting these will likely improve accuracy considering how poorly our model did at predicting them, but there isn’t clear category to merge them with. Many of these categories seem to have been falsely labeled as “Road Maintenance.” While converting these columns over to this might lead to higher accuracy, it doesn’t really make any sense in this case, and likely would hurt performance in the future.\nWe could reassign these variables to “Others,” but that category performed abysmally, only correctly predicting 3 out of 43 complaints. On one hand, moving them there probably won’t hurt, but it likely won’t improve “Others” result either.\nLakes and Advertisements, which the model predicted quite a few correctly, will be left untouched for now. For the remaining categories under 12 test observations, they will be merged in with Others.\n\n\nShow code\n# Read in Data | Copy and Paste from Above\ndf = pd.read_csv(\"bengaluru-grievances.csv\")\npd.DataFrame(df.category.unique()).values \ncharacter_limit = (df[\"description\"].str.len() > 12)\ndf = df.loc[character_limit]\ndf = df.sample(15000, random_state = 2).copy() # Select New Rows \n\n# Convert Columns \ndf[\"category\"] = df[\"category\"].replace({'Markets': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Estate': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Welfare Schemes': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Education': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Town Planning': 'Others'})\n\n\n“Optical Fiber Cables” and “Storm Water Drains,” though, are directly related to “Road Maintenance,” and if we look at the chart, that’s what the model ended up incorrectly guessing the most of. For these categories, it makes more sense to convert them over to “Road Maintenance” as opposed to “Others.”\n\n\nShow code\ndf[\"category\"] = df[\"category\"].replace({'Optical Fiber Cables (OFC)': 'Road Maintenance(Engg)'})\ndf[\"category\"] = df[\"category\"].replace({'Storm  Water Drain(SWD)': 'Road Maintenance(Engg)'})\n\n\nWhile we’ve converted the total number of categories down from 20 to 13, we’ve only changed a total of 45 test rows. Even if the improved model were able to correctly predict all these observations now, we would only see an improvement of 1.2%. It’s certainly not insignificant, but hardly substantial. Improving our stop word list, on the other hand, will hypothetically improve the accuracy of the model overall.\nRecall earlier when we found the most common unigrams and bigrams for each category. Several terms that appeared most often had little or nothing to do with their respective grievances, and should be able to be removed while maintaining or improving original accuracy.\nThe original stop word list comes from another function in sklearn, and already contains over 300 words. We want to keep those words while adding to it, so we will union them together in a new list and use it in TfidfVectorizer.\n\n\nShow code\n# Import Function \nfrom sklearn.feature_extraction import text\n\n# Add Stop Words \nstop_words = text.ENGLISH_STOP_WORDS.union([\"please\", \"plz\", \"look\", \"help\", \"causing\", \"coming\", \"kindly\", \"refused\", \"senior\", \"help\", \"one\", \"two\", \"three\", \"also\", \"77\", \"1\", \"2\", \"3\", \"since\"])\n\n# TfidfVectorizer \ntfidf = TfidfVectorizer(\n  sublinear_tf = True, # Set term frequency to logarithmic scale\n  min_df = 5, # Remove terms that appear less than 'min_df' times\n  ngram_range = (1, 2), # Keep unigrams and bigrams\n  stop_words = stop_words # Use custom stop words \n)\n\n\n\n\nRedo Text Preprocessing\nWe have to redo the text preprocessing from earlier, so this is all copy-and-paste from before.\n\n\nShow code\n# Copy and Paste\ndf['category_id'] = df[\"category\"].factorize()[0]\ncategory_id_df = df[[\"category\", \"category_id\"]].drop_duplicates()\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[[\"category_id\", \"category\"]].values)\n\nfeatures = tfidf.fit_transform(df.description).toarray()\nlabels = df.category_id\nX = df[\"description\"]\ny = df[\"category\"]\n\nX_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(\n  features, \n  labels, \n  df.index, test_size = 0.25, \n  random_state = 2)\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n\n\n\nNew Results\nWe don’t need the complicated for loop from before because we only have one model this time. Therefore, we simply use cross_val_score as we did before and print the results to console.\n\n\nShow code\naccuracy_svc = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n\ncv_mean_accuracy_svc = accuracy_svc.mean()\ncv_mean_std_svc = accuracy_svc.std()\n\nprint(cv_mean_accuracy_svc * 100)\nprint(cv_mean_std_svc * 100)\n\n\n\n\n\n\nResults\n\n\n\nOur new Linear SVC model was able to achieve 91.987% accuracy with an average standard deviation of 0.282% across five iterations. That’s an improvement of 3.214% while also reducing variance within model performance by 0.086%.\nAnother way to look at these refinements; out of a possible 3,750 complaints, our model was able to correctly classify an additional 120 complaints, going from 3,328 correct predictions to 3,449."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#conclusions",
    "href": "blog/2023/bengaluru-text-mining/index.html#conclusions",
    "title": "Bengaluru Text Mining",
    "section": "Conclusions",
    "text": "Conclusions\nOnce again, the top three categories performed similarly as well as before. Road maintenance was able to correctly predict an additional 63 complaints on this iteration. These three categories also continue to make up most of the incorrect predictions.\nThe “Others” categories once again performed dreadfully, only recording an additional two correct predictions despite even more chances. Given it’s a category meant to be all-emcompassing, it probably makes sense to manually reclassify those comaplaints into new or existing categories.\nMinor categories saw little or no improvement. The “Health Dept” was the only category that performed worse, dropping from 69.7% to 54.6% accuracy. The model incorrectly chose “Solid Waste” and “Road Maintenance” much more often than the previous model did, though it’s unclear as to why this is.\nIncreasing the stop word list seems to have improved accuracy overall. A more thorough list and additional adjustments might boost performance slightly more."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#results",
    "href": "blog/2023/bengaluru-text-mining/index.html#results",
    "title": "Bengaluru Text Mining",
    "section": "Results",
    "text": "Results\n\nOver 80% of complaints filed were related to street lights not working, road maintenance, or garbage collection issues.\nOf the four classification models, the linear support vector classifier performed the best, recording 88.773% accuracy. The top three models all performed similarly as well, though, all falling within three percentage points.\nImprovements were made by increasing the number of stop words, as well as combining smaller categories into larger ones. Using LinearSVC, these changes led to a 3.214% increase, ultimately recording an accuracy of 91.987%.\nFurther improvements can be made by adding to the stop word list, changing the contents of the “Others” category, and adjusting the downsampling of the model."
  },
  {
    "objectID": "blog/2023/bengaluru-text-mining/index.html#source",
    "href": "blog/2023/bengaluru-text-mining/index.html#source",
    "title": "Bengaluru Text Mining",
    "section": "Source",
    "text": "Source\nYou can check out the Python source code for the MLE model here. Download the full dataset here."
  },
  {
    "objectID": "blog/2023/covid-19-stimulus-database-part-1/index.html#quick-rant",
    "href": "blog/2023/covid-19-stimulus-database-part-1/index.html#quick-rant",
    "title": "Building a COVID-19 Economic Relief Database: Part 1 (usaspending.gov)",
    "section": "Quick Rant",
    "text": "Quick Rant\nWe live in 2023, in a country of over 330+ million people in it. We all collectively pay taxes, which our representatives then utilize several trillion dollars to give out insured loans, contracts, and award grants. Perhaps I only speak for myself, but such information should be easily available for every citizen to view in simple terms.\nAn official “government” database didn’t exist until 2007. 2007. It took 2007 to create a database to track subsidies, one of the basic blocks of our economic system!\nusaspending.gov was created from this bill, but the original database wasn’t created by the government. It was created by OMB Watch (now named the Project on Government Oversight), who was paid $600,000 to develop the existing database that OMB Watch had already created. To be fair, the article does say that they worked together with the OMB (as in the government agency, Office of Management and Budget), but shouldn’t something this important deserve more funding?\nAt the very least, OMB Watch is a non-profit. Several government databases are the result of contractual work, often with few bidders. One of the most difficult things when maintaining a database is consistency, making sure columns are filled in correctly, and that APIs / download links are easy and accessible. If you’re a private company, you are disincentivized to care about these issues because they’re irrelevant once the contract is over.\nusaspending.gov isn’t directly tracking data, but they’re compiling data from various different agencies. A Senate Permanent Subcommittee on Investigations found that from April-June 2017, 55 percent of the data submitted was inaccurate, incomplete, or both. A recent report by the Government Accountability Office found that agencies are often still slow with reporting data. And of course, data from usaspending.gov still requires a lot of work to use.\nThis database should be 100x better than what is currently is, and it wouldn’t be that difficult to improve it. The ability to view the parent company has still yet to be implemented; what the hell is that? It’s all very annoying."
  },
  {
    "objectID": "blog/2023/covid-19-stimulus-database-part-1/index.html#other-data",
    "href": "blog/2023/covid-19-stimulus-database-part-1/index.html#other-data",
    "title": "Building a COVID-19 Economic Relief Database: Part 1 (usaspending.gov)",
    "section": "Other Data",
    "text": "Other Data\nAlright, with that out of my system…\nThis database isn’t meant to be solely about subsidies, but to connect them with other relevant economic information that people might want to analyze. In addition to this information, in the future, I’d like to build databases on;\n\nStock Buybacks: For those unaware, stock buybacks is the practice of a company purchasing shares of their own stock. This reduces the number of shares on the market, which in turn, increases the share price for those already holding it. Prior to 1982, stock buybacks were subjected to harsh scrutiny from the SEC and was generally classified as market manipulation; however, Rule 10b-18 reversed this and made it much easier for large share repurchasing, which companies now devote significant capital to. The practice is controversial; when airline workers were on strike in September 2022, one of their demands was to extend a COVID-19 provision that prohibited airline companies from engaging in stock buybacks, which ultimately did not come to fruition. This information is clearly relevant, and would offer a more comprehensive overview of how companies affect the greater economy.\nWages and Employee Numbers: A sizable portion of aid went towards various payroll support programs to help businesses pay their workers. Different agencies have different formulas for figuring out how much to give each recipient, which is usually calculated based on employment size, average salary, etc., but this data isn’t publicly available, so there’s no real way to determine if funds are distributed equitably or not. Such a database would also allow for tracking company outcomes during and after the pandemic. An October 2020 congressional report found that at least 15 companies had fired or furloughed over 16,500 employees after receiving payroll support.\nLegal Violations: Many of the companies who received aid during the pandemic have also been sued for billions of dollars for a variety of infractions, including civil court cases, OSHA incidents, DOJ investigations, etc. Many loans came from some of the largest banks in America, who are also among the most sued companies in the country. For reference, the four largest domestic banks have been sued at least 882 times for over $167 billion since 1995. Rewarding companies who continually break the law is - in my personal opinion - bad, and a database that tracks those violations would allow citizens to see if their tax dollars are going towards responsible corporations.\nOther Subsidies: Companies who are already heavily subsidized are arguably less deserving of government aid. Since 2019, you can download a PostgreSQL database from usaspending.gov that contains all subsidy information since FY 2001. However, similar to this COVID-19 dataset, it’s in a format that’s essentially unusable, and requires a lot of work to make use of it.\n\nGathering the data for these projects will require a ton of additional work, and won’t be completed anytime in the near future. However, I’m laying this out now to consider what schematics this database will have, considering they will all directly relate to each other.\n\nWith that out of the way, let’s get the data."
  },
  {
    "objectID": "blog/2023/pcaob-part-2-enforcement/index.html#domestic-sanctions",
    "href": "blog/2023/pcaob-part-2-enforcement/index.html#domestic-sanctions",
    "title": "Analyzing 20 Years of PCAOB Data: Part 2 (Enforcement)",
    "section": "Domestic Sanctions",
    "text": "Domestic Sanctions\n\n\nShow code\ndomestic_overview |>\n  gt() |>\n  gt_img_rows(\n    columns = logo,\n    img_source = \"local\"\n  ) |>\n  gt_color_rows(\n    columns = c(firm_fines, auditors_fines, total_fines),\n    palette = \"ggsci::indigo_material\",\n    domain = c(0, 11000000)\n  ) |>\n  cols_align(\n    align = \"left\",\n    columns = c(logo, firm, revenue)\n  ) |>\n  cols_align(\n    align = \"center\",\n    columns = c(firm_sanctions, auditors_sanctions, total_sanctions)\n  ) |>  \n  fmt_currency(\n    columns = revenue,\n    currency = \"USD\",\n    suffixing = TRUE\n  ) |>\n  fmt_currency(\n    columns = c(firm_fines, auditors_fines, total_fines),\n    currency = \"USD\"\n  ) |>\n  tab_spanner(\n    label = \"FIRMS\",\n    columns = c(firm_sanctions, firm_fines)\n  ) |>\n  tab_spanner(\n    label = \"AUDITORS\",\n    columns = c(auditors_sanctions, auditors_fines)\n  ) |>\n  tab_spanner(\n    label = \"TOTALS\",\n    columns = c(total_sanctions, total_fines)\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"bottom\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_spanners()\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_spanners()\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_body(columns = c(firm_fines, auditors_fines, total_fines))\n  ) |>\n  tab_style(\n    cell_text(\n      color = \"#212124\",\n      size = \"large\",\n      transform = \"uppercase\"\n    ),\n    locations = cells_column_spanners()\n  ) |>  \n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_labels(columns = c(firm_fines, auditors_fines, total_fines))\n  ) |>\n  tab_style(\n    cell_text(\n      weight = \"bold\"\n    ),\n    locations = cells_body(columns = c(firm_sanctions, auditors_sanctions, total_sanctions))\n  ) |>  \n  tab_header(\n    title = md(\"PCAOB: ENFORCEMENT AGAINST DOMESTIC FIRMS\")\n  ) |>\n  tab_source_note(md(\"**Data**: PCAOB\")) |>\n  gt_theme_nytimes() |>\n  tab_options(\n    table.background.color = \"#fcfcff\",\n    column_labels.font.size = 9.5\n  ) |>\n  tab_footnote(\n    md(\"*Everyone Else* includes all firms **except** for sole proprietors and the eight firms listed above.\")\n  ) |>\n  cols_label(\n    logo = \"\",\n    firm = \"firm\",\n    revenue = \"2022 REVENUE\",\n    firm_sanctions = \"SANCTIONS\",\n    firm_fines = \"FINES\",\n    auditors_sanctions = \"SANCTIONS\",\n    auditors_fines = \"FINES\",\n    total_sanctions = \"SANCTIONS\",\n    total_fines = \"FINES\"\n  ) |>\n  cols_width(\n    logo ~ px(70),\n    firm ~ px(200),\n    revenue ~ px(100),\n    firm_sanctions ~ px(70),\n    firm_fines ~ px(125),\n    auditors_sanctions ~ px(70),\n    auditors_fines ~ px(115),\n    total_sanctions ~ px(70),\n    total_fines ~ px(125)\n  ) \n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      PCAOB: ENFORCEMENT AGAINST DOMESTIC FIRMS\n    \n    \n  \n  \n    \n      \n      firm\n      2022 REVENUE\n      \n        FIRMS\n      \n      \n        AUDITORS\n      \n      \n        TOTALS\n      \n    \n    \n      SANCTIONS\n      FINES\n      SANCTIONS\n      FINES\n      SANCTIONS\n      FINES\n    \n  \n  \n    \nDeloitte\n$59.30B\n3\n$3,500,000.00\n5\n$110,000.00\n8\n$3,610,000.00\n    \nPricewaterhouseCoopers\n$50.30B\n1\n$1,000,000.00\n5\n$110,000.00\n6\n$1,110,000.00\n    \nErnst & Young\n$45.40B\n1\n$2,000,000.00\n10\n$210,000.00\n11\n$2,210,000.00\n    \nKPMG\n$32.30B\n0\n$0.00\n3\n$100,000.00\n3\n$100,000.00\n    \nBDO\n$10.30B\n0\n$0.00\n2\n$0.00\n2\n$0.00\n    \nRSM\n$6.30B\n1\n$15,000.00\n0\n$0.00\n1\n$15,000.00\n    \nGrant Thornton\n$5.80B\n3\n$2,290,000.00\n7\n$30,000.00\n10\n$2,320,000.00\n    \nCrowe\n$4.30B\n0\n$0.00\n0\n$0.00\n0\n$0.00\n    \nEveryone Else\nNA\n127\n$2,284,500.00\n217\n$1,605,500.00\n344\n$3,890,000.00\n  \n  \n    \n      Data: PCAOB\n    \n  \n  \n    \n       Everyone Else includes all firms except for sole proprietors and the eight firms listed above."
  },
  {
    "objectID": "blog/2023/pcaob-part-2-enforcement/index.html#international-sanctions",
    "href": "blog/2023/pcaob-part-2-enforcement/index.html#international-sanctions",
    "title": "Analyzing 20 Years of PCAOB Data: Part 2 (Enforcement)",
    "section": "International Sanctions",
    "text": "International Sanctions\n\n\nShow code\ninternational_overview |>\n  gt() |>\n  gt_img_rows(\n    columns = logo,\n    img_source = \"local\",\n    height = px(40)\n  ) |>\n  gt_color_rows(\n    columns = c(firm_fines, auditors_fines, total_fines),\n    palette = \"ggsci::pink_material\",\n    domain = c(0, 11000000)\n  ) |>\n  cols_align(\n    align = \"left\",\n    columns = c(logo, firm, revenue)\n  ) |>\n  cols_align(\n    align = \"center\",\n    columns = c(firm_sanctions, auditors_sanctions, total_sanctions)\n  ) |>  \n  fmt_currency(\n    columns = revenue,\n    currency = \"USD\",\n    suffixing = TRUE\n  ) |>\n  fmt_currency(\n    columns = c(firm_fines, auditors_fines, total_fines),\n    currency = \"USD\"\n  ) |>\n  tab_spanner(\n    label = \"FIRMS\",\n    columns = c(firm_sanctions, firm_fines)\n  ) |>\n  tab_spanner(\n    label = \"AUDITORS\",\n    columns = c(auditors_sanctions, auditors_fines)\n  ) |>\n  tab_spanner(\n    label = \"TOTALS\",\n    columns = c(total_sanctions, total_fines)\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"bottom\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_spanners()\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_spanners()\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_body(columns = c(firm_fines, auditors_fines, total_fines))\n  ) |>\n  tab_style(\n    cell_text(\n      color = \"#212124\",\n      size = \"large\",\n      transform = \"uppercase\"\n    ),\n    locations = cells_column_spanners()\n  ) |>  \n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_labels(columns = c(firm_fines, auditors_fines, total_fines))\n  ) |>\n  tab_style(\n    cell_text(\n      weight = \"bold\"\n    ),\n    locations = cells_body(columns = c(firm_fines, auditors_fines, total_fines))\n  ) |>  \n  tab_header(\n    title = md(\"PCAOB: ENFORCEMENT AGAINST FIRMS\"),\n    subtitle = md(\"According to Sarbanes-Oxley, the Public Company Accounting Oversight Board has the authority to sanction any firm that violates the board's regulations, which can involve censure, limiting of business activities, barring from the accounting industry, and/or a monetary fine. Fines can be as high as $2 million for normal violations, while severe offenses can be as high as $15 million. <br><br> Below is a table of all **269** sanctions and **$35,952,000** given by the PCAOB in their 20+ history against firms, broken down between domestic and international affiliates among the eight largest firms. Sole Proprietors, or single owned businesses, have been condensed into their own category. <br>\")\n  ) |>\n  tab_source_note(md(\"**Data**: PCAOB\")) |>\n  gt_theme_nytimes() |>\n  tab_options(\n    table.background.color = \"#fcfcff\",\n    column_labels.font.size = 11,\n    table.font.size = 18\n  ) |>\n  tab_footnote(\n    md(\"*Everyone Else* includes all firms **except** for sole proprietors and the eight firms listed above.\")\n  ) |>\n  cols_label(\n    logo = \"\",\n    firm = \"firm\",\n    revenue = \"2022 REVENUE\",\n    firm_sanctions = \"SANCTIONS\",\n    firm_fines = \"FINES\",\n    auditors_sanctions = \"SANCTIONS\",\n    auditors_fines = \"FINES\",\n    total_sanctions = \"SANCTIONS\",\n    total_fines = \"FINES\"\n  ) |>\n  cols_width(\n    logo ~ px(90),\n    firm ~ px(200),\n    revenue ~ px(105),\n    firm_sanctions ~ px(45),\n    firm_fines ~ px(120),\n    auditors_sanctions ~ px(45),\n    auditors_fines ~ px(110),\n    total_sanctions ~ px(45),\n    total_fines ~ px(120)\n  ) \n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      PCAOB: ENFORCEMENT AGAINST FIRMS\n    \n    \n      According to Sarbanes-Oxley, the Public Company Accounting Oversight Board has the authority to sanction any firm that violates the board's regulations, which can involve censure, limiting of business activities, barring from the accounting industry, and/or a monetary fine. Fines can be as high as $2 million for normal violations, while severe offenses can be as high as $15 million.  Below is a table of all 269 sanctions and $35,952,000 given by the PCAOB in their 20+ history against firms, broken down between domestic and international affiliates among the eight largest firms. Sole Proprietors, or single owned businesses, have been condensed into their own category. \n    \n  \n  \n    \n      \n      firm\n      2022 REVENUE\n      \n        FIRMS\n      \n      \n        AUDITORS\n      \n      \n        TOTALS\n      \n    \n    \n      SANCTIONS\n      FINES\n      SANCTIONS\n      FINES\n      SANCTIONS\n      FINES\n    \n  \n  \n    \nDeloitte\n$59.30B\n7\n$10,515,000.00\n29\n$325,000.00\n36\n$10,840,000.00\n    \nPricewaterhouseCoopers\n$50.30B\n10\n$2,400,000.00\n1\n$10,000.00\n11\n$2,410,000.00\n    \nErnst & Young\n$45.40B\n2\n$1,010,000.00\n3\n$95,000.00\n5\n$1,105,000.00\n    \nKPMG\n$32.30B\n13\n$9,170,000.00\n10\n$275,000.00\n23\n$9,445,000.00\n    \nBDO\n$10.30B\n6\n$610,000.00\n8\n$37,500.00\n14\n$647,500.00\n    \nRSM\n$6.30B\n1\n$10,000.00\n0\n$0.00\n1\n$10,000.00\n    \nGrant Thornton\n$5.80B\n3\n$40,000.00\n1\n$0.00\n4\n$40,000.00\n    \nCrowe\n$4.30B\n2\n$25,000.00\n0\n$0.00\n2\n$25,000.00\n    \nEveryone Else\nNA\n26\n$699,500.00\n21\n$75,000.00\n47\n$774,500.00\n  \n  \n    \n      Data: PCAOB\n    \n  \n  \n    \n       Everyone Else includes all firms except for sole proprietors and the eight firms listed above.\n    \n  \n\n\n\n\n\nIf you remember from our first section, the PCAOB issues annual investigation reports for each of the largest firms, and one would expect that firms who perform worse on these investigations would be more likely to be fined.\nA quick glance at the tables shows this is almost entirely not the case. Despite both KPMG and BDO performing the worse on inspection reports among their domestic affiliates, they’ve been sanctioned the least among the eight largest domestic firms. In fact, neither domestic firm has been fined a single time in PCAOB history.\nOutside of performance, we’d expect enforcement to be skewed towards the largest firms due to their size. If the largest firms are auditing most the companies, it would be logical to think they’d also get most of the enforcement. The PCAOB seemingly has other ideas. Grant Thornton, who recorded a 1/6 of the revenue of KPMG in 2022, has been fined $2,250,000 by the board, the second most of any domestic firm - and all while having a much lower deficiency rate of ___ compared to ___ of KPMG.\nIn total, while the eight largest firms have been sanctioned 8 times for $8,765,000, everyone else has been sanctioned 193 times for $2,767,500. So while larger firms receive harsher fines, the PCAOB is clearly focused on smaller firms. Specifically, 70 of those sanctions were against sole proprietors, with most cases involving penny stocks.\nSanctions against international affiliates looks much different comparatively. Deloitte and KPMG have been absolutely hammered on the international stage, though most of these fines are the result of single incidents. In 2016, Deloitte was fined $8 million regarding its audit work of the Brazilian GOL airlines that also included 12 auditors receiving sanctions. Recently, on 12/06/22, the PCAOB went on a rampage against KPMG, sanctioning four international affiliates for a total of $6.8 million.\nIt’s interesting that the PCAOB would target international affiliates so aggressively considering that they are much less important than their domestic counterparts. It could be argued that it’s because the international affiliates perform worse, but that logic doesn’t seem to apply to KPMG and BDO at all. As a result, it’s hard to look at PCAOB enforcement as anything other than arbitrary and random.\nMore fundamentally, if the board can fine firms over $15 million for “repeated instances of negligent conduct”, why are the amounts so low given such high deficiency rates? If BDO can fail almost half of their audit inspections and not be subjected to a single fine, or if KPMG can fail a third of their audits while also trying to cheat PCAOB investigations which eventually gets them sanctioned by the SEC for $50 million - what the hell does “repeated instances of negligent conduct” really mean?"
  },
  {
    "objectID": "blog/2023/pcaob-part-2-enforcement/index.html#all-enforcement-actions",
    "href": "blog/2023/pcaob-part-2-enforcement/index.html#all-enforcement-actions",
    "title": "Analyzing 20 Years of PCAOB Data: Part 2 (Enforcement)",
    "section": "All Enforcement Actions",
    "text": "All Enforcement Actions\n\n\nShow code\nenforcement |>\n  select(country, date, parent_company, fine, link) |>\n  arrange(desc(fine)) |>\n  reactable(\n    columns = list(\n      country = colDef(name = \"Country\"),\n      parent_company = colDef(name = \"Parent Company\"),\n      date = colDef(\n        name = \"Date\",\n        format = colFormat(date = TRUE, locales = \"en-US\")\n      ),\n      fine = colDef(\n        name = \"Fine\",\n        format = colFormat(prefix = \"$\", separators = TRUE, digits = 0)\n      ),\n      link = colDef(name = \"Link\")\n    )\n  )\n\n\n\nAnd here’s all PCAOB enforcement actions since 12/22/2022."
  },
  {
    "objectID": "notes/echarts4r/index.html#a-tooltip-theming",
    "href": "notes/echarts4r/index.html#a-tooltip-theming",
    "title": "echarts4r Cookbook",
    "section": "4.A Tooltip Theming",
    "text": "4.A Tooltip Theming"
  },
  {
    "objectID": "notes/echarts4r/index.html#a.-column",
    "href": "notes/echarts4r/index.html#a.-column",
    "title": "echarts4r Cookbook",
    "section": "2.A. Column",
    "text": "2.A. Column\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_tooltip(trigger = \" axis\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#heatmap",
    "href": "notes/echarts4r/index.html#heatmap",
    "title": "echarts4r Cookbook",
    "section": "2. . Heatmap",
    "text": "2. . Heatmap"
  },
  {
    "objectID": "notes/echarts4r/index.html#candlestick",
    "href": "notes/echarts4r/index.html#candlestick",
    "title": "echarts4r Cookbook",
    "section": "2. . Candlestick",
    "text": "2. . Candlestick"
  },
  {
    "objectID": "notes/echarts4r/index.html#treemap",
    "href": "notes/echarts4r/index.html#treemap",
    "title": "echarts4r Cookbook",
    "section": "2. . Treemap",
    "text": "2. . Treemap"
  },
  {
    "objectID": "notes/echarts4r/index.html#calendars",
    "href": "notes/echarts4r/index.html#calendars",
    "title": "echarts4r Cookbook",
    "section": "2. . Calendars",
    "text": "2. . Calendars\n\ndates <- seq.Date(as.Date(\"2017-01-01\"), as.Date(\"2018-12-31\"), by = \"day\")\nvalues <- rnorm(length(dates), 20, 6)\n\nyear <- data.frame(date = dates, values = values)\n\nyear |> \n  e_charts(date) |> \n  e_calendar(range = \"2018\") |> \n  e_heatmap(values, coord_system = \"calendar\") |> \n  e_visual_map(max = 30) |> \n  e_title(\"Calendar\", \"Heatmap\") |>\n  e_tooltip()\n\n\n\n\n\nWhen charting by multiple years, call e_calendar again and group by year.\n\nyear |> \n  dplyr::mutate(year = format(date, \"%Y\")) |>\n  group_by(year) |> \n  e_charts(date) |> \n  e_calendar(range = \"2017\",top=\"40\") |> \n  e_calendar(range = \"2018\",top=\"260\") |> \n  e_heatmap(values, coord_system = \"calendar\") |> \n  e_visual_map(max = 30) |> \n  e_title(\"Calendar\", \"Heatmap\")|>\n  e_tooltip(\"item\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#j.-pie-donut-chart",
    "href": "notes/echarts4r/index.html#j.-pie-donut-chart",
    "title": "echarts4r Cookbook",
    "section": "2.J. Pie / Donut Chart",
    "text": "2.J. Pie / Donut Chart\nNo."
  },
  {
    "objectID": "notes/echarts4r/index.html#a.-set-common-theme",
    "href": "notes/echarts4r/index.html#a.-set-common-theme",
    "title": "echarts4r Cookbook",
    "section": "3.A. Set Common Theme",
    "text": "3.A. Set Common Theme\nSet theme for all charts on the page and set font.\n\ne_common(\n  theme = \"my-theme\",\n  font_family = \"my-font\"\n)"
  },
  {
    "objectID": "notes/echarts4r/index.html#b.-tooltip-theme",
    "href": "notes/echarts4r/index.html#b.-tooltip-theme",
    "title": "echarts4r Cookbook",
    "section": "3.B. Tooltip Theme",
    "text": "3.B. Tooltip Theme\nI always customize the tooltip to get rid of the ugly border and the white background.\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_legend(show = FALSE) |>\n  e_axis_labels(\n    x = \"Island\",\n    y = \"Averge Mass\"\n  ) |>\n  e_title(\n    text = \"Some meaningful title.\"\n  ) |>\n  e_tooltip(\n    trigger = \" axis\",\n    backgroundColor = \"rgba(40, 40, 40, 0.75)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    textStyle = list(\n      color = \"#fcfcff\"\n    )\n  )"
  },
  {
    "objectID": "notes/echarts4r/index.html#c.-remove-legend",
    "href": "notes/echarts4r/index.html#c.-remove-legend",
    "title": "echarts4r Cookbook",
    "section": "3.C. Remove Legend",
    "text": "3.C. Remove Legend\n\npenguins |>\n  group_by(island) |>\n  summarise(average_mass = mean(body_mass_g, na.rm = TRUE)) |>\n  e_chart(island) |>\n  e_bar(average_mass) |>\n  e_legend(show = FALSE) |>\n  e_axis_labels(\n    x = \"Island\",\n    y = \"Averge Mass\"\n  ) |>\n  e_legend(show = FALSE) |> # THIS LINE OF CODE # \n  e_title(\n    text = \"Some meaningful title.\"\n  ) |>\n  e_tooltip(trigger = \" axis\")"
  },
  {
    "objectID": "notes/echarts4r/index.html#c.-default-tooltip-formatter",
    "href": "notes/echarts4r/index.html#c.-default-tooltip-formatter",
    "title": "echarts4r Cookbook",
    "section": "3.C. Default Tooltip Formatter",
    "text": "3.C. Default Tooltip Formatter\nOne of the arguments in e_tooltip is formatter. There are three different types of default formats that make it easy to display tooltips nicer:\n\nmy_echart |>\n  e_tooltip(\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    # OR #\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n    # OR #\n    formatter = e_tooltip_pointer_formatter(style = \"percent\"),\n  )"
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html#gathering-the-data",
    "href": "blog/2023/pcaob-part-1-inspections/index.html#gathering-the-data",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Gathering the Data",
    "text": "Gathering the Data\nBefore diving into things, I’ll make a quick note on how the data was collected.\nIf you go to pcaobus.org, you’ll find two separate databases: inspections and enforcement actions. Both databases provide reports in PDF format, so I manually gathered the relevant details from each file, and stored them in two separate spreadsheets.\nThe PCAOB has performed over 3,500 inspection reports since its creation in 2002, with about 2,800 being on domestic firms. I collected data on the firm, year, number of audits inspected, and number of audits that were deficient from each of the report. This takes a long time, which is why I only have data on the largest auditing firms. Only 14 firms receive annual inspections from the PCAOB, so the data outside of them is sparse anyways. I also recorded industry and revenue range for the four largest firms, who were the focus of the article, and will be the focus here.\nThose Big Four - those being Deloitte, PricewaterhouseCoopers, KPMG, and Ernst & Young - audit almost half of all publicly traded companies, including almost every company on the S&P 500. They have a disproportionate influence on overall audit quality, which is also why the POGO article spends so much time focusing on them. This post will follow a similar approach."
  },
  {
    "objectID": "blog/2023/pcaob-part-1-inspections/index.html#programming",
    "href": "blog/2023/pcaob-part-1-inspections/index.html#programming",
    "title": "Analyzing 20 Years of PCAOB Data: Part 1 (Inspections)",
    "section": "Programming",
    "text": "Programming\nCode for my programmers; scroll down to table if you don’t care.\nSetup: Here, we import the neccessary libraries and the data. The tables will be created using gt, while the interactive charts are created using ehcarts4r.\n\n\nShow code\n# Load Libraries ---- \nlibrary(tidyverse)\nlibrary(gt)\nlibrary(sysfonts)\nlibrary(ggtext)\nlibrary(gtExtras)\nlibrary(here)\nlibrary(extrafont)\nlibrary(echarts4r)\n\n# Set Directory ----\nhere::set_here()\n\n# Import Data ----\ninspections <- read_csv(\"_data/inspections.csv\")\ninspections_category <- read_csv(\"_data/inspections_category.csv\")\ninspections_revenue <- read_csv(\"_data/inspections_float.csv\")\n\n\nData Wrangling: In order to get the table, we need to create two dataframes containing both domestic and international summations. Then, we can join those dataframes together and calculate totals. I also manually added the logo path and 2022 revenue amounts using case_when.\n\n\nShow code\n# Extracting year from date \ninspections$year <- as.Date(as.character(inspections$year), format = \"%Y\")\ninspections$year <- substring(inspections$year, 1, 4)\n\n# Calculating rates, totals, and select column order for domestic and international \noverview_domestic <- inspections |>\n  filter(!is.na(audits_inspected)) |>\n  filter(!is.na(audits_failed)) |>\n  filter(country == \"United States\") |>\n  group_by(firm) |>\n  summarise(\n    domestic_audits_inspected = sum(audits_inspected, na.rm = TRUE),\n    domestic_audits_failed = sum(audits_failed, na.rm = TRUE),\n    domestic_fail_rate = domestic_audits_failed / domestic_audits_inspected,\n  )\n\noverview_international <- inspections |>\n  filter(!is.na(audits_inspected)) |>\n  filter(!is.na(audits_failed)) |>\n  filter(country != \"United States\") |>\n  group_by(firm) |>\n  summarise(\n    international_audits_inspected = sum(audits_inspected, na.rm = TRUE),\n    international_audits_failed = sum(audits_failed, na.rm = TRUE),\n    international_fail_rate = international_audits_failed / international_audits_inspected\n  )\n\n# Joining data together \noverview <- left_join(\n  x = overview_domestic,\n  y = overview_international,\n  by = \"firm\"\n)  |>\n  drop_na()\n\n# Calculating totals and adding revenue/logo path\noverview <- overview |>\n  mutate(\n    total_audits_inspected = domestic_audits_inspected + international_audits_inspected,\n    total_audits_failed = domestic_audits_failed + international_audits_failed,\n    total_fail_rate = total_audits_failed / total_audits_inspected,\n    revenue = case_when(\n      firm == \"Deloitte\" ~ 59300000000,\n      firm == \"PricewaterhouseCoopers\" ~ 50300000000, \n      firm == \"Ernst & Young\" ~ 45400000000,\n      firm == \"KPMG\" ~ 34600000000,\n      firm == \"BDO\" ~ 12800000000,\n      firm == \"RSM\" ~ 8132000000,\n      firm == \"Grant Thornton\" ~ 7200000000,\n      firm == \"Crowe\" ~ 3800000000\n    ),\n    logo = case_when(\n      firm == \"Deloitte\" ~ \"_logos/deloitte.png\",\n      firm == \"PricewaterhouseCoopers\" ~ \"_logos/pwc.png\", \n      firm == \"Ernst & Young\" ~ \"_logos/ernst-and-young.png\",\n      firm == \"KPMG\" ~ \"_logos/kpmg.png\",\n      firm == \"BDO\" ~ \"_logos/bdo.png\",\n      firm == \"RSM\" ~ \"_logos/rsm.png\",\n      firm == \"Grant Thornton\" ~ \"_logos/grant-thornton.png\",\n      firm == \"Crowe\" ~ \"_logos/crowe.png\"\n    )\n  ) |>\n  drop_na() |>\n  select(logo, firm, revenue, domestic_audits_inspected, domestic_audits_failed, domestic_fail_rate, international_audits_inspected, international_audits_failed, international_fail_rate, total_audits_inspected, total_audits_failed, total_fail_rate)\n\n\nTable: Finally, making the table with gt.\n\n\nShow code\noverview |>\n  arrange(desc(revenue)) |>\n  gt() |>\n  gt_img_rows(\n    columns = logo,\n    img_source = \"local\",\n    height = px(40)\n  ) |>\n  fmt_percent(\n    columns = c(domestic_fail_rate, international_fail_rate, total_fail_rate),\n    decimals = 1\n  ) |>\n  fmt_currency(\n    columns = revenue,\n    currency = \"USD\",\n    suffixing = TRUE\n  ) |>\n  gt_color_rows(\n    columns = c(domestic_fail_rate, international_fail_rate, total_fail_rate),\n    palette = \"ggsci::blue_material\",\n    domain = c(0.18, 0.54)\n  ) |>\n  gt_theme_nytimes() |>\n  cols_align(\n    align = \"center\",\n    columns = c(revenue, domestic_audits_inspected, domestic_audits_failed, domestic_fail_rate, international_audits_inspected, international_audits_failed, international_fail_rate, total_audits_inspected, total_audits_failed, total_fail_rate)\n  ) |>\n  tab_header(\n    title = md(\"PCAOB INSPECTION RESULTS\"),\n    subtitle = md(\"The Public Company Accounting Oversight Board is a quasi-governmental agency founded in 2002 by the Sarbanes-Oxley Act. They are responsible for setting guidelines, regulating, and investigating the audit industry. About 45% of the PCAOB's budget is dedicated to inspections, where the board investigates whether an audit was conducted properly or not. <br><br> When audit opinions lack credible evidence, or the audit was conducted improperly, the board will declare it **deficient**, meaning the audit lacked evidence to support its audit opinion. Below are the inspection results for the eight largest audit firms in the United States, broken down between domestic and international firms, from 2009 - 2021. <br>\")\n  ) |>\n  tab_source_note(md(\"**Data**: PCAOB\")) |>\n  tab_footnote(\n    \"Deloitte includes years 2007 & 2008.\",\n    locations = cells_body(columns = firm, rows = 1)\n  ) |>\n  tab_footnote(\n    \"PwC includes 2008.\",\n    locations = cells_body(columns = firm, rows = 2)\n  ) |>\n  tab_footnote(\n    \"RSM only includes years 2015 - 2021.\",\n    locations = cells_body(columns = firm, rows = 6)\n  ) |>\n  tab_footnote(\n    \"All data was collected from 2009 - 2021 unless stated otherwise below.\"\n  ) |>\n  tab_spanner(\n    label = \"Domestic\",\n    columns = c(domestic_audits_inspected, domestic_audits_failed, domestic_fail_rate)\n  ) |>\n  tab_spanner(\n    label = \"International\",\n    columns = c(international_audits_inspected, international_audits_failed, international_fail_rate)\n  ) |>\n  tab_spanner(\n    label = \"Totals\",\n    columns = c(total_audits_inspected, total_audits_failed, total_fail_rate)\n  ) |>\n  tab_style(\n    cell_text(\n      color = \"#212124\",\n      size = \"large\",\n      transform = \"uppercase\"\n    ),\n    locations = cells_column_spanners()\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"bottom\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_spanners()\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_spanners()\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_body(columns = c(domestic_fail_rate, international_fail_rate))\n  ) |>\n  tab_style(\n    cell_borders(\n      sides = \"right\", color = \"#212124\", style = \"solid\", weight = px(2)\n    ),\n    locations = cells_column_labels(columns = c(domestic_fail_rate, international_fail_rate))\n  ) |>\n  tab_style(\n    cell_text(\n      weight = \"bold\"\n    ),\n    locations = cells_body(columns = c(domestic_fail_rate, international_fail_rate, total_fail_rate))\n  ) |>\n  tab_options(\n    table.background.color = \"#fcfcff\",\n    column_labels.font.size = 9.5,\n    table.font.size = 18,\n    heading.title.font.size = 30\n  ) |>\n  cols_width(\n    logo ~ px(70),\n    firm ~ px(210),\n    revenue ~ px(105),\n    everything() ~ px(65)\n  ) |>\n  cols_label(\n    logo = \"\",\n    revenue = \"2022 REVENUE\",\n    domestic_audits_inspected = \"AUDITS INSP.\",\n    domestic_audits_failed = \"AUDITS DEF.\",\n    international_audits_inspected = \"AUDITS INSP.\",\n    international_audits_failed = \"AUDITS DEF.\",\n    total_audits_inspected = \"AUDITS INSP.\",\n    total_audits_failed = \"AUDITS DEF.\",\n    domestic_fail_rate = \"DEF. RATE\",\n    international_fail_rate = \"DEF. RATE\",\n    total_fail_rate = \"DEF. RATE\"\n  ) \n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      PCAOB INSPECTION RESULTS\n    \n    \n      The Public Company Accounting Oversight Board is a quasi-governmental agency founded in 2002 by the Sarbanes-Oxley Act. They are responsible for setting guidelines, regulating, and investigating the audit industry. About 45% of the PCAOB's budget is dedicated to inspections, where the board investigates whether an audit was conducted properly or not.  When audit opinions lack credible evidence, or the audit was conducted improperly, the board will declare it deficient, meaning the audit lacked evidence to support its audit opinion. Below are the inspection results for the eight largest audit firms in the United States, broken down between domestic and international firms, from 2009 - 2021. \n    \n  \n  \n    \n      \n      firm\n      2022 REVENUE\n      \n        Domestic\n      \n      \n        International\n      \n      \n        Totals\n      \n    \n    \n      AUDITS INSP.\n      AUDITS DEF.\n      DEF. RATE\n      AUDITS INSP.\n      AUDITS DEF.\n      DEF. RATE\n      AUDITS INSP.\n      AUDITS DEF.\n      DEF. RATE\n    \n  \n  \n    \nDeloitte1\n$59.30B\n835\n176\n21.1%\n317\n142\n44.8%\n1152\n318\n27.6%\n    \nPricewaterhouseCoopers2\n$50.30B\n824\n197\n23.9%\n406\n117\n28.8%\n1230\n314\n25.5%\n    \nErnst & Young\n$45.40B\n729\n204\n28.0%\n349\n107\n30.7%\n1078\n311\n28.8%\n    \nKPMG\n$34.60B\n691\n234\n33.9%\n380\n155\n40.8%\n1071\n389\n36.3%\n    \nBDO\n$12.80B\n325\n156\n48.0%\n61\n25\n41.0%\n386\n181\n46.9%\n    \nRSM3\n$8.13B\n109\n42\n38.5%\n6\n2\n33.3%\n115\n44\n38.3%\n    \nGrant Thornton\n$7.20B\n444\n138\n31.1%\n61\n20\n32.8%\n505\n158\n31.3%\n    \nCrowe\n$3.80B\n183\n60\n32.8%\n9\n2\n22.2%\n192\n62\n32.3%\n  \n  \n    \n      Data: PCAOB\n    \n  \n  \n    \n       All data was collected from 2009 - 2021 unless stated otherwise below.\n    \n    \n      1 Deloitte includes years 2007 & 2008.\n    \n    \n      2 PwC includes 2008.\n    \n    \n      3 RSM only includes years 2015 - 2021.\n    \n  \n\n\n\n\nLet me give a quick breakdown of what you’re looking at.\nAbove are the eight largest audit firms in the United States, who audit over half of all publicly traded companies, including every single company on the S&P 500. I’ve also included their 2022 revenue amounts for comparison. As a reference, $59.3 billion would put Deloitte as the third largest privately owned company in the United States in terms of revenue, and top 50 overall.\nThe PCAOB will issue annual reports for the very largest firms, where they will inspect a select number of audits to see if they were conducted properly or not. They categorize deficiencies into two different types, though type B deficiencies are not recorded in any meaningful capacity. If the PCAOB finds an audit to be type A deficient, it means the audit was conducted so poorly that the results have no evidence to support their conclusions. This doesn’t mean that the company is engaged in fraud, or other accounting misconduct, but if they were, it likely would of gone undetected.\nIt should be noted that the PCAOB uses a risk-based selection process when choosing which audits to investigate. This means that deficiency rates aren’t representative of how often the average audit by a given firm will be deficient. Since 2016, the board began including a sample of randomly selected audits to include in their annual audit inspections, but the reports do not specify which audits were deficient or not.\nEach of these firms have affiliates all over the world, primarily in Canada, Mexico, the United Kingdom, and Bermuda. If a company/asset located in another country also appears on a US market, then is must audited to PCAOB standards.\nPOGO released another article talking about this, but even if we were to acknowledge that audits are selected based on risk, successfully predicting audits 33% to nearly 50% is insane. Even BDO’s most recent inspection report failed 16 out of 30 audits despite the fact 11 of them were selected randomly. Unfortunately, the reports don’t disclose the companies whose audits were found to be deficient, so it’s not possible to determine what tangible effect this has on the economy."
  },
  {
    "objectID": "blog/2023/cfo-act-of-1990/index.html",
    "href": "blog/2023/cfo-act-of-1990/index.html",
    "title": "Explaining the CFO Act of 1990",
    "section": "",
    "text": "There’s a good chance that if you’ve heard of this Act already, you read it in relation to the Department of Defense being unable to “pass” an audit. I want to give a more detailed breakdown about the Pentagon’s financial struggles, but it makes more sense to break them down into different sections."
  },
  {
    "objectID": "blog/2023/cfo-act-of-1990/index.html#cfo-act-of-1990",
    "href": "blog/2023/cfo-act-of-1990/index.html#cfo-act-of-1990",
    "title": "Explaining the CFO Act of 1990",
    "section": "CFO Act of 1990",
    "text": "CFO Act of 1990\nThe Government Accountability Office (GAO) has a really good report that I recommend reading if you want a more in-depth explanation. The only problem is that it’s 140 pages long, so here’s a paraphrased version of it.\nAnd if you want to look at a table with pretty colors, skip here.\n-blah-"
  },
  {
    "objectID": "blog/2023/cfo-act-of-1990/index.html#results",
    "href": "blog/2023/cfo-act-of-1990/index.html#results",
    "title": "Explaining the CFO Act of 1990",
    "section": "Results",
    "text": "Results\n-table-"
  },
  {
    "objectID": "blog/2023/cfo-act-of-1990/index.html#why-care",
    "href": "blog/2023/cfo-act-of-1990/index.html#why-care",
    "title": "Explaining the CFO Act of 1990",
    "section": "Why Care?",
    "text": "Why Care?\nI hope some people got a chuckle reading the footnotes on the table. Arthur Andersen was the firm responsible for “auditing” Enron, once the sixth largest corporation in the United States. They quickly collapsed once it was discovered their profits were largely the result of accounting fraud; Arthur Andersen would later be indicted by the DOJ for their (alleged) complicity. Given their conduct in that case, it really shouldn’t be a surprise their work in other areas was similarly lacking.\nThis does raise some questions. Why did NASA struggle so much while other agencies didn’t? Departments like HHS have budgets ten times over that of NASA, even if we were to account for discretionary vs. mandatory budgeting, yet those departments have received unmodified opinions for decades. For that matter, why do several other departments, like DOS and HUD, go years passing audits, only to find themselves failing multiple in a row?\nIf the answer is the same reason why Arthur Andersen failed to properly inspect Enron, then the question of auditor independence should be discussed. The same auditors who are providing services to some of the largest government contractors are also the same ones auditing those government departments in order to improve their financial security. There is a clear conflict of interest that exists in this arrangement.\nHowever, isn’t it conspiratorial to think audit firms are appeasing their clients by purposefully ensuring the financial systems of their buyers are sub-par? Ignoring intent, results collected by the PCAOB (source: me) show that audits of publicly traded companies are regularly conducted improperly, with an average deficiency rate of over 25% among the eight largest firms. The PCAOB doesn’t review the audits of government departments; that’s actually the responsibility of the GAO, though it’s far from their primary focus. The audit reviews they’ve done, especially in regards to the DoD, have not been glowing.\nOver the past two decades, government contracting has increased enormously. There is absolutely no question that government waste, especially in the DoD, exists. It exists outside of it, but the area that should be receiving the most attention is getting the least. Anyone who is genuinely concerned with government waste and isn’t starting with the DoD and/or government contracting is missing the big picture."
  },
  {
    "objectID": "blog/2023/cfo-act-of-1990/index.html#some-other-stuff",
    "href": "blog/2023/cfo-act-of-1990/index.html#some-other-stuff",
    "title": "Explaining the CFO Act of 1990",
    "section": "Some Other Stuff",
    "text": "Some Other Stuff\nReceiving unmodified opinions doesn’t mean a company’s financials are entirely in order.\n-chart-\nAuditors will classify problems into different categories, including ___, material weaknesses, and significant deficiencies. The chart only shows material weaknesses, which indicate a deficiency/combination of deficiencies that could potentially cause a financial misstatement. Audit opinions only represent the opinion that the financial statements are presented fairly, not whether there’s a possibility they aren’t.\nWhen it comes to the DoD, they keep finding problems faster than they can fix them. FY 2022 was the sixth straight year the department reported the same or more material weaknesses. In some ways, it’s a good thing. Acknowledging problems is the step to fixing them, but how long will it take to actually start fixing them remains to be seen."
  }
]