{"title":"Building a COVID-19 Economic Relief Database: Part 2 (Apache Arrow)","markdown":{"yaml":{"title":"Building a COVID-19 Economic Relief Database: Part 2 (Apache Arrow)","description":"Using the power of big data to compress 140GB of data to less than 10GB.","author":"Nathan States","date":"02/02/2023","draft":true},"headingText":"Quick Rundown","containsRefs":false,"markdown":"\n\nA quick recap of the last post. \n\n* We've now downloaded the compressed database from usaspending.gov and have it stored locally onto our computer. \n* We need to group the data by each `award_unique_key`. However, the data is spread across 139 different files, each one being over a giga-byte big, totaling over 140GB. \n* `R` stores datasets into RAM, which means we need over 140GB of RAM to perform computations. No commercial computer comes with that much memory, so how do we go about importing the dataset. \n\nThat's where **Apache Arrow** comes in. \n\n![](thumbnail.png){.preview-image}\n\n\n*Arrow* comes from the [Apache Software Foundation](https://www.apache.org/), who releases free and open source tools for software engineers and data scientists. The package was written using C++, and is available in a variety of programming languages, including Python. What's cool about this is that improvements made to the underlying code will improve performance across all languages as opposed to only one. You can download the `R` version by running: \n\n```{r, eval=FALSE}\ninstall.packages(\"arrow\")\n```\n\nArrow isn't specifically meant to be a solution for big data, but rather, **it's an attempt to standardize data formatting across programming languages**. It uses a special file type called *parquet* for storing data, which is designed to take advantage of modern CPU and GPU capabilities, meaning it can perform operations on billions of rows of data within seconds. Other packages - even `data.table` - can't compete. \n\n`data.table` was considered to be the fastest package at performing computations before Arrow, but it relied on the original framework `R` used when it was created in 1995. For most modern computers, Arrow should be even faster in the vast majority of use cases. Furthermore, unlike `data.table`, *Arrow* can understand `dplyr` language perfectly fine, which means you don't need to learn a different syntax when data wrangling. \n\nThere is, essentially, no reason not to use Arrow. It's simply superior to other options. With that said, *Arrow* will only offer milliseconds of improvement for dataframes under 10,000 rows, which is why it's usually reserved for big data problems only.  \n\n## Arrow Setup \n\nLet's quickly load the packages that we need. \n\n```{r, eval=TRUE}\nlibrary(arrow)\nlibrary(dplyr)\n\nhere::set_here()\n```\n\nI have all 139 files stored locally in a folder called \"covid-files,\" which we now want to import. Instead of calling the file name, Arrow allows us to call the *directory* to load the files all together. \n\n```{r, eval=TRUE}\ndf <- open_dataset(\n  \"_dummy_data\", \n  format = \"csv\"\n)\n```\n\nNow, we didn't *actually* load all the data, because that would break the laws of computer hardware. All we did is load the *metadata*, or the **data about our data**. We can see this by calling `df`, which normally would display a spreadsheet, but in this case, gives us the **assumed schema** of our data. \n\n```{r, echo=TRUE}\ndf\n```\n\nI included a demo spreadsheet for this post that contains 20 rows from one of the datasets, including all the original columns, in case you are wondering why only 1 csv file was read. If you are doing this with the original dataset, you will of course have many more files, but the concept is the same in both cases. \n\nLooking at the schema, some of the columns need to be changed to their correct data type. This should be straightforward for most programmers, but something to keep in mind when using *Arrow* specifically is that they differentiate between bit sizes. This means that if you were trying to run computations on a column with a `int64` datatype on a Windows 32-bit computer, you'd run into errors because *Arrow* would be trying to take advantage of computing power that doesn't exist. If you don't know what any of that means, there is a 99.99% chance you are running a 64 bit computer, in which case you will want to set all data types to that format. \n\nWe'll use the `schema` function to denote our fixed data types and reread in the data. When including a schema, you also have to skip the first row for some reason, so we'll make sure to do that. \n\n```{r}\nschema_fix <- schema(\n  owning_agency_name = string(),\n  reporting_agency_name = string(),\n  submission_period = string(),\n  allocation_transfer_agency_identifier_code = string(),\n  agency_identifier_code = string(),\n  beginning_period_of_availability = double(),\n  ending_period_of_availability = double(),\n  availability_type_code = string(),\n  main_account_code = string(),\n  sub_account_code = string(),\n  treasury_account_symbol = string(),\n  treasury_account_name = string(),\n  agency_identifier_name = string(),\n  allocation_transfer_agency_identifier_name = string(),\n  budget_function = string(),\n  budget_subfunction = string(),\n  federal_account_symbol = string(),\n  federal_account_name = string(),\n  program_activity_code = string(),\n  program_activity_name = string(),\n  object_class_code = double(),\n  object_class_name = string(),\n  direct_or_reimbursable_funding_source = string(),\n  disaster_emergency_fund_code = string(),\n  disaster_emergency_fund_name = string(),\n  transaction_obligated_amount = double(),\n  gross_outlay_amount_FYB_to_period_end = double(),\n  USSGL487200_downward_adj_prior_year_prepaid_undeliv_order_oblig = double(),\n  USSGL497200_downward_adj_of_prior_year_paid_deliv_orders_oblig = double(),\n  award_unique_key = string(),\n  award_id_piid = string(),\n  parent_award_id_piid = string(),\n  award_id_fain = string(),\n  award_id_uri = string(),\n  award_base_action_date = date64(),\n  award_base_action_date_fiscal_year = double(),\n  award_latest_action_date = date64(),\n  award_latest_action_date_fiscal_year = double(),\n  period_of_performance_start_date = date64(),\n  period_of_performance_current_end_date = date64(),\n  ordering_period_end_date = date64(),\n  award_type_code = string(),\n  award_type = string(),\n  idv_type_code = string(),\n  idv_type = string(),\n  prime_award_base_transaction_description = string(),\n  awarding_agency_code = string(),\n  awarding_agency_name = string(),\n  awarding_subagency_code = string(),\n  awarding_subagency_name = string(),\n  awarding_office_code = string(),\n  awarding_office_name = string(),\n  funding_agency_code = string(),\n  funding_agency_name = string(),\n  funding_sub_agency_code = string(),\n  funding_sub_agency_name = string(),\n  funding_office_code = string(),\n  funding_office_name = string(),\n  recipient_uei = string(),\n  recipient_duns = string(),\n  recipient_name = string(),\n  recipient_parent_uei = string(),\n  recipient_parent_duns = string(),\n  recipient_parent_name = string(),\n  recipient_country = string(),\n  recipient_state = string(),\n  recipient_county = string(),\n  recipient_city = string(),\n  recipient_congressional_district = string(),\n  recipient_zip_code = string(),\n  primary_place_of_performance_country = string(),\n  primary_place_of_performance_state = string(),\n  primary_place_of_performance_county = string(),\n  primary_place_of_performance_congressional_district = string(),\n  primary_place_of_performance_zip_code = string(),\n  cfda_number = double(),\n  cfda_title = string(),\n  product_or_service_code = string(),\n  product_or_service_code_description = string(),\n  naics_code = double(),\n  naics_description = string(),\n  national_interest_action_code = string(),\n  national_interest_action = string(),\n  usaspending_permalink = string(),\n  last_modified_date = date64()\n)\n\n# Import Data ----\ndf <- open_dataset(\n  \"_dummy_data\", \n  format = \"csv\",\n  skip_rows = 1,\n  schema = schema_fix\n)\n```\n\nIt's possible to do all the computations that we need to extract the information we want from here, including who the top recipients are, the congressional districts that received the most aid, award breakdowns by legislation, and so on. Still, I want to present this databse in a format that's easy and usable to the general public, which is going to require some:\n\n## Data Wrangling \n\nEven though there are a total of 85 columns, the majority of them are metadata pulled from other agencies that we don't need. In terms of columns that need to be inspected, those include (with descriptions):\n\n* `owning_agency_name`: The agency the award comes from. \n* `submission_period`: The year and month the award entry was submitted to usaspending.gov.\n* `beginning_period_of_availability`: The year the award was first available.\n* `ending_period_of_avaliability`: The year the award expires. \n* `treasury_account_name`: The program name along with the subagency name.\n* `budget_function`: General description of the award. \n* `budget_subfunction`: Slightly more specific description of the award.\n* `program_activity_name`: Name of the program.\n* `direct_or_reimbursable_funding_source`: \n* `disaster_emergency_fund_name`: CARES, ARPA, etc.\n* `transaction_obligated_amount`: The amount the recipient was obligated to. \n* `gross_outlay` and `USSGL` columns: Click [here]() for more details. \n* `award_type`: Whether the award was a grant, loan, contract, etc.\n* `prime_award_base_transaction_description`: A more specific description of what the award is trying to accomplish. \n* `award_office_name`: Office name from agency.\n* `recipient_name`: Company who received the award. \n* `recipient_parent_name`: Parent of the recipient, if available. \n* `recipient_country/state/county/city/cd/zip_code`: All should be self explanatory. It should be noted that these represent the *facility* that received the award, and not the recipients *headquarters*. \n* `cfda_title`: CFDA program name. \n* `usaspending_permalink`: Source for award data. \n* `last_modified_date`: Last time the entry was modified. \n\nAll of these columns are going to have to be dealt with differently, so I'll go through them one-by-one.\n\n### Preliminary Glance\n\nBefore we do that, let's take a quick look to make sure there are no duplicates in the dataset. \n\n```{r, eval=FALSE}\ndf %>%\n  distinct() \n```\n\nWhen using arrow, you have to call `dplyr::collect` to get the results of the query, otherwise nothing will be collected. \n\nNext, let's see how many NAs there are in total to get an idea about how we might want to deal with them later on. \n\n```{r, eval=FALSE}\ndf %>% summarise(across(everything(), ~ sum(is.na(.)))) %>% collect()\n```\n\n---\n\nThe rest of this post will just be me transforming the data into a usable format. My next post will be converting it into a `PostgreSQL` database, and after that, we'll actually analyze the data by creating some cool ass charts. Stay tuned. \n\n## Submission Period \n\nThese columns are written in the style of \"FY2020P11,\" where \"FY\" represents fiscal year and \"P\" represents the month. Representing this amount as \"2020/11\" not only is easier to read, but will for simple computations because its formatted as a proper date. \n\nUsing `dplyr` and `gsub` can easily replace the characters to transform the format into \"YYYY-MM,\" and `lubridate` can convert it the column into `datetime` type.  \n\n```{r, eval=FALSE}\ndf <- df %>%\n  mutate(submission_period = gsub(\"FY\", \"\", as.character(submission_period))) %>% # Replace FY with blank space\n  mutate(submission_period = gsub(\"P\", \"-\", as.character(submission_period))) %>% # Replace P with -\n  mutate(submission_period = lubridate::ym(submission_period)) %>% # Convert into datetime\n```\n\n## Award Types \n\nThere are several different award types within our dataset. We can view them all using `dplyr::summarise`. \n\n```{r}\ndf %>% \n  group_by(award_type) %>%\n  summarise(count = n()) %>%\n  collect()\n```\n\nThere are a few - what I can only assume, anyways - errors with some award types. Out of 100+ million rows, 20 of them have the type \"DO,\" and exactly one award has the type \"DCA.\" Out of curiosity, I had to know what that one record was. \n\n```{r, eval=FALSE}\ndca_award <- df %>% \n  filter(award_type == \"DCA\") %>% \n  collect()\n```\n\nIt was \"awarded\" to the National Academy of Sciences of the District of Columbia for the amount of -\\$16,830.09 with the description of \"IGF::CL::IGF PROFESSIONAL SERVICES ARE REQUESTED BY FIMA, OPPA AND ICPD TO ASSIST THE GOVERNEMENT IN COMMUNITY RESILIENCE.\" I like the idea of assisting the \"government in community resilience\" by withdrawing award funds, but anyways:\n\nThe discrepancies between grants and loans are important to differentiate from, so we want to keep them. You can find exact definition at the [Data Dictionary at usaspending.gov](https://www.usaspending.gov/data-dictionary), but to describe them in layman terms: \n\n* **Block Grant (A)**: Grants that can be used at the recipient's discretion. \n* **Formula Grant (A)**: Grants that have been awarded based on a distribution formula for activities of a \"continuing nature.\"  \n* \n\n```{r, eval=FALSE}\ndf %>%\n  mutate(\n    award_type = case_when(\n      award_type == \"DIRECT LOAN (E)\" ~ \"Loan\",\n      award_type == \"GUARANTEED/INSURED LOAN (F)\" ~ \"Loan\",\n      award_type == \"DIRECT PAYMENT FOR SPECIFIED USE, AS A SUBSIDY OR OTHER NON-REIMBURSABLE DIRECT FINANCIAL AID (C)\" ~ \"Grant\",\n      award_type == \"PURCHASE ORDER\" ~ \"Contract\",\n      award_type == \"DEFINITIVE CONTRACT\" ~ \"Contract\",\n      award_type == \"PROJECT GRANT (B)\" ~ \"Grant\",\n      award_type == \"COOPERATIVE AGREEMENT (B)\" ~ \"Grant\",\n      award_type == \"FORMULA GRANT (A)\" ~ \"Grant\",\n      award_type == \"BLOCK GRANT (A)\" ~ \"Grant\",\n      award_type == \"DELIVERY ORDER\" ~ \"Contract\",\n      award_type == \"DIRECT PAYMENT WITH UNRESTRICTED USE (RETIREMENT, PENSION, VETERANS BENEFITS, ETC.) (D)\" ~ \"Grant\",\n      award_type == \"BPA CALL\" ~ \"Contract\",\n      award_type == \"OTHER REIMBURSABLE, CONTINGENT, INTANGIBLE, OR INDIRECT FINANCIAL ASSISTANCE\" ~ \"Other\",\n      award_type == \"DO\" ~ \"Other\",\n      award_type == \"DCA\" ~ \"Other\"\n    ),\n    award_subtype = case_when(\n      award_type == \"DIRECT LOAN (E)\" ~ \"Direct\",\n      award_type == \"GUARANTEED/INSURED LOAN (F)\" ~ \"Insured\",\n      award_type == \"DIRECT PAYMENT FOR SPECIFIED USE, AS A SUBSIDY OR OTHER NON-REIMBURSABLE DIRECT FINANCIAL AID (C)\" ~ \"Specified Use\",\n      award_type == \"PURCHASE ORDER\" ~ \"Purchase\",\n      award_type == \"DEFINITIVE CONTRACT\" ~ \"Definitive\",\n      award_type == \"PROJECT GRANT (B)\" ~ \"Project\",\n      award_type == \"COOPERATIVE AGREEMENT (B)\" ~ \"Cooperative Agreement\",\n      award_type == \"FORMULA GRANT (A)\" ~ \"Formula\",\n      award_type == \"BLOCK GRANT (A)\" ~ \"Block\",\n      award_type == \"DELIVERY ORDER\" ~ \"Delivery Order\",\n      award_type == \"DIRECT PAYMENT WITH UNRESTRICTED USE (RETIREMENT, PENSION, VETERANS BENEFITS, ETC.) (D)\" ~ \"Unspecified Use\",\n      award_type == \"BPA CALL\" ~ \"BPA\",\n      award_type == \"OTHER REIMBURSABLE, CONTINGENT, INTANGIBLE, OR INDIRECT FINANCIAL ASSISTANCE\" ~ \"Other\",\n      award_type == \"DO\" ~ \"Other\",\n      award_type == \"DCA\" ~ \"Other\"\n    )\n  )\n```\n\nThe more concerning part is the number of NA values, which is essential to understanding award amounts. \n\nLoans and grants are broken down into different \n\n## Outlay and USSGL Amounts \n\nAs mentioned before, award recipients may not use all of their funds at once. Loans are sometimes paid back with interest, meaning some are \"profitable\" for the government. Others are forgiven. \n\nThe dataset includes a `transaction_obligated_amount`, which is the amount the recipient is entitled to, while the other columns represent incremental updates to each unique reward key. According to the directions, `transaction_obligated_amount` should be summed by itself, while the other three columns should be summed separately. Once again, `dplyr::summarise` makes this task simple. \n\n```{r, eval=FALSE}\ndf %>%\n  group_by(award_unique_key) %>%\n  summarise(\n    award_amount_obligated = sum(transaction_obligated_amount),\n    award_amount_utilized = sum(gross_outlay_amount_FYB_to_period_end, na.rm = TRUE) + sum(USSGL487200_downward_adj_prior_year_prepaid_undeliv_order_oblig, na.rm = TRUE) + sum(USSGL497200_downward_adj_of_prior_year_paid_deliv_orders_oblig, na.rm = TRUE)\n  )\n```\n\n## Base Action Dates \n\nThese refer to the dates that each award entry was entered. We're condensing these rows down, so we want the earliest base date along with the latest base date. Date types with `arrow` are quite intuitive to use; simply using `min` and `max` will solve this problem.\n\n```{r, eval=FALSE}\ndf %>%\n  group_by(award_unique_key) %>%\n  summarise(\n    first_action_date = min(award_base_action_date),\n    last_action_date = max(award_latest_action_date),\n  )\n```\n\n## Zip Codes\n\n**What the hell is this, usaspending.gov**? \n\nZip codes are notoriously one of the most annoying things to deal with in data, and they didn't even *try* to use any sort of standard for this column. Most zip codes are listed as five digits, but a few are three, nine, some are even seven. Nine digit codes are fine, and so are five, but codes in-between need leading 0s. However, zip codes *under* five also need leading 0s, but only until it reaches a standard five digit code, because increasing all the way to nine wouldn't be intuitive for use (for example, converting a 123 zip into 000000123 doesn't make any sense).\n\nTrying to do this in base `R` would be annoying and inefficient, requiring multiple `ifelse` statements, but various packages have been created to tackle our very issue. \n\n\n\n\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../assets/styles.css"],"highlight-style":{"light":"../../../assets/syntax-light.theme","dark":"../../../assets/syntax-dark.theme"},"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","editor":"source","theme":{"light":"../../../assets/states-light.scss","dark":"../../../assets/states-dark.scss"},"smooth-scroll":true,"code-block-border-left":true,"title-block-banner":true,"title":"Building a COVID-19 Economic Relief Database: Part 2 (Apache Arrow)","description":"Using the power of big data to compress 140GB of data to less than 10GB.","author":"Nathan States","date":"02/02/2023","draft":true},"extensions":{"book":{"multiFile":true}}}}}