{"title":"Bengaluru Text Mining","markdown":{"yaml":{"title":"Bengaluru Text Mining","description":"Analyzing over 100,000 complaints from the Silicon Valley of India.","author":"Nathan States","date":"01/22/2023","categories":["text_mining","machine_learning"],"format":{"html":{"code-fold":true,"code-summary":"Show code","toc":true,"toc-location":"right","code-tools":true}}},"headingText":"Results","containsRefs":false,"markdown":"\n\n![Aerial view of *Bengaluru*, India.](_images/preview.jpg)\n\n<br>\n\n\n* Over **80%** of complaints filed were related to street lights not working, road maintenance, or garbage collection issues. \n* Of the four classification models, the linear support vector classifier performed the best, recording **88.773%** accuracy. The top three models all performed similarly as well, though, all falling within three percentage points.\n* Improvements were made by increasing the number of stop words, as well as combining smaller categories into larger ones. Using LinearSVC, these changes led to a 3.214% increase, ultimately recording an accuracy of **91.987%**. \n* Further improvements can be made by adding to the stop word list, changing the contents of the \"Others\" category, and adjusting the downsampling of the model. \n\n## Background\n\nText data can be difficult to analyze in large amounts, but raw text is invaluable in numerous different ways. Using simple Python libraries, modern machine learning models can parse thousands of rows in seconds, which can be used for a variety of purposes. One of the most common of these is *classification*, or categorizing text into different groups.  \n\nThe *Bruhat Bengaluru Mahangara Palike* (BBMP) - an administrative body that oversees city development in Bengaluru, the largest tech city in India - created a web application that allows citizens to file grievances with the city. From February 8th, 2020 to February 21st, 2021, a total of **105,956** complaints were filed to BBMP, or about 280 a day. Exploring this data not only provides insight into the most common problems facing this city (or at least the complaints most likely to be sent), but also presents an opportunity to quantify and categorize them. \n\nIn the dataset, complaints have been manually categorized by the administrators who oversee the app at BBMP, but this is extremely inefficient. Usefully, though, the developers have already created categories that they felt best sorted the data, which means, assuming complaints don't change substantially in the future, we can train a machine learning model that performs this task automatically. This could save hours and hours of time.\n\nBecause the categories are already defined, this will be a **supervised** classification model. \n\n## Data Wrangling\n\nData wrangling and EDA done using `R`. \n\nFirst, we import the data and the libraries we will be using. \n\n```{r echo=TRUE, message=FALSE, warning=FALSE}\n# Load Libraries\nlibrary(tidyverse) \nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(echarts4r)\nlibrary(here)\nlibrary(lubridate)\nlibrary(wordcloud2)\nlibrary(reticulate)\n\n# Set Directory \nhere::set_here()\n\n# Import Data\ngrievances <- readr::read_csv(\"bengaluru-grievances.csv\")\n```\n\nThe admins at BBMP keep their data neat and tidy, so there's not many problems to fix. There are a couple things to consider, however. \n\nThe first issue is that some descriptions are extremely short, making classifying them accurately near impossible. We can limit the number of characters that a complaint must have, though the appropriate number of rows to remove is debatable. Ideally, we don't want to exclude too much of the data while still removing descriptions too short to be properly categorized. \n\nUsing `str_length` from the `stringr` package, we see 5,392 complaints contained fewer than 12 characters, meaning removing them would still preserve 95% of the original data. Using `filter` from `dplyr`, we can keep all complaints containing more than 12 characters. \n\n```{r, echo=TRUE, message=FALSE, warning=FALSE}\n# Counting number of rows \ngrievances %>%\n  filter(str_length(description) < 12) %>%\n  count() \n```\n\n```{r, echo=TRUE, message=FALSE, warning=FALSE}\n# Removing those rows \ngrievances <- grievances %>%\n  filter(str_length(description) > 12)\n```\n\nThe other consideration is whether the existing categories accurately reflect the data or not. It's possible certain similarities between different categories would better be combined into one, and likewise, single categories that should be multiple ones. These changes might not only provide a better description of the data, but improve accuracy in the long run. \n\nFor now, we will leave the original categories intact and proceed, but future models may benefit from this step.  \n\n## Exploratory Data Analysis  \n\nInteractive charts created using `echarts4r`. \n\n### Number of Grievances By Day \n\n```{r, echo=TRUE, message=FALSE, warning=FALSE}\n# Set Theme \ne_common(\n  font_family = \"Georgia\"\n)\n\n# Chart \ngrievances %>%\n  group_by(created_at = as.Date(created_at)) %>%\n  summarise(Total = n()) %>%\n  e_charts(created_at) %>%\n  e_line(Total, symbol = \"none\") %>%\n  e_x_axis(axisLabel = list(interval = 0)) %>%\n  e_title(\n    text = \"Total number of grievances by day\",\n    subtext = \"Data: BBMP\"\n  ) %>%\n  e_color(\n    \"#0a32d2\", \n    background = \"rgb(0, 0, 0, 0)\"\n  ) %>%\n  e_legend(show = FALSE) %>%\n  e_tooltip(\n    backgroundColor = \"rgba(20, 20, 20, 0.5)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    textStyle = list(\n      color = \"#ffffff\"\n    ),\n    trigger = \"axis\"\n  )\n```\n\nOn most days, grievances would vary between 200 to 400 a day, with some spikes in September onwards, including a massive single-day one in March of 742. On average, 280 complaints were filed each day. \n\nBecause complaints don't seem to fluctuate significantly, we can likely assume that the day the grievance was filed isn't indicative of its contents.  \n\n### Grievances by Category\n\n```{r, echo=TRUE}\ngrievances %>%\n  group_by(category) %>%\n  summarise(Total = n()) %>%\n  arrange(desc(Total)) %>%\n  filter(Total > 1000) %>%\n  slice(1:10) %>%\n  e_charts(category) %>%\n  e_bar(Total) %>%\n  e_x_axis(\n    axisLabel = list(\n      interval = 0, \n      rotate = 45,\n      fontSize = 9.25\n    )\n  ) %>%\n  e_title(\n    text = \"Most common grievances by category\", \n    subtext = \"Only categories above 1,000 visible\"\n  ) %>%\n  e_legend(show = FALSE) %>%\n  e_labels(show = FALSE) %>%\n  e_color(\n    \"#8c5ac8\", \n    background = \"rgb(0, 0, 0, 0)\"\n  ) %>%\n  e_tooltip(\n    trigger = \"axis\",\n    backgroundColor = \"rgba(20, 20, 20, 0.5)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    textStyle = list(\n      color = \"#ffffff\"\n    )\n  )\n```\n\n81.89% of total grievances were categorized as electrical, solid waste or garbage related, or road maintenance. The next three largest categories make up an additional 11.09%, meaning the remaining categories have less than 1,350 occurrences combined. \n\nNote that this is *not* due to a lack of categories; in fact, there are a total of 20 categories in the data. \n\n```{r}\nlength(unique(grievances$category))\n```\n\nLarge imbalances like this are important to consider when building machine learning models. Algorithms are specifically programmed to achieve the highest accuracy regardless of original purposes, and they tend to overestimate larger categories. If a model discovers it can restrict itself to three options while still recording 80%+ accuracy, it will almost always do so. \n\nThis means, though, machine learning models will tend to ignore minor categories, because - using our current data as an example - predicting a category outside the top three has an inherent 81.89% *fail* rate, so this will need to be addressed when creating the models. \n\nThere is a question as to whether certain smaller categories should exist at **all**, though. \n\n```{r}\ngrievances %>%\n  group_by(category) %>%\n  summarise(total = n()) %>%\n  filter(total < 100)\n```\n\nFive categories have less than 100 complaints total, including two which have less than thirty. This is far too few complaints to reliably build a model with, especially considering we haven't split the data yet.\n\n### Grievances by Subcategory \n\n```{r}\ngrievances %>%\n  group_by(subcategory) %>%\n  summarise(Total = n()) %>%\n  arrange(Total) %>%\n  filter(Total > 1000) %>%\n  e_charts(subcategory) %>%\n  e_bar(Total) %>%\n  e_legend(show = FALSE) %>%\n  e_title(\n    text = \"Most common grievances by subcategory\", \n    subtext = \"Only categories above 1,000 visible\"\n    ) %>%\n  e_color(\"#8c5ac8\", background = \"rgb(0,0,0,0)\") %>%\n  e_flip_coords() %>%\n  e_tooltip(\n    trigger = \"axis\",\n    backgroundColor = \"rgba(20, 20, 20, 0.5)\",\n    borderColor = \"rgba(0, 0, 0, 0)\",\n    textStyle = list(\n      color = \"#ffffff\"\n    )\n  )\n```\n\nWhile the model being built is only to predict the main categories, by viewing subcategories, we see over 35% of total complaints were related specifically to street lights not working, comprising almost all of the electrical category. Solid waste related problems is divided into two subcategories; garbage vehicle not arriving, and \"Garbage dump\" (whatever that means). Meanwhile, road maintenance has been divided into three subcategories, those being potholes, road side drains, and debris removal.\n\n### Most Common Complaint Words \n\nWe first have to get the 50 most common words...\n\n```{r, message=FALSE, warning=FALSE}\ntidy <- grievances %>%\n  unnest_tokens(word, description) %>%\n  anti_join(get_stopwords()) %>%\n  count(word) %>%\n  arrange(desc(n)) %>%\n  slice(1:50)\n```\n\nThen chart them using `wordcloud2`. \n\n```{r}\nwordcloud2(\n  tidy,\n  color = rep_len(c(\"#8c5ac8\", \"#0b0d21\", \"#0a32d2\"), nrow(tidy)),\n  backgroundColor = \"#fcfcfc\"\n)\n```\n\nSeeing as over a third of the data was subcategorized as street lights not working, we see that they are the most common words across all the complaints.\n\nMissing from that is the word *not*, but this is because we removed all **stop words** from the data. Put simply, stop words are words that don't add anything to the process of categorizing text. They usually include words like *he*, *she*, *there*, *they*, *I*, and so on, which is why they don't appear in the chart. \n\nThe stop words included in the `tidytext` package (and the ones used in the function above) are meant to apply universally, but if you look closely at the word cloud, there are several words that almost certainly don't apply to our problem. These include terms like *please*, *kindly*, *last request*, *sir*, and individual numbers like *1*, *2*, and *3*. While there might exist some incidental correlation between some of these words and their respective categories (perhaps citizens filing animal control complaints are nicer on average, so the term *please* could be used to identify those complaints more accurately, for example), it's likely this will just throw off our model's accuracy in the long run. \n\n---\n\nFrom the EDA, the primary factors that should be considered when building the models are: \n\n1. Account for imbalances in number of occurrences per category. \n2. Reduce the number of categories by combining them into existing ones. \n3. Add additional stop words. \n\n## MLE Models \n\n### Preparing the Data \n\nWe'll opt to use Python for creating the MLE models, as Python libraries are generally more efficient and developed than their R counterparts. Because creating models is computer intensive, the code here has been evaluated locally and presented here for demonstration.\n\nTo start, we'll import the necessary libraries and load the data using `pandas`. The models here will be built using `sklearn`. \n\n```{python, eval=FALSE, echo=TRUE, warning=FALSE}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns  \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n\ndf = pd.read_csv(\"bengaluru-grievances.csv\")\n```\n\nOne of the packages imported here is `TfidfVectorizer`, which will be the algorithm used to create the models. I'll explain why I specifically chose this package later on. \n\nHere, I quickly apply the earlier data wrangling techniques by removing complaints less than 12 characters, this time using Python syntax. \n\n```{python, eval=FALSE, echo=TRUE, warning=FALSE}\n# Reduce Character Limit  \ncharacter_limit = (df[\"description\"].str.len() > 12)\ndf = df.loc[character_limit]\n```\n\nA model based on over 100,000 observations is **extremely** hardware intensive, and it causes my laptop to overheat a lot. For practical purposes, we'll cut down on our data by choosing 15,000 rows at random. \n\n```{python, eval=FALSE, echo=TRUE, warning=FALSE}\ndf = df.sample(15000, random_state = 1).copy()\n```\n\nIf we wanted to get another random 15,000 rows, we could change the `random_state = 1` argument to any other number, like 42, 671, or 7. \n\nWith no other changes to be made, we can begin creating the models. \n\n### Text Preprocessing \n\nThere are several methods to building models, but the simplest method is to create a new column in our data - we'll call it **category_id** - that is a factor variable of all existing categories in our data. This essentially amounts to assigning each category a number (Electrical = 0, Road Engineering = 1, etc), which is necessary for getting our model to run properly, as `sklearn` will not understand strings as factors.   \n\n```{python, eval=FALSE, echo=TRUE, warning=FALSE}\ndf['category_id'] = df[\"category\"].factorize()[0]\ncategory_id_df = df[[\"category\", \"category_id\"]].drop_duplicates()\n```\n\nNext, the description column (which stores the text for grievances) needs to be converted to vectors using a chosen algorithm. The algorithm chosen here is **Term Frequency - Inverse Document Frequency** (TF-IDF), which is the product of $TF$ and $IDF$ scores. This is the `TfidfVectorizer` function that we imported earlier. \n\nIt's useful to present these terms mathematically.\n\n<br>\n\n**Term Frequency**: $$ TF = \\frac{Number \\hspace{0.15cm} of \\hspace{0.15cm} times \\hspace{0.15cm} a \\hspace{0.15cm} term \\hspace{0.15cm} appears \\hspace{0.15cm} in \\hspace{0.15cm} the \\hspace{0.15cm} description}{Total \\hspace{0.15cm} number \\hspace{0.15cm} of \\hspace{0.15cm} words \\hspace{0.15cm} in \\hspace{0.15cm} the \\hspace{0.15cm} description} $$\n\n**Inverse Document Frequency**: $$ IDF = log(\\frac{Number \\hspace{0.15cm} of \\hspace{0.15cm} rows \\hspace{0.15cm} in \\hspace{0.15cm} a \\hspace{0.15cm} data}{Number \\hspace{0.15cm} of \\hspace{0.15cm} rows \\hspace{0.15cm} a \\hspace{0.15cm} term \\hspace{0.15cm} appears \\hspace{0.15cm} in \\hspace{0.15cm} a \\hspace{0.15cm} data}) $$\n\n**TF-IDF**: $$ TF-IDF = TF * IDF $$\n\n<br>\n\nThe reason for choosing this algorithm was for the $IDF$ component, which **downsamples** words that appear frequently across all complaints, while adding extra weight to terms that appear less often. \n\nAnalyzing it mathematically: as the denominator of the $IDF$ variable increases, the closer it rapidly (more precisely, *exponentially*) approaches zero. Let $N$ represent the total number of rows in the data, and let $t$ represent a chosen term. If a certain word were to appear in **every** single complaint, then we would have $IDF(N, t) = log(\\frac{N}{t}) = log(\\frac{N}{N}) = log(1) = 0$, which would mean that when calculating $TF-IDF$, that specific word would have absolutely no weight attached to it when classifying complaints. \n\nNow; why do this? As discussed previously during the EDA section, almost 82% of all complaints fell into exactly three categories. Classification models will tend to stick to only a few categories, struggling to identify minor categories. While this may record higher accuracy scores on average, doing so means minor categories will rarely be classified at all, and in some instances, could lower overall accuracy if skew is significant enough. By ranking terms on an exponentially decreasing scale, we hope to reduce this issue. \n\nWe first setup our `TfidfVectorizer` and assign it to a variable, `tfidf`. \n\n```{python, eval=FALSE, echo=TRUE, warning=FALSE}\ntfidf = TfidfVectorizer(\n  sublinear_tf = True, # Set term frequency to logarithmic scale\n  min_df = 5, # Remove terms that appear less than 'min_df' times\n  ngram_range = (1, 2), # Unigrams and bigrams are considered \n  stop_words = 'english' # Use common English stop words\n)\n```\n\nFrom here, we can begin building our models, but before doing so, let's see what the most common terms were for each category. \n\nTo do so, we use `tfidf.get_feature_names_out()` on each category and assign that to a variable that we'll call **feature_names**. This contains all of the most common words associated with each category, which we then split into two separate lists for unigrams and bigrams (fancy words for \"one word\" and \"two words\"). From there, we print to console the $N = 3$ most common terms from each list. We wrap all this in a `for` loop, automatically progressing through each category. \n\n```{python, eval=FALSE, echo=TRUE, warning=FALSE}\n# Defaults \nfeatures = tfidf.fit_transform(df.description).toarray()\nlabels = df.category_id\nN = 3\n\n# For Loop\nfor category, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names_out())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"\\n==> %s:\" %(Product)) # Space for formatting\n  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))\n```\n\nAs this part was performed offline, here is the output in screenshots.  \n\n![](_images/term-correlation-1.png)\n\n![](_images/term-correlation-2.png)\n\n![](_images/term-correlation-3.png)\n\nThe results are largely what we would expect, though there are some things to note. \n\nSome common phrases that appear for certain categories seemingly have nothing to do with them, such as the terms \"77\", \"kindly\", and \"plz look\", which is one of the most common bigrams for both \"Education\" and \"Welfare Schemes.\" Remember, these were the categories that had less than 100 observations **total**. When we split the data to grab 15,000 random rows, these categories were split even further, which is probably why these nonsense phrases appear. \n\n### Building the Models \n\nTo begin, we first split the data into a 75:25 training and test split. The model will \"learn\" how to classify grievances based on the training data, and then it will \"test\" its accuracy on the remaining 25%.\n\n```{python, eval=FALSE, echo=TRUE, warning=FALSE}\n# We define them here as independent variables \nX = df[\"description\"]\ny = df[\"category\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n```\n\nThere are several different models to choose from, but it's hard to know which will perform best before actually building them. That's why we'll test several models simultaneously by storing them in a list and looping through each model.\n\n```{python, eval=FALSE, echo=TRUE, warning=FALSE}\nmodels = [\n    RandomForestClassifier(n_estimators = 100, max_depth = 5, random_state = 0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state = 0)    \n]\n```\n\nHere, we stored in a list the following:  \n\n+ Random Forest Model\n+ Linear Support Vector Classifier Model\n+ Multinomial Naive Bayes Model\n+ Logistic Regression Model\n\nAfter, we apply each model to the training data and record the results. The accuracy of each model is inherently random, as model performance is somewhat due to chance, so we'll use a five-fold cross-validation and take the mean average of each iteration to get a more balanced result. We store the results in a `pandas` dataframe for analysis. \n\n```{python, eval=FALSE, echo=TRUE, warning=FALSE}\n# Copy and pasted from before \nfeatures = tfidf.fit_transform(df.description).toarray()\nlabels = df.category_id\n\nCV = 5 # Number of cross-validations\ncv_df = pd.DataFrame(index = range(CV * len(models))) # CV dataframe\nentries = [] # Array for storing model results \n\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\n    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n```\n\n### Results \n\nBecause we used a five-fold cross-validation, we have a total of 20 accuracy results - five for each model. We grab the mean accuracy and standard deviation for each model, storing them into a list. \n\n```{python, eval=FALSE, echo=TRUE, warning=FALSE}\nmean_accuracy = cv_df.groupby('model_name').accuracy.mean()\nstd_accuracy = cv_df.groupby('model_name').accuracy.std()\n\naccuracy = pd.concat([mean_accuracy, std_accuracy], axis= 1, ignore_index=True)\naccuracy.columns = ['Mean Accuracy', 'Standard Deviation']\n\naccuracy\n```\n\n| Model               | Mean Accuracy | Standard Deviation |\n|---------------------|---------------|--------------------|\n| Linear SVC          | 88.773%       | 0.368%             |\n| Logistic Regression | 87.767%       | 0.433%             | \n| Multinomial NB      | 85.720%       | 0.117%             |  \n| Random Forest       | 66.213%       | 1.411%             | \n\nThe top three models all performed similarly as well, all falling within 3.1% percentage points. The Linear Support Vector Classifier performed the best among the three, while the Random Forest performed atrociously. Standard deviation among the top three remained fairly low, but especially for multinomial naive bayes.  \n\n## Improvements \n\nWe will focus model improvement on the Linear SVC because it performed the best. \n\nAs a reminder, these were the three main considerations before going in. \n\n1. Account for imbalances in number of occurrences per category. \n2. Consider reducing number of categories. \n3. Consider adding additional stopwords. \n\nTo get a better idea of how our model performed, we will plot a **confusion matrix**, which displays the total number of attempts our classification model made along with how many were accurately categorized.\n\n```{python, eval=FALSE, echo=TRUE}\n# Recreating LinearSVC Model \nX_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, \n                                                               labels, \n                                                               df.index, test_size=0.25, \n                                                               random_state=1)\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Confusion Matrix Plot\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize = (8, 8))\nsns.heatmap(conf_mat, annot = True, cmap = \"Greens\", fmt = 'd',\n            xticklabels = category_id_df.category.values, \n            yticklabels = category_id_df.category.values)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix for LinearSVC \\n\", size = 20)\n```\n\n<center>\n![](_images/heatmap.png)\n</center>\n\nOn the diagonal are the number of rows that the model correctly predicted for each category. The horizontals and verticals represent the number of incorrect guesses, with the vertical representing incorrect guesses for that specific category. For example, the model correctly classified 1,484 complaints as \"Electrical,\" incorrectly classified 2 as \"Electrical\" when they should of been classified as \"COVID-19,\" and classified 10 as \"Road Maintenance\" when they should of been classified as \"Electrical.\" \n\n### Modifications \n\nLooking at the chart, the top three categories dominate the total number of occurrences, comprising 2,969 rows out of 3,750 in our test data. Most of the incorrect predictions appear in the vertical of each of these three columns, meaning the model was incorrectly classifying complaints as them often. Even though we chose an algorithm to specifically downsample those categories, our model still has a tendency to over-predict them. \n\nA few categories have 12 or fewer observations: those being Markets, Estate, OFC, Welfare Schemes, Advertisement, Education, Town Planning, Lakes, and Parks and Playgrounds. Converting these will likely improve accuracy considering how poorly our model did at predicting them, but there isn't clear category to merge them with. Many of these categories seem to have been falsely labeled as \"Road Maintenance.\" While converting these columns over to this might lead to higher accuracy, it doesn't really make any sense in this case, and likely would hurt performance in the future. \n\nWe could reassign these variables to \"Others,\" but that category performed *abysmally*, only correctly predicting 3 out of 43 complaints. On one hand, moving them there probably won't hurt, but it likely won't improve \"Others\" result either. \n\nLakes and Advertisements, which the model predicted quite a few correctly, will be left untouched for now. For the remaining categories under 12 test observations, they will be merged in with Others.\n\n```{python, eval=FALSE, echo=TRUE}\n# Read in Data | Copy and Paste from Above\ndf = pd.read_csv(\"bengaluru-grievances.csv\")\npd.DataFrame(df.category.unique()).values \ncharacter_limit = (df[\"description\"].str.len() > 12)\ndf = df.loc[character_limit]\ndf = df.sample(15000, random_state = 2).copy() # Select New Rows \n\n# Convert Columns \ndf[\"category\"] = df[\"category\"].replace({'Markets': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Estate': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Welfare Schemes': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Education': 'Others'})\ndf[\"category\"] = df[\"category\"].replace({'Town Planning': 'Others'})\n```\n\n\"Optical Fiber Cables\" and \"Storm Water Drains,\" though, are directly related to \"Road Maintenance,\" and if we look at the chart, that's what the model ended up incorrectly guessing the most of. For these categories, it makes more sense to convert them over to \"Road Maintenance\" as opposed to \"Others.\" \n\n```{python, eval=FALSE, echo=TRUE}\ndf[\"category\"] = df[\"category\"].replace({'Optical Fiber Cables (OFC)': 'Road Maintenance(Engg)'})\ndf[\"category\"] = df[\"category\"].replace({'Storm  Water Drain(SWD)': 'Road Maintenance(Engg)'})\n```\n\nWhile we've converted the total number of categories down from 20 to 13, we've only changed a total of 45 test rows. Even if the improved model were able to correctly predict all these observations now, we would only see an improvement of 1.2%. It's certainly not insignificant, but hardly substantial. Improving our stop word list, on the other hand, will hypothetically improve the accuracy of the model overall. \n\nRecall earlier when we found the most common unigrams and bigrams for each category. Several terms that appeared most often had little or nothing to do with their respective grievances, and should be able to be removed while maintaining or improving original accuracy. \n\nThe original stop word list comes from another function in `sklearn`, and already contains over 300 words. We want to keep those words while adding to it, so we will union them together in a new list and use it in `TfidfVectorizer`.  \n\n```{python, eval=FALSE, echo=TRUE}\n# Import Function \nfrom sklearn.feature_extraction import text\n\n# Add Stop Words \nstop_words = text.ENGLISH_STOP_WORDS.union([\"please\", \"plz\", \"look\", \"help\", \"causing\", \"coming\", \"kindly\", \"refused\", \"senior\", \"help\", \"one\", \"two\", \"three\", \"also\", \"77\", \"1\", \"2\", \"3\", \"since\"])\n\n# TfidfVectorizer \ntfidf = TfidfVectorizer(\n  sublinear_tf = True, # Set term frequency to logarithmic scale\n  min_df = 5, # Remove terms that appear less than 'min_df' times\n  ngram_range = (1, 2), # Keep unigrams and bigrams\n  stop_words = stop_words # Use custom stop words \n)\n```\n\n### Redo Text Preprocessing \n\nWe have to redo the text preprocessing from earlier, so this is all copy-and-paste from before. \n\n```{python, eval=FALSE, echo=TRUE}\n# Copy and Paste\ndf['category_id'] = df[\"category\"].factorize()[0]\ncategory_id_df = df[[\"category\", \"category_id\"]].drop_duplicates()\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[[\"category_id\", \"category\"]].values)\n\nfeatures = tfidf.fit_transform(df.description).toarray()\nlabels = df.category_id\nX = df[\"description\"]\ny = df[\"category\"]\n\nX_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(\n  features, \n  labels, \n  df.index, test_size = 0.25, \n  random_state = 2)\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n```\n\n### New Results \n\nWe don't need the complicated `for` loop from before because we only have one model this time. Therefore, we simply use `cross_val_score` as we did before and print the results to console. \n\n```{python, eval=FALSE, echo=TRUE}\naccuracy_svc = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n\ncv_mean_accuracy_svc = accuracy_svc.mean()\ncv_mean_std_svc = accuracy_svc.std()\n\nprint(cv_mean_accuracy_svc * 100)\nprint(cv_mean_std_svc * 100)\n```\n\n<center>\n![Results](_images/results.png)\n</center>\n\nOur new Linear SVC model was able to achieve **91.987%** accuracy with an average standard deviation of *0.282%* across five iterations. That's an improvement of **3.214%** while also reducing variance within model performance by *0.086%*. \n\nAnother way to look at these refinements; out of a possible **3,750** complaints, our model was able to correctly classify an additional *120* complaints, going from **3,328** correct predictions to **3,449**.  \n\n<center>\n![](_images/heatmap-2.png)\n</center>\n\n## Conclusions \n\nOnce again, the top three categories performed similarly as well as before. Road maintenance was able to correctly predict an additional *63* complaints on this iteration. These three categories also continue to make up most of the incorrect predictions. \n\nThe \"Others\" categories once again performed dreadfully, only recording an additional two correct predictions despite even more chances. Given it's a category meant to be all-emcompassing, it probably makes sense to manually reclassify those comaplaints into new or existing categories. \n\nMinor categories saw little or no improvement. The \"Health Dept\" was the only category that performed worse, dropping from 69.7% to 54.6% accuracy. The model incorrectly chose \"Solid Waste\" and \"Road Maintenance\" much more often than the previous model did, though it's unclear as to why this is. \n\nIncreasing the stop word list seems to have improved accuracy overall. A more thorough list and additional adjustments might boost performance slightly more. \n\n## Source\n\nYou can check out the Python source code for the MLE model [here](https://github.com/Nathan-States/Bengaluru-Text-Mining). Download the full dataset [here](https://www.dolthub.com/repositories/bkowshik/bbmp-sahaaya).\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../assets/styles.css"],"highlight-style":{"light":"../../../assets/syntax-light.theme","dark":"../../../assets/syntax-dark.theme"},"toc":true,"output-file":"index.html"},"language":{"code-summary":"Show code"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","editor":"source","theme":{"light":"../../../assets/states-light.scss","dark":"../../../assets/states-dark.scss"},"smooth-scroll":true,"code-block-border-left":true,"title-block-banner":true,"title":"Bengaluru Text Mining","description":"Analyzing over 100,000 complaints from the Silicon Valley of India.","author":"Nathan States","date":"01/22/2023","categories":["text_mining","machine_learning"],"toc-location":"right"},"extensions":{"book":{"multiFile":true}}}}}